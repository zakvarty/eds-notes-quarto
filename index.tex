% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Effective Data Science},
  pdfauthor={Zak Varty},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Effective Data Science}
\author{Zak Varty}
\date{2025-01-17}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\chapter*{About this Course}\label{about-this-course}
\addcontentsline{toc}{chapter}{About this Course}

\markboth{About this Course}{About this Course}

Model building and evaluation are necessary but not sufficient skills
for the effective practice of data science. In this module you will
develop the technical and personal skills that are required to work
successfully as a data scientist within an organisation.

During this module you will critically explore how to:

\begin{itemize}
\tightlist
\item
  effectively scope and manage a data science project;
\item
  work openly and reproducibly;
\item
  efficiently acquire, manipulate, and present data;
\item
  interpret and explain your work for a variety of stakeholders;
\item
  ensure that your work can be put into production;
\item
  assess the ethical implications of your work as a data scientist.
\end{itemize}

This interdisciplinary course will draw from fields including
statistics, computing, management science and data ethics. Each topic
will be investigated through a selection of lecture videos, conference
presentations and academic papers, supported by hands-on lab exercises
and readings on industry best-practices as published by recognised
professional bodies.

\section*{Schedule}\label{schedule}
\addcontentsline{toc}{section}{Schedule}

\markright{Schedule}

These notes are intended for students on the course \textbf{MATH70076:
Data Science} in the academic year 2024/25.

As the course is scheduled to take place over five weeks, the suggested
schedule is:

\begin{itemize}
\tightlist
\item
  1st week: effective data science workflows;
\item
  2nd week: acquiring and sharing data;
\item
  3rd week: exploratory data analysis and visualisation;
\item
  4th week: preparing for production;
\item
  5th week: ethics and context of data science.
\end{itemize}

An alternative pdf version of these notes may be downloaded
\href{./Effective-Data-Science.pdf}{here}. Please be aware that this pdf
version is secondary to this course webpage and will be updated less
frequently.

\section*{Learning outcomes}\label{learning-outcomes}
\addcontentsline{toc}{section}{Learning outcomes}

\markright{Learning outcomes}

On successful completion of this module students should be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independently scope and manage a data science project;
\item
  Source data from the internet through web scraping and APIs;
\item
  Clean, explore and visualise data, justifying and documenting the
  decisions made;
\item
  Evaluate the need for (and implement) approaches that are explainable,
  reproducible and scalable;
\item
  Appraise the ethical implications of a data science projects,
  particularly the risks of compromising privacy or fairness and the
  potential to cause harm.
\end{enumerate}

\section*{Allocation of Study Hours}\label{allocation-of-study-hours}
\addcontentsline{toc}{section}{Allocation of Study Hours}

\markright{Allocation of Study Hours}

\textbf{Lectures:} 10 Hours (2 hours per week)

\textbf{Group Teaching:} 5 Hours (1 hour per week)

\textbf{Lab / Practical:} 10 hours (2 hours per week)

\textbf{Independent Study:} 100 hours (11 hours per week + 45 hours
coursework)

\textbf{Drop-In Sessions:} Each week there will be an optional drop-in
session to address any questions about the course or material. This is
where you can get support from the course lecturer or GTA on the topics
covered each week, either individually or in small groups.

These will be held on Fridays 14:00-15:00 in Huxley 711C.

\section*{Assessment Structure}\label{assessment-structure}
\addcontentsline{toc}{section}{Assessment Structure}

\markright{Assessment Structure}

The course will be assessed entirely by coursework, reflecting the
practical and pragmatic nature of the course material.

\textbf{Coursework 1 (30\%):} A reproducible data journalism task, to be
completed during one week of the course.

\textbf{Coursework 2 (70\%):} A student-led development of a data
product, e.g.~an in depth statistical analysis, portfolio website, R
package or dashboard. To be released during the course and submitted
following the examination period in Summer term.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

These notes were created by Dr Zak Varty. They were inspired by a
previous lecture series by Dr Purvasha Chakravarti at Imperial College
London and draw from many resource that were made available by the R
community, which are attributed throughout.

\part{Effective Workflows}

\section*{Introduction}\label{introduction}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

As a data scientist you will never work alone.

Within a single project as a data scientist it is likely that you will
interact with a range of other people, including but not limited to: one
or more project managers, stakeholders and subject matter experts.
Depending on the type of work that you are doing, these experts might
come from a single specialism or form a multi-disciplinary team. To get
your project put into production and working at scale you will likely
have to collaborate with data engineers. You're also likely to work
closely with other data scientists, reviewing one another's work or
collaborating on larger projects. Familiarity with the skills, processes
and practices that make for effective collaboration is therefore
instrumental to being a successful as a data scientist.

The aim for this part of the course is to provide you with a structure
on how you organise and perform your work, so that you can be a good
collaborator to current colleges and your future self.

This is going to require a bit more effort upfront, but the benefits
will compound over time. You will get more done by wasting less time
staring quizzically at messy folders of indecipherable code. You will
also gain a reputation of someone who is good to work with. This
promotes better professional relationships and greater levels of trust,
which can in turn lead to working on more exciting and impactful
projects.

\chapter{Organising your work}\label{workflows-organising-your-work}

Welcome to this course on effective data science. This week we'll be
considering effective data science workflows. These workflows are ways
of progressing a project that will help you to produce high quality work
and help to make you a better collaborator.

In this chapter, we'll kick things off by looking at how you can
structure data science projects and organise your work. Familiarity with
these skills, processes and practices for collaborative working will be
instrumental to you become a successful data scientist.

\section{What are we trying to do?}\label{what-are-we-trying-to-do}

First, let's consider why we want to provide our data science projects
with some sense of structure and organization.

As a data scientist you'll never work alone. Within a single project
you'll interact with a whole range of other people. This might be a
project manager, one or more business stakeholders or a variety of
subject matter experts. These experts might be trained as sociologists,
chemists, or civil servants depending on the exact type of data science
work that you're doing.

To then get your project put into use and working at scale you'll have
to collaborate with data engineers. You'll also likely work closely with
other data scientists. For smaller projects this might be to act as
reviewers for one another's work. For larger projects, working
collaboratively will allow you to tackle larger challenges. These are
the sorts of project that wouldn't be feasible alone, because of the
inherent limitations on the time and skill of any one individual person.

Even if you work in a small organization, where you're the only data
scientist, then working in a way that is optimised for collaboration
will pay dividends over time. This is because when you inevitably return
to your current project in several weeks, months or years. At that point
you will have forgotten almost everything that you did the first time
around, let alone why you chose to do it that way. You'll have forgotten
about not only the decisions that you made, but also alternative options
that you decided against.

This is exactly like working with a current colleague who has poor
communication skills and working practices. Nobody wants to be that
colleague to somebody else, let alone to their future self. Treating
that future self like a current collaborator will make you a kind
colleague and a pleasure to work with.

In addition to this, you'll get more done by wasting less time staring
quizzically at a mess of folders and indecipherable code. You'll also
get a reputation as someone who is well-organized and enjoyable to work
with. This promotes better professional relationships and greater levels
of trust within your team. These can then, in turn, tead to you working
on more exciting and more impactful projects in the future.

The aim of this week is to provide you with a guiding structure on how
to organize and perform your work. None of this is going to be
particularly difficult or onerous. However it will require some up-front
effort and daily discipline to maintain. As with brushing your teeth and
flossing, the daily effort required is not large but the benefits
compound over time.

\section{An R Focused Approach}\label{an-r-focused-approach}

The structures and workflows that are recommend here and throughout the
rest of this module are focused strongly on a workflow that
predominantly uses R, markdown and LaTeX.

Similar techniques, code and software can achieve the same results that
I show you here when coding in Python or C, or when writing up projects
in JuPyteR notebooks or using some other markup language. Similarly,
different organizations have their own variations on these best
practices that we'll go through together. Often organisations will have
extensive guidance on these topics.

The important thing is that once you understand what good habits are and
have built them in one programming language or business, then
transferring these skills to a new setting is largely a matter of
learning some new vocabulary or slightly different syntax.

With that said, let's get going!

\section{One Project = One Directory}\label{one-project-one-directory}

If there's one thing you should take away from this chapter, it's this
one Golden Rule:

\begin{quote}
Every individual project you work on as a data scientist should be in a
single, self-contained directory or folder.
\end{quote}

This is worth repeating. Every single project that you work on should be
self-contained and live in a single directory. An analogy here might be
having a separate ring-binder folder for each of your modules on a
degree program.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}1.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-1.png}

This one golden rule is deceptively simple.

The first issue here is that it requires a predetermined scope of what
is and what isn't going to be covered by this particular project. This
seems straightforward but at the outset of the project you often don't
know exactly where your project will go, or how it will link to other
pieces of work within your organization.

The second issue is that the second law of Thermodynamics applies
equally well to project management as it does to tidying your bedroom or
the heatdeath of the universe. It takes continual external effort to
prevent the contents of this one folder from becoming chaotic and
disordered over time.

That being said, having a single directory does have several benefits
that more than justify this additional work.

\section{Properties of a Well-Orgainsed
Project}\label{properties-of-a-well-orgainsed-project}

What are the properties that we would like this single, well-organized
project to have?

Ideally, we'd like to organize our project so that it has the following
properties:

\begin{itemize}
\tightlist
\item
  Portable
\item
  Version Control Friendly
\item
  Reproducible
\item
  IDE friendly.
\end{itemize}

Don't worry if you haven't heard of some of these terms already. We're
going to look at each of them in a little bit of detail.

\subsection{Portability}\label{portability}

A project is said to be portable if it can be easily moved without
breaking.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}2.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-2.png}

This might be a small move, like relocating the directory to a different
location on your own computer. It might also mean a moderate move, say
to another machine if yours dies just before a big deadline.
Alternatively, it might be a large shift - to be used by another person
who is running a different operating system.

From this small thought experiment, you can see that there's a full
spectrum of how portable a project may or may not need to be.

\subsection{Version Control Friendly}\label{version-control-friendly}

A project under Version Control has all changes tracked either manually
or automatically. This means that snapshots of the project are taken
regularly as it gradually develops and evolves over time. Having these
snapshots as many, incremental changes are made to the project allow it
to be rolled back to a specific previous state if something goes wrong.

A version controlled pattern of working helps to avoid the horrendous
state that we have all found ourselves in - renaming
\texttt{final\_version.doc} to \texttt{final\_final\_version.doc} and so
on.

By organising your workflow around incremental changes, you acknowledge
that no work is ever finally complete. There will always be small
changes that need to be done in the future.

\subsection{Reproducibility}\label{reproducibility}

\begin{quote}
A study is reproducible if you can take the original data and the
computer code used to analyze the data and recreate all of the numerical
findings from the study.

Broman et al (2017). ``Recommendations to Funding Agencies for
Supporting Reproducible Research''
\end{quote}

In their paper, Broman et al define reproducibility as a project where
you can take the original data and code used to perform the analysis and
using these we create all of the numerical findings of the study.

This definition leads naturally to several follow-up questions.

Who exactly is \emph{you} in this definition? Does it specifically mean
yourself in the future or should someone else with access to all that
data and code be able to recreate your findings too? Also, should this
reproducibility be limited to just the numerical results? Or should they
also be able to create the associated figures, reports and press
releases?

Another important question is \emph{when} this project needs to be
reproduced. Will it be in a few weeks time or in 10 years time? Do you
need to protect your project from changes in dependencies, like new
versions of packages or modules? How about different versions of R or
Python? Taking this time-scale out even further, what about different
operating systems and hardware?

It's unlikely that you would consider someone handing you a
\href{https://en.wikipedia.org/wiki/Floppy_disk}{floppy disk} of code
that only runs on Windows XP to be acceptably reproducible. Sure, you
could probably find a way to get it to work, but that would be an awful
lot of effort on your end.

That's perhaps a bit of an extreme example, but it emphasizes the
importance of clearly defining the level of reproducibility that you're
aiming for within every project you work on. This example also
highlights the amount of work that can be required to reproduce an
analysis, especially after quite some time. It's important to explicitly
think about how we are dividing that effort between ourselves as the
original developer and the person trying to reproduce the analysis in
the future.

\subsection{IDE Friendly}\label{ide-friendly}

Our final desirable property is that we'd like our projects to play
nicely with integrated development environments.

When you're coding document and writing your data science projects it
would be possible for you to work entirely in either a plain text editor
or typing code directly at the command line. While these approaches to a
data science workflow have the benefit of simplicity, they also expect a
great deal from you as a data scientist.

These workflows expect you to type everything perfectly accurately every
time, that you recall the names and argument orders of every function
you use, and that you are constantly aware of the current state of all
objects within your working environment.

Integrated Development Environments (IDEs) are applications to help
reduce this burden, allowing you a to be more effective programmer and
data scientist. IDEs offer tools like code completion and highlighting
that make your code easier to write and to read. They offer tools for
debugging, to fix your code when (not if!) things are going wrong. IDEs
also offer environment panes so that you don't have to hold everything
in your head all at once. Many IDEs have additional templating
facilities. These let you save and reuse snippets of code so that you
can avoid typing out repetitive, boilerplate code and introducing errors
in the process.

Even if you haven't heard of IDEs before, you've likely already used
one. Some common examples might be RStudio for R-users, PyCharm for
python users, or Visual Studio as a more language agnostic coding
environment. Whichever of these we use, we'd like our project to play
nicely with them. This lets us reap their benefits while keeping our
project portable, version controlled, and reproducible for someone
working with a different set-up.

\section{Project Structure}\label{project-structure}

We have given a pretty exhaustive argument for why having a single
directory for each project is a good idea. Let's now take a look
\emph{inside} that directory and define a common starting layout for the
content of all of your projects.

Having this sort of project directory template will mean that you'll
always know where to find what you're looking for and other members of
your team will too. Again, before we start I'll reiterate that we're
taking an opinionated approach here and providing a sensible starting
point for organizing many projects.

Every project is going to be slightly different and some might require
slight alterations to what I suggest here. Indeed, even if you start as
I suggest then you might have to adapt your project structure as it
develops and grows. I think it's helpful to consider yourself as a
tailor when making these changes. I'm providing you with a one size fits
all design, that's great for lots of projects but perfect for none of
them. It's your job to alter and refine this design for each individual
case.

One final caveat before we get started: companies and businesses usually
have a house style for how to write and organize your code or projects.
If that's the case, then follow the style guide that your business or
company uses. The most important thing here is to be consistent at both
an individual level and across the entire data science team. It's this
consistency that reaps the benefits, not the particular structure.

Okay, so imagine now that you've been assigned an exciting new project
and have created a single directory in which to house that project. Here
we've, quite imaginatively, called that directory
\texttt{exciting-new-project}. What do we populate this folder with?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}3.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-3.png}

In the rest of this chapter, I'll define the house-style for organizing
the root directory of your data science projects in this module.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}4.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-4.png}

Within the project directory there will be some subdirectories. You can
tell that these are folders in this file structure diagram because they
have a forward-slash following their names. There will also be some
files directly in the root directory. One of these is called
\texttt{readme.md} and the another called either \texttt{makefile} or
\texttt{make.r}. We will explore each of these files and directories in
turn.

\subsection{README.md}\label{readme.md}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}5.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-5.png}

Let's begin with the readme file. This gives a brief introduction to
your project and gives information on what the project aims to do. The
readme file should describe how to get started using the project and how
to contribute to its development.

The readme is written either in a plain text format so
\texttt{readme.txt} or in markdown format \texttt{readme.md}. The
benefit of using markdown is that it allows some light formatting such
as sections headers and lists using plain text characters. Here you can
see me doing that by using hashes to mark out first and second level
headers and using bullet points for a unnumbered list. Whichever format
you use, the readme file for your project is always stored in the root
directory and is typically named in all uppercase letters.

The readme file should be the first thing that someone who's new to your
project reads. By placing the readme in the root directory and
capitalising the file name you are increase the visibility of this file
and increase the chances of this actually happening.

An additional benefit to keeping the readme in the root directory of
your project is that code hosting services like GitHub, GitLab or
BitBucket will display the contents of that readme file next to the
contents of your project, nicely formatting and displaying any markdown
synax that you use.

When writing the readme it can be useful to imaginge that you're writing
this for a new, junior team member. The readme file should let them get
started with the project and make some simple contributions after
reading only that file. It might also link out to more detailed project
documentation that helps the new team member toward having a more
advanced understanding of the project or making a more complex
contribution.

\subsection{Inside the README}\label{inside-the-readme}

Let's take a moment to cover in more detail what should be included
within a readme file.

\subsubsection{Project name and status}\label{project-name-and-status}

A readme we should include the name of the project, which should be
self-explanatory (so nothing like my generic choice of
\texttt{exciting-new-project}). The readme should also give the project
status, which is just a couple of sentences to say whether your project
is still under development, the version oft the current release or, on
the other end of the project life-cycle, if the project is being
deprecated or closed.

Following this, we should also include a description of your project.
This will state the purpose of your work and to provide, or link to, any
additional context or references that visitors aren't assumed to be
familiar with.

\subsubsection{Code dependencies and
examples}\label{code-dependencies-and-examples}

If your project involves code or depends on other packages then you
should give some instruction on how to install those dependencies and
run your code. This might just be text but it could also include things
like screenshots, code snippets, gifs or a video of the whole process.

It's also a good practice to include some simple examples of how to use
the code within your project an the expected results, so that new users
can confirm that everything is working on their local instance. Keep the
examples as simple and minimal as you can so that new users

For longer or more complicated examples that aren't necessary in this
short introductory document you can add links to those in the readme and
explain them in detail elsewhere.

\subsubsection{How to contribute}\label{how-to-contribute}

There should ideally be a short description of how people can report
issues with the project and also how people can get started in resolving
those issues or extend the project in some way.

That leads me on to one point that I've forgotten to list here. There
there should be a section listing the authors of the work and the
license in which under which it's distributed. This is to give credit to
all the people who've contributed to your project and the license file
then says how other people may use your work. The license declares how
other may use your project and whether they have to give direct
attribution to your work in any modifications that they use.

\subsection{data}\label{data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}7.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-7.png}

Moving back to our project structure, next we have the data directory.

The data directory will have two subdirectories one called \texttt{raw}
and one called \texttt{derived}. All data that is not generate as part
of your project is stored in the \texttt{raw} subdirectory. To ensure
that a project is reproducible, data in the \texttt{raw} folder should
never be edited or modified.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}8.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-8.png}

In this example we've got two different data types: an Excel spreadsheet
and a JSON file. These files are exacty as we received them from our
project stakeholder.

A plain text file, \texttt{metadata.txt} explains the contents and
interpretation of each of the raw data sets. This metadata should
include descriptions of all the measured variables, the units that are
recorded in, the date the file was created or acquired, and the source
from which it was obtained.

The raw data most likely is not going to be in a form that's amenable to
analyzing straight away. To get the data into a more pleasant form to
work with, we will need to do some data manipulation and cleaning. Any
operations applied when cleaning the raw data should be well documented
and the resulting cleaned files are saved within the \texttt{derived}
data directory for use in our modelling or analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}12.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-12.png}

In our exciting new project, we can see the cleaned versions of the
previous data sets that are ready for modelling. There is also a third
file in this folder. This is data that we've acquired for ourselves
through web scraping, using a script within the project.

\subsection{src}\label{src}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}16.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-16.png}

The \texttt{src} or source directory contains all the source code for
your project. This will typically be the functions that you've written
to make the analysis or modelling code more accessible.

In this project, we have saved each function in its own R script and
used subdirectories to organise these by their use case. We have two
functions used in data cleaning: the first replaces \texttt{NA} values
with a user specified value, the second function replaces missing values
by the mean of all non-missing values.

We also have three helper functions: the first two calculate the rolling
mean and the geometric mean of a given vector, while the third is a
function to scrape the web data we previously found in the derived data
subdirectory.

\subsection{tests}\label{tests}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}12.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-12.png}

The structure of the \texttt{tests/} directory mirrors that of the
source directory. Each function file has its own counterpart file of
tests.

These test files provide example sets of inputs and the expected outputs
for each function. The test files are used to check edge cases of a
function and to assure yourself that you didn't break anything while you
are fixing some small bug or adding new capabilities to that function.

\subsection{analyses}\label{analyses}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}23.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-23.png}

The analyses directory contains what you probably think of as the bulk
of your data science work. It's going to have one subdirectory for each
major analysis that's performed within your project and within each of
these there might be a series of steps which we separate into separate
scripts.

The activity performed at each step is made clear by the name of each
script, as is the order in which we're going to perform these steps.
Here we can see the scripts used for the 2021 annual report. First is a
script used to take the raw monthly receipts and produce the
\emph{cleaned} version of the same data set that we saw earlier. This is
followed by a trend analysis of this cleaned data set.

Similarly for the spending review we have a data cleaning step, followed
by some forecast modelling and finally the production of some diagnostic
plots to compare these forecasts.

\subsection{outputs}\label{outputs}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}26.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-26.png}

The outputs directory has again one subdirectory for each meta-analysis
within the project. These are then further organized by the output type,
whether that be some data, a figure, or a table.

Depending on the nature of your project, you might want to use a
modified subdirectory structure here. For example, if you're doing
several numerical experiments then you might want to arrange your
outputs by experiment, rather than by output type.

\subsection{reports}\label{reports}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}29.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-29.png}

The reports directory is then where everything comes together. This is
where the written documents that form the final deliverables of your
project are created. If these final documents are written in LaTeX or
markdown, both the source and the compiled documents can be found within
this directory.

When including content in this report, for example figures, I'd
recommend against making copies of those figure files within the reports
directory. If you do that, then you'll have to manually update the files
every time you modify them. Instead you can use relative file paths to
include these figures. Relative file paths specify how to get to the
image, starting from your TeX document and moving up and down through
the levels of your project directory.

For example when including a figure in the annual report you might use
the following code to include a figue within the markdown source code.

\begin{verbatim}
![](../../outputs/2021-annual-report/figures/2021-student-survey-histogram.png)
\end{verbatim}

While in the spending review, the company logo might be included in the
LaTeX source code as follows.

\begin{verbatim}
\includegraphics[width=0.2\textwidth]{images/logo.svg}
\end{verbatim}

If you're not using markdown or LaTeX to write your reports, but instead
use an online platform like \href{https://www.overleaf.com/}{Overleaf}
as a LaTeX editor or \href{https://docs.google.com/}{Google Docs} to
write collaboratively then add links to these in the reports directory
by adding additional readme files. Make sure you set the read and write
permissions for those links appropriately, too!

When using these online writing systems, you'll have to manually upload
and update your plots and other outputs whenever you modify any of your
earlier analysis. That's one of the drawbacks of these online tools that
has to be traded off against their ease of use.

In our exciting new project, the annual report is written in a markdown
format, which is compiled to both HTML and PDF. The spending review is
written in LaTeX and we only have the source for it, we don't have the
compiled pdf version of the document but should be able to create this
for ourselves using the materials provided.

\subsection{make file}\label{make-file}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}32.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-32.png}

The final element of our template project structure is a
\texttt{makefile}. We aren't going to cover how to read or write make
files in this course. Instead, I'll give you a brief description of what
one is and what it is supposed to do.

At a high level, the \texttt{makefile} is just another text file. What
makes it special is what it contains. Similar to a shell or a bash
script, a makefile contains code that can be run at the command line.
This code will create or update each element of your project.

The \texttt{makefile} makes this easier by defining shorthand commands
for the full lines of code that create each element of your project
(running R scripts, compiling LaTeX documents and so on). The
\texttt{makefile} also records the order in which these operations have
to happen and which of these steps are dependent on one another. This
means that if a single step part of your project is updated then any
changes will be propagated through your entire project. This is done in
quite a clever way, so the only part of your projects that are re-run
are those which need to be updated.

We're omitting the writing of makefiles from this course not because
they're fiendishly difficult to write or read, but rather because they
require a reasonable foundation in working at the command line to be
understood. What I suggest you do instead throughout this course is to
create your own \texttt{make.R} or \texttt{make.md} file. This file will
define the intended running order and dependencies of your project and
if it is an R file might also automate some parts of your analysis.

If you'd like to learn more about makefiles, some good resoruces are:

\begin{itemize}
\tightlist
\item
  \href{https://monashbioinformaticsplatform.github.io/2017-11-16-open-science-training/topics/automation.html}{Monash
  Informatics Platform} (Motivates makefiles via compiling Rmd to
  multiple output formats)
\item
  \href{https://robjhyndman.com/hyndsight/makefiles/}{Rob Hyndman blog
  post} (Old, but motivates use well)
\item
  \href{https://makefiletutorial.com/\#getting-started}{Makefile
  tutorial} (Pretty and current but extensive and with a CS focus)
\end{itemize}

\section{Wrapping up}\label{wrapping-up}

Wrapping up then, that's everything for this chapter.

We have introduced a project structure that will serve you well as a
baseline for the vast majority of projects in data science.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/101{-}workflows{-}organising{-}your{-}work/directory{-}structure{-}drawings/directory{-}structure{-}drawing{-}33.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-33.png}

In your own work, remember that the key here is standardisation. Working
consistently across projects, a company or a group is more important
than sticking rigidly to the particular structure that I have defined
here.

There are two notable exceptions where you probably don't want to use
this project structure. That's when you're building an app or you're
building a package. These require specific organisation of the files
within your project directory. We'll explore the project structure used
for package development during the live session this week.

\section{Session Information}\label{session-information}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pander}\SpecialCharTok{::}\FunctionTok{pander}\NormalTok{(}\FunctionTok{sessionInfo}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{loaded via a namespace (and not attached):}
\emph{compiler(v.4.4.2)}, \emph{fastmap(v.1.2.0)}, \emph{cli(v.3.6.3)},
\emph{tools(v.4.4.2)}, \emph{htmltools(v.0.5.8.1)},
\emph{rstudioapi(v.0.17.1)}, \emph{yaml(v.2.3.10)},
\emph{Rcpp(v.1.0.14)}, \emph{pander(v.0.6.5)}, \emph{rmarkdown(v.2.29)},
\emph{knitr(v.1.49)}, \emph{jsonlite(v.1.8.9)}, \emph{xfun(v.0.50)},
\emph{digest(v.0.6.37)}, \emph{rlang(v.1.1.4)}, \emph{png(v.0.1-8)} and
\emph{evaluate(v.1.0.3)}

\chapter{Naming Files}\label{workflows-naming}

\section{Introduction}\label{introduction-1}

\begin{quote}
``There are only two hard things in Computer Science: cache invalidation
and naming things.''

\textbf{Phil Karlton, Netscape Developer}
\end{quote}

When working on a data science project we can in principle name
directories, files, functions and other objects whatever we like. In
reality though, using an ad-hoc naming system is likely to cause
confusion, mistakes and headaches. We obviously want to avoid all of
those things in the spirit of being kind to our current colleges and
also to our future selves.

Coming up with good names is a bit of an art form, it's an activity that
you get better at with practice. Another similarity to art is that the
best naming systems don't come from giving data scientists free reign
over their naming system. The best approaches to naming things give you
strong guidelines and boundaries within which to express your creativity
and skill.

In this chapter we'll explore what these boundaries and what we want
them to achieve for us. The content of this chapter is based largely
around a \href{https://speakerdeck.com/jennybc/how-to-name-files}{talk
of the same name} given by Jennifer Bryan and the
\href{https://style.tidyverse.org/}{tidyverse style guide}, which forms
the basis of Google's style guide for R programming.

The core principles we introduce hold across most high-level programming
languages. Just like in natural languages, there are differences in how
the principles are implemented between languages and between different
groups of people who use each language. A fun aspect of this is that
people often develop a code ``accent'' based on their ``native''
programming language.

If you'd like to check out how this guidance translates to Python, check
out the
\href{https://peps.python.org/pep-0008/\#naming-conventions}{naming
conventions section} of the PEP8 style guide.

\section{Naming Files}\label{naming-files}

We'll be begin by focusing in on what we call our files. That is, we'll
first focus on the part of the file name that comes before the dot.
Later in the chapter, we'll cycle back around to discuss file
extensions.

\subsection{What do we want from our file
names?}\label{what-do-we-want-from-our-file-names}

To decide how we name our files, we should first consider what we want
from our file naming system. There are three key properties that that we
would like to satisfy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Machine Readable
\item
  Human Readable
\item
  Order Friendly
\end{enumerate}

Thee first desirable property is for file names to be easily readable by
computers, the second is for the file names to be easily readable by
humans and finally the file names should take advantage of the default
ordering imposed on our files.

This set of current file names is sorely lacking across all of these
properties:

\begin{verbatim}
abstract.docx
Effective Data Science's module guide 2022.docx 
fig 12.png
Rplot7.png
1711.05189.pdf
HR Protocols 2015 FINAL (Nov 2015).pdf
\end{verbatim}

We want to provide naming conventions to move us toward the better file
names listed below.

\begin{verbatim}
2015-10-22_human-resources-protocols.pdf
2022_effective-data-science-module-guide.docx
2022_RSS-conference-abstract.docx 
fig12_earthquake-timeseries.png 
fig07_earthquake-location-map.png
ogata_1984_spacetime-clustering.pdf
\end{verbatim}

Let's take a few minutes to examine what exactly we mean by each of
these properties.

\subsection{Machine Readable}\label{machine-readable}

What do we mean by machine readable file names?

\begin{itemize}
\tightlist
\item
  Easy to compute on by \emph{deliberate use of delimiters}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{underscores\_separate\_metadata},
    \texttt{hyphens-separate-words}.
  \end{itemize}
\item
  Play nicely with \emph{regular expressions} and \emph{globbing}:

  \begin{itemize}
  \tightlist
  \item
    avoid spaces, punctuation, accents, cases;
  \item
    \texttt{rm\ Rplot*.png}
  \end{itemize}
\end{itemize}

Machine readable names are useful when:

\begin{itemize}
\item
  \emph{managing files}: ordering, finding, moving, deleting:
\item
  \emph{extracting information} directly from file names,
\item
  \emph{working programmatically} with file names and regex.
\end{itemize}

\subsubsection{Computing on File Names}\label{computing-on-file-names}

When we are operating on a large number of files it is useful to be able
to work with them programmatically.

One example of where this might be useful is when downloading and
marking assessments. This requires me to download and unzip a large
number of compressed folders, copying the pdf report from each unzipped
folder into a single directory and then repeat this, moving all of the R
scripts from each unzipped folder into a second directory. The marked
scripts and code then need to be paired back up in folders named by
student, and re-zipped ready to be returned.

This is \emph{monotonously} dull and might work for \textasciitilde50
students but would not scale to a class of \textasciitilde5000. Working
programmatically with files is the way to get this job done efficiently.
This requires the file names to play nicely with the way that computers
interpret file names, which they regard as a string of characters.

It is often helpful to have some meta-data included in the file name,
for example the student's id number and the assessment title. We will
use an underscore (\texttt{\_}) to separate elements of meta-data within
the file name and a hyphen (\texttt{-}) to separate sub-elements of
meta-data, for example words within the assessment title.

\begin{verbatim}
001562679_assessment-1.tex
001562679_assessment-1.pdf
001562679_assessment-1.r
\end{verbatim}

\subsubsection{Regular expressions and
Globbing}\label{regular-expressions-and-globbing}

\href{https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html}{Regular
expressions} and
\href{https://en.wikipedia.org/wiki/Glob_(programming)}{globbing} are
two ideas from string manipulation that you may not have met, but which
will inform our naming conventions.

Regular expressions allow you to search for strings (in our case file
names) that match a particular pattern. Regular expressions can do
really complicated searches but become gnarly when you have to worry
about special characters like spaces, punctuation, accents and cases, so
these should be avoided in file names.

A special type of regular expression is called a glob. The most common
use of globbing is the use of an asterisk (\texttt{*}) to replace any
number of subsequent characters in a file name. Globbing becomes
particular powerful when you use a consistent structure to create your
file names. We might, for example, delete all \texttt{png} images in our
wokring directory that begin with Rplot using a single line of code at
the command line: \texttt{rm\ Rplot*.png}.

Having machine readable file names is particularly useful when managing
files, such as ordering, finding, moving or deleting them. Another
common use case is when an analysis requires you to load a large number
of individual data files. Machine readable file names are also useful in
this setting for extracting meta-information from files without having
to open them in memory. This is particularly useful when the files might
be too large to load into memory, or you only want to load data from a
certain year.

The final benefit we list here is the scalability, reduction in drudgery
and lowered risk for human error when operating on a very large number
of files.

\subsection{Order Friendly}\label{order-friendly}

The next property we will focus on also links to how computers operate.
We'd like our file names to exploit the default orderings used by
computers. This means starting file names with character strings or
metadata that allow us order our files in some meaningful way.

\subsubsection{Running Order}\label{running-order}

One example of this is where there's some logical order in which your
code should be executed, as in the example analysis below.

\begin{verbatim}
diagnositc-plots.R
download.R
runtime-comparison.R
...
model-evaluation.R
wrangle.R
\end{verbatim}

Adding numbers to the start of these file names can make the intended
ordering immediately obvious.

\begin{verbatim}
00_download.R
01_wrangle.R
02_model.R
...
09_model-evaluation.R
10_model-comparison-plots.R
\end{verbatim}

Starting single digit numbers with a leading 0 is a very good idea here
to prevent script 1 being sorted in with the tens, script 2 in with the
twenties and so on. If you might have over 100 files, for example when
saving the output from many simulations, use two or more zeros to
maintain this nice ordering.

\subsubsection{Date Order}\label{naming-things-date-order}

A second example of orderable file names is when the file has a date
associated with it. This might be a version of a report or the date on
which some data were recorded, cleaned or updated.

\begin{verbatim}
2015-10-22_human-resources-protocols.pdf
...
...
2022-effective-data-science-module-guide.docx
\end{verbatim}

When using dates, in file names or elsewhere, you should conform to the
\href{https://en.wikipedia.org/wiki/ISO_8601}{ISO standard date format}.

\begin{quote}
ISO 8601 sets an international standard format for dates:
\texttt{YYYY-MM-DD}.
\end{quote}

This format uses four numbers for the year, followed by two numbers for
the month and two numbers of the day of the month. This structure
mirrors a nested file structure moving from least to most specific. It
also avoids confusion over the ordering of the date elements. Without
using the ISO standard a date like \texttt{04-05-22} might be
interpreted as the fourth of May 2022, the fifth of April 2022, or the
twenty-second of May 2004.

\subsection{Human Readable}\label{human-readable}

The final property we would like our file names to have is human
readability. This requires the names of our files to be meaningful,
informative and easily read by real people.

The first two of these are handled by including appropriate metadata in
the file name. The ease with which these are read by real people is
determined by the length of the file name and by how that name is
formatted.

There are lots of formatting options with fun names like
\texttt{camelCase}, \texttt{PascalCase}, and \texttt{snake\_case}.

\begin{verbatim}
   easilyReadByRealPeople (camelCase)
   EasilyReadByRealPeople (PascalCase)
   easily_read_by_real_people (snake_case)
   easily-read-by-real-people (skewer-case)
\end{verbatim}

There is weak evidence to suggest that snake case and skewer case are
most the readable. We'll use a mixture of these two, using snake case
\emph{between} metadata items and skewer case \emph{within} them. This
has a slight cost to legibility, in a trade-off against making computing
on these file names easier.

The final aspect that you have control over is the length of the name.
Having short, evocative and useful file names is not easy and is a skill
in itself. For some hints and tips you might want to look into tips for
writing
\href{https://stackoverflow.com/questions/4230846/what-is-the-etymology-of-slug-in-a-url}{URL
slugs}. These are last part of a web address that are intended to
improve accessibility by being immediately and intuitively meaningful to
any user.

\subsection{Naming Files - Summary}\label{naming-files---summary}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  File names should be meaningful, informative and scripts end in
  \texttt{.r}
\item
  Stick to letters, numbers underscores (\texttt{\_}) and hyphens
  (\texttt{-}).
\item
  Pay attention to capitalisation \texttt{file.r} \(\neq\)
  \texttt{File.r} on all operating systems.
\item
  Show order with left-padded numbers or ISO dates.
\end{enumerate}

\section{File Extensions and Where You
Work}\label{file-extensions-and-where-you-work}

So far we have focused entirely on what comes before the dot, that is
the file name. Equally if not more important is the file extension that
comes after the dot.

\begin{verbatim}
example-script.r
example-script.py

project-writeup.doc
project-writeup.tex
\end{verbatim}

The file extension describes how information is stored in that file and
determines the software that can use, view or run that file.

You likely already use file extensions to distinguish between code
scripts, written documents, images, and notebook files. We'll now
explore the benefits and drawbacks of various file types with respect to
several important features.

\subsection{Open Source vs Proprietary File
Types}\label{open-source-vs-proprietary-file-types}

The first feature to consider is whether the file type is open source
and can be used by anyone without charge or if specialist software must
be paid for to interact with those files.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image\_path }\OtherTok{\textless{}{-}} \StringTok{"images/102{-}workflows{-}naming{-}files/file{-}types{-}image.png"}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\AttributeTok{path =}\NormalTok{ image\_path)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=4.8in,height=\textheight]{images/102-workflows-naming-files/file-types-image.png}

In the figure above, each column represents a different class of file,
moving left to right we have example file types for tabular data,
list-like data and text documents. File types closer to the top are open
source while those lower down rely on proprietary software, which may or
may not require payment.

To make sure that our work is accessible to as many people as possible
we should favour the open source options like csv files over Google
sheets or excel, JSON files over Matlab data files, and tex or markdown
over a word or Google doc.

This usually has a benefit in terms of project longevity and
scalability. This is because open source file types are often somewhat
simpler in structure, making them more robust to changes over time and
less memory intensive.

To see this, let's take a look inside some data files.

\subsection{Inside Data Files}\label{inside-data-files}

\subsubsection{Inside a CSV file}\label{inside-a-csv-file}

CSV or comma separated value files are used to store tabular data.

In tabular data, each row of the data represents one record and each
column represents a data value. A csv encodes this by having each record
on a separate line and using commas to separate values with that record.
You can see this by opening a csv file in a text editor such as notepad.

The raw data stores line breaks using \texttt{\textbackslash{}n} and
indicates new rows by \texttt{\textbackslash{}r}. These backslashed
indicae that these are \emph{escape characters} with special meanings,
and should not be literally interpreted as the letters n and r.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{read\_file}\NormalTok{(}\AttributeTok{file =} \StringTok{"images/102{-}workflows{-}naming{-}files/example.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Name,Number\r\nA,1\r\nB,2\r\nC,3"
\end{verbatim}

When viewed in a text editor, the example file would look something like
this.

\begin{verbatim}
Name,Number 
A,1
B,2
C,3
\end{verbatim}

\subsubsection{Inside a TSV file}\label{inside-a-tsv-file}

TSV or tab separated value files are also used to store tabular data.

Like in a csv each record is given on a new line but in a tsv tabs
rather than commas are used to separate values with each record. This
can also be seen by opening a tsv file in a text editor such as notepad.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{read\_file}\NormalTok{(}\AttributeTok{file =} \StringTok{"images/102{-}workflows{-}naming{-}files/example.tsv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Name\tNumber\r\nA\t1\r\nB\t2\r\nC\t3"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Name    Number }
\NormalTok{A   1}
\NormalTok{B   2}
\NormalTok{C   3}
\end{Highlighting}
\end{Shaded}

One thing to note is that tabs are a separate character and are not just
multiple spaces. In plain text these can be impossible to tell apart, so
most text editors have an option to display tabs differently from
repeated spaces, though this is usually not enabled by default.

\subsubsection{Inside an Excel file}\label{inside-an-excel-file}

When you open an excel file in a text editor, you will immediately see
that this is not a human interpretable file format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{504b 0304 1400 0600 0800 0000 2100 62ee}
\NormalTok{9d68 5e01 0000 9004 0000 1300 0802 5b43}
\NormalTok{6f6e 7465 6e74 5f54 7970 6573 5d2e 786d}
\NormalTok{6c20 a204 0228 a000 0200 0000 0000 0000}
\NormalTok{0000 0000 0000 0000 0000 0000 0000 0000}
\NormalTok{.... .... .... .... .... .... .... ....}
\NormalTok{0000 0000 0000 0000 ac92 4d4f c330 0c86}
\NormalTok{ef48 fc87 c8f7 d5dd 9010 424b 7741 48bb}
\NormalTok{2154 7e80 49dc 0fb5 8da3 241b ddbf 271c}
\NormalTok{1054 1a83 0347 7fbd 7efc cadb dd3c 8dea}
\NormalTok{.... .... .... .... .... .... .... ....}
\end{Highlighting}
\end{Shaded}

Each entry here is a four digit hexadecimal number and there are a lot
more of them than we have entries in our small table.

This is because excel files can carry a lot of additional information
that a csv or tsv are not able to, such as cell formatting or having
multiple tables stored within a single file (called sheets by excel).

This means that excel files take up much more memory because they are
carrying a lot more information than is strictly contained within the
data itself.

\subsubsection{Inside a JSON file}\label{inside-a-json-file}

JSON, or Java Script Object Notation, files are an open source format
for list-like data. Each record is represented by a collection of
\texttt{key:value} pairs. In our example table each entry has two
fields, one corresponding to the \texttt{Name} key and one corresponding
to the \texttt{Number} key.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{[}\FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \StringTok{"A"}\FunctionTok{,}
    \DataTypeTok{"Number"}\FunctionTok{:} \StringTok{"1"}
\FunctionTok{\}}\OtherTok{,} \FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \StringTok{"B"}\FunctionTok{,}
    \DataTypeTok{"Number"}\FunctionTok{:} \StringTok{"2"}
\FunctionTok{\}}\OtherTok{,} \FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \StringTok{"C"}\FunctionTok{,}
    \DataTypeTok{"Number"}\FunctionTok{:} \StringTok{"3"}
\FunctionTok{\}}\OtherTok{]}
\end{Highlighting}
\end{Shaded}

This list-like structure allows non-tabular data to be stored by using a
property called nesting: the value taken by a key can be a single value,
a vector of values or another list-like object.

This ability to create nested data structures has lead to this data
format being used widely in a range of applications that require data
transfer.

\subsubsection{Inside an XML file}\label{inside-an-xml-file}

XML files are another open source format for list-like data, where each
record is represented by a collection of \texttt{key:value} pairs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{\textless{}?xml}\NormalTok{ version="1.0" encoding="UTF{-}8"}\KeywordTok{?\textgreater{}}
\DataTypeTok{\textless{}}\KeywordTok{root}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{row}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{Name}\DataTypeTok{\textgreater{}}\NormalTok{A}\DataTypeTok{\textless{}/}\KeywordTok{Name}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{Number}\DataTypeTok{\textgreater{}}\NormalTok{1}\DataTypeTok{\textless{}/}\KeywordTok{Number}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}/}\KeywordTok{row}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{row}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{Name}\DataTypeTok{\textgreater{}}\NormalTok{B}\DataTypeTok{\textless{}/}\KeywordTok{Name}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{Number}\DataTypeTok{\textgreater{}}\NormalTok{2}\DataTypeTok{\textless{}/}\KeywordTok{Number}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}/}\KeywordTok{row}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{row}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{Name}\DataTypeTok{\textgreater{}}\NormalTok{C}\DataTypeTok{\textless{}/}\KeywordTok{Name}\DataTypeTok{\textgreater{}}
    \DataTypeTok{\textless{}}\KeywordTok{Number}\DataTypeTok{\textgreater{}}\NormalTok{3}\DataTypeTok{\textless{}/}\KeywordTok{Number}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}/}\KeywordTok{row}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}/}\KeywordTok{root}\DataTypeTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

The difference from a JSON file is mainly in how those records are
formatted within the file. In a JSON file this is designed to look like
objects in the Java Script programming language and in XML the
formatting is done to look like HTML, the markup language used to write
websites.

\subsection{A Note on Notebooks}\label{a-note-on-notebooks}

\begin{itemize}
\item
  There are two and a half notebook formats that you are likely to use:
  \texttt{.rmd}, \texttt{.ipynb} or alternatively \texttt{.qmd}.
\item
  R markdown documents \texttt{.rmd} are plain text files, so are very
  human friendly.
\item
  \textbf{\emph{JuPy}}te\textbf{\emph{R}} notebooks have multi-language
  support but are not so human friendly (JSON in disguise).
\item
  Quarto documents offer the best of both worlds and more extensive
  language support. Not yet as established as a format.
\end{itemize}

In addition to the files you read and write, the files that you code in
will largely determine your workflow.

There are three main options for the way that you code. You might typing
your code directly at the command line, you might using a text editor or
IDE to write scripts or you might use a notebook file that mixes code,
text and output together in a single document.

We'll compare these methods of working soon, but first let's do a quick
review of what notebooks are available to you and why you might want to
use them.

As a data scientist, there are two and a half notebook formats that
you're likely to have met before. The first two are Rmarkdown files for
those working predominantly in R and interactive Python or JuPyteR
notebooks for those working predominantly in Python. The final half
format are quarto markdown documents, which are relatively new and
extend the functionality of Rmarkdown files to provide multi-language
support.

The main benefit of Rmarkdown documents is that they're plain text
files, so they're very human friendly and work very well with version
control software like git. \textbf{\emph{JuPy}}te\textbf{\emph{R}}
notebooks have the benefit of supporting code written in Julia, Python
or R, but are not so human friendly - under the hood these documents are
JSON files that should not be edited directly (because a misplaced
bracket will break them!).

Quarto documents offer the best of both worlds, with plain text
formatting and even more extensive language support than jupyter
notebooks. Quarto is a recent extension of Rmarkdown, which is rapidly
becoming popular in the data science community. Quarto also allows you
to create a wider range of documents, including websites, these course
notes and the associated slides.

Each format has its benefits and drawbacks depending on the context in
which they are used and all have some shared benefits and limitations by
nature of them all being notebook documents.

\subsection{File Extensions and Where You
Code}\label{file-extensions-and-where-you-code}

\begin{longtable}[]{@{}lccc@{}}
\toprule\noalign{}
Property & Notebook & Script & Command Line \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
reproducible & \textasciitilde{} &  & X \\
readable & \textasciitilde{} &  & \textasciitilde{} \\
self-documenting &  & X & X \\
in production & X &  & \textasciitilde{} \\
ordering / automation & \textasciitilde{} &  & \textasciitilde{} \\
\end{longtable}

The main benefit of notebook documents is that they are
self-documenting: they can mix the documentation, code and report all
into a single document. Notebooks also provide a level of interactivity
when coding that is not possible when working directly at the command
line or using a text editor to write scripts. This limitation is easily
overcome by using an integrated development environment when scripting,
rather than a plain text editor.

Writing code in \texttt{.r} files is not self-documenting but this
separation of code, documentation and outputs has many other benefits.
Firstly, the resulting scripts provide a reproducible and automatable
workflow, unlike one-off lines of code being run at the command line.
Secondly, using an IDE to write these provides you with syntax
highlighting and code linting features to help you write readable and
accurate code. Finally, the separation of code from documentation and
output allows your work to be more easily or even directly put into
production.

In this course we will generally advocate for a scripting-first approach
to data science, though notebooks and command line work definitely have
their place.

Notebooks are great as a teaching tool, for small reports and for rapid
prototyping but they have strong limitations with being put into
production. Conversely, coding directly at the command line is perfect
for simple one-time tasks but it leaves no trace of your workflow and
leads to an analysis that cannot be easily replicated in the future.

\subsection{Summary}\label{summary}

Finally, let's wrap things up by summarising what we have learned about
naming files.

Before the dot we want to pick file names that machine readable, human
friendly and play nicely with the default orderings provided to us.

\begin{quote}
\textbf{Name files so that they are:}

\begin{itemize}
\tightlist
\item
  Machine Readable,
\item
  Human Readable,
\item
  Order Friendly.
\end{itemize}
\end{quote}

After the dot, we want to pick file types that are widely accessible,
easily read by humans and allow for our entire analysis to be
reproduced.

\begin{quote}
\textbf{Use document types that are:}

\begin{itemize}
\tightlist
\item
  Widely accessible,
\item
  Easy to read and reproduce,
\item
  Appropriate for the task at hand.
\end{itemize}
\end{quote}

Above all we want to name our files and pick our file types to best
match with the team we are working in and the task that is at hand.

\section{Session Information}\label{session-information-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pander}\SpecialCharTok{::}\FunctionTok{pander}\NormalTok{(}\FunctionTok{sessionInfo}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{other attached packages:} readr(v.2.1.5)

\textbf{loaded via a namespace (and not attached):}
\emph{digest(v.0.6.37)}, \emph{R6(v.2.5.1)}, \emph{fastmap(v.1.2.0)},
\emph{xfun(v.0.50)}, \emph{tzdb(v.0.4.0)}, \emph{magrittr(v.2.0.3)},
\emph{glue(v.1.8.0)}, \emph{tibble(v.3.2.1)}, \emph{knitr(v.1.49)},
\emph{pkgconfig(v.2.0.3)}, \emph{htmltools(v.0.5.8.1)},
\emph{png(v.0.1-8)}, \emph{rmarkdown(v.2.29)},
\emph{lifecycle(v.1.0.4)}, \emph{cli(v.3.6.3)}, \emph{pander(v.0.6.5)},
\emph{vctrs(v.0.6.5)}, \emph{compiler(v.4.4.2)},
\emph{rstudioapi(v.0.17.1)}, \emph{tools(v.4.4.2)}, \emph{hms(v.1.1.3)},
\emph{pillar(v.1.10.1)}, \emph{evaluate(v.1.0.3)},
\emph{Rcpp(v.1.0.14)}, \emph{yaml(v.2.3.10)}, \emph{rlang(v.1.1.4)} and
\emph{jsonlite(v.1.8.9)}

\chapter{Code}\label{workflows-code}

\section{Introduction}\label{introduction-2}

We have already described how we might organise an effective data
science project at the directory and file level. In this chapter we will
delve one step deeper and consider how we can structure our work within
those files.

In particular, we'll focus on code files here. We'll start by comparing
the two main approaches to structuring our code, called functional
programming and object oriented programming. We'll then consider
conventions on how we should order code within our scripts and how we
name the functions and objects we create.

To round up this chapter, we'll summarise the main points from the R
style guide that we will be following in this course and highlight some
useful packages for writing effective code.

\section{Functional Programming}\label{functional-programming}

A functional programming style has two major properties:

\begin{itemize}
\item
  Object immutability,
\item
  Complex programs are written using function composition.
\end{itemize}

The first point states that the original data or objects should never be
modified or altered by the code that we write. We've met this idea
before when making new, cleaner versions of our raw data while taking
care to leave the original messy data intact. Object immutability
extends this same idea to cover our code as well as our data.

The second point encapsulates that in functional programming complex
problems are solved by decomposing them into a series of smaller
problems. A separate, self-contained function is then written to solve
each sub-problem. This means that each individual function is simple and
easy to understand. This makes each of these small functions easy to
test and to reuse in many places thoughout our analysis. Code complexity
is then built up by composing these functions in various ways.

It can be difficult to get into this way of thinking, but people with
mathematical training often find it quite natural. This is because
mathematicians have many years experience of working with function
compositions in the abstract, mathematical sense.

\[y = g(x) = f_3 \circ f_2 \circ f_1(x).\]

\subsection{The Pipe Operator}\label{the-pipe-operator}

One issue with functional programming is that lots of nested functions
means that there are also lots of nested brackets. These start to get
tricky to keep track of when you have upwards of 3 functions being
composed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{log}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\FunctionTok{cos}\NormalTok{(}\FunctionTok{sin}\NormalTok{(pi))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

This reading difficulty is only exacerbated if your functions have
additional arguments on top of the original inputs.

The pipe operator \texttt{\%\textgreater{}\%} from the
\texttt{\{magrittr\}} package helps with this issue. It works exactly
like function composition: it takes the whatever is on the left (whether
that is an existing object or the output of a function) and passes it to
the following function call as the first argument of that function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{pi }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{sin}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cos}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{exp}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{log}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
\end{verbatim}

The pipe operator is often referred to as ``syntactic sugar''. This is
because it doesn't add anything to your code in itself, but rather it
makes your code \emph{so} much more palatable to read.

In R versions 4.1 and greater, there's a built-in version of this pipe
operator,\texttt{\textbar{}\textgreater{}}. This is written using the
vertical bar symbol followed by a greater than sign. You can usually
find this vertical bar above backslash on the keyboard. (Just to cause
confusion, the vertical bar symbol is also called the pipe symbol and
performs a similar operation in other programming langauges.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{pi }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sin}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cos}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{exp}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{log}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
\end{verbatim}

The base R pipe usually behaves in the same way as the pipe from
magrittr, but there are a few cases where they differ. For reasons of
back-compatibility and consistency we'll stick to the
\texttt{\{magrittr\}} pipe in this course. (For an outline of some of
the differences between these two pipes, check out this
\href{https://ivelasq.rbind.io/blog/understanding-the-r-pipe/}{blog
post} by Isabella Velasquez)

\subsection{When not to pipe}\label{when-not-to-pipe}

Pipes are designed to put focus on the the actions you are performing
rather than the object that you are preforming those operations on. This
means that there are two cases where you should almost certainly not use
a pipe.

The first of these is when you need to manipulate more than one object
at a time. Using secondary objects as reference points (but leaving them
unchanged) is of course perfectly fine, but pipes should be used when
applying a sequence of steps to create a new, modified version of one
primary object.

Secondly, just because you \emph{can} chain together many actions into a
single pipeline that doesn't mean you necessarily should do so. Very
long sequences of piped operations are easier to read than nested
functions but they still burden the reader with the same cognitive load
on their short-term memory. Be kind to them and create meaningful
intermediate objects with informative names. This will help the reader
to more easily understand the logic within your code.

\section{Object Oriented Programming}\label{object-oriented-programming}

The main alternative to functional programming is object oriented
programming.

\begin{quote}
TL;DR

\begin{itemize}
\tightlist
\item
  OOP solves complex problems by using lots of simple objects
\item
  R has 3 OOP systems: S3, S4 and R6.
\item
  Objects belong to a class, have methods and fields.
\item
  Example: agent based simulation of beehive.
\end{itemize}
\end{quote}

\subsection{OOP Philosophy}\label{oop-philosophy}

In functional programming, we solve complicated problems by using lots
of simple functions. In object oriented programming we solve complicated
problems using lots of simple objects. Which of these programming
approaches is best will depend on the particular type of problem that
you are trying to solve.

Functional programming is excellent for most types of data science work.
Object oriented comes into its own when your problem has many small
components interacting with one another. This makes it great for things
like designing agent-based simulations, which I'll come back to in a
moment.

In R there are three different systems for doing object oriented
programming (called S3, S4, and R6), so things can get a bit
complicated. We won't go into detail about them here, but I'll give you
an overview of the main ideas.

This approach to programming might be useful for you in the future, for
example if you want to extend base R functions to work with new types of
input, and to have user-friendly displays. In that case
\href{https://adv-r.hadley.nz/}{Advanced R} by Hadley Wickham is an
excellent reference text.

In OOP, each object belongs to a class and has a set of methods
associated with it. The class defines what an object \emph{is} and
methods describe what that object can \emph{do}. On top of that, each
object has class-specific attributes or data fields. These fields are
shared by all objects in a class but the values that they take give
information about that specific object.

\subsection{OOP Example}\label{oop-example}

This is is all sounding very abstract. Let's consider writing some
object oriented code to simulate a beehive. Each object will be a bee,
and each bee is an instance of one of three bee classes: it might be a
\texttt{queen}, a \texttt{worker} or a \texttt{drone}. Different bee
classes have different methods associated with them, which describe what
the bee can do, for example all bees would have 6 methods that let them
move \texttt{up}, \texttt{down}, \texttt{left}, \texttt{right},
\texttt{forward} and \texttt{backward} within the hive. An additional
\texttt{reproduce} method might only be defined for queen bees and a
\texttt{pollinate} method might only be defined for workers. Each
instance of a bee has then has its own fields, which give data about
that specific bee. All bees have \texttt{x}, \texttt{y} and \texttt{z}
coordinate fields that give the bee's current location within the hive.
These fields can then be altered by using the movement methods defined
above. The queen class might have an additional field, counting its
number of offspring and the workers might have an additional field for
how much pollen they are carrying.

As the simulation progresses, methods are applied to each object
altering their fields and potentially creating or destroying objects.
This is very different from the preservation mindset of functional
programming, but hopefully you can see that it is a very natural
approach to many types of problem.

\section{Structuring R Script
Headers}\label{structuring-r-script-headers}

\begin{quote}
TL;DR

\begin{itemize}
\tightlist
\item
  Start script with a comment of 1-2 sentences explaining what it
  \textgreater{} does.
\item
  \texttt{setwd()} and \texttt{rm(ls())} are the devil's work.
\item
  ``Session'' \textgreater{} ``Restart R'' or Keyboard shortcut:
  crtl/cmd + shift \textgreater{} + 0
\item
  Polite to gather all \texttt{library()} and \texttt{source()} calls.
\item
  Rude to mess with other people's set up using
  \texttt{install.packages()}.
\item
  Portable scripts use paths relative to the root directory of the
  project.
\end{itemize}
\end{quote}

First things first, let's discuss what should be at the top of your R
scripts.

It is almost always a good idea to start your file with a few commented
out sentences describing the purpose of the script and, if you work in a
large team, perhaps who contact with any questions about this script.
(There is more on comments coming up soon, don't worry!)

It is also good practise to move all \texttt{library()} and
\texttt{source()} calls to the top of your script. These indicate the
packages and helper function that are dependencies of your script; it's
useful to know what you need to have installed before trying to run any
code.

That segues nicely to the next point, which is never to hard code
package installations. It is extremely bad practise and very rude to do
so because then your script might alter another person's R installation.
If you don't know already, this is precisely the difference between an
\texttt{install.packages()} and \texttt{library()} call:
\texttt{install.packages()} will download the code for that package to
the users computer, while \texttt{library()} takes that downloaded code
and makes it available in the current R session. To avoid messing with
anyone's R installation, you should always type
\texttt{install.package()} commands directly in the console and then
place the corresponding \texttt{library()} calls within your scripts.

Next, it is likely that you, or someone close to you, will commit the
felony of starting every script by setting the working directory and
clearing R's global environment. This is \emph{very} bad practice, it's
indicative of a workflow that's not project based and it's problematic
for at least two reasons. Firstly, the path you set will likely not work
on anyone else's computer. Secondly, clearing the environment like this
may \emph{look} like it gets you back to fresh, new R session but all of
your previously loaded packages will still be loaded and lurking in the
background.

Instead, to achieve your original aim of starting a new R session, go to
the menu and select the ``Session'' drop down then select ``Restart R''.
Alternatively, you can use keyboard shortcuts to do the same. This is
``crtl + shift + 0'' on Windows and ``cmd + shift + 0'' on a mac. The
fact that a keyboard shortcut exists for this should quite strongly hint
that, in a reproducible and project oriented workflow, you should be
restarting R quite often in an average working day. This is the
scripting equivalent of ``clear all output and rerun all'' in a
notebook.

Finally, let's circle back to the point I made earlier about setting the
working directory. This is fragile because you are likely giving file
paths that are specific to your computer, your operating system and your
file organisation system. The chances of someone else having all of
these the same are practically zero.

\section{\texorpdfstring{Portable File paths with
\texttt{\{here\}}}{Portable File paths with \{here\}}}\label{portable-file-paths-with-here}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad {-} breaks if project moved}
\FunctionTok{source}\NormalTok{(}\StringTok{"zaks{-}mbp/Desktop/exciting{-}new{-}project/src/helper\_functions/rolling\_mean.R"}\NormalTok{)}

\CommentTok{\# Better {-} breaks if Windows (older)}
\FunctionTok{source}\NormalTok{(}\StringTok{"../../src/helper\_functions/rolling\_mean.R"}\NormalTok{)}

\CommentTok{\# Best {-} but use here:here() to check root directory correctly identified}
\FunctionTok{source}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"src"}\NormalTok{,}\StringTok{"helper\_functions"}\NormalTok{,}\StringTok{"rolling\_mean.R"}\NormalTok{))}

\CommentTok{\# For more info on the \{here\} package:}
\FunctionTok{vignette}\NormalTok{(}\StringTok{"here"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To fix the problem of person- and computer-specific file paths you can
have two options.

The first is to use \emph{relative} file paths. In this you assume that
each R script is being run in its current location and my moving up and
down through the levels of your project directory you point to the file
that you need.

This is good in that it solves the problem of paths breaking because you
move the project to a different location on your own laptop. However, it
does not fully solve the portability problem because you might move your
file to a different location \emph{within} the same project. It also
does not solve the problem that windows uses MacOS and linux use forward
slashes in file paths with widows uses backslashes.

To resolve these final two issues I recommend using the \texttt{here()}
function from the \{here\} package. This package looks for a
\texttt{.Rproj} or \texttt{.git} file to identify the root directory of
your project. It then creates file paths relative to the root of your
project that are suitable for the operating system on which the code is
being run. It really is quite marvellous.

For more information on how to use the here package, explore its chapter
in \href{https://rstats.wtf/project-oriented-workflow.html}{R - What
They Forgot}, \href{https://r4ds.had.co.nz/workflow-projects.html}{R for
Data Science} or this
\href{https://www.tidyverse.org/blog/2017/12/workflow-vs-script/}{project
oriented workflow blog post}.

\section{Code Body}\label{code-body}

We will moving on now from the head of the code to the body.

Having well named and organised code will do most of the work of helping
people to read and understand your code. Comments and sectioning do the
rest of the work.

Note, this section is designed as an introduction to the
\href{https://style.tidyverse.org/}{tidyverse style guide} and not as a
replacement to it.

\subsection{Comments}\label{comments}

Comments may be either short in-line comments at the end of a line or
full lines dedicated to comments. To create either type of comment in R,
simply type hash followed by one space. The rest of that line will not
be evaluated and will function as a comment. If multi-line comments are
needed simply start multiple lines with a hash and a space.

Comments can also be used to add structure to your code, buy using
commented lines of hyphens and equal signs to chunk your files into
minor and major sections.

Markdown-like section titles can be added to these section and
subsection headers. Many IDEs, such as RStudio, will interpret these as
a table of contents for you, so that you can more easily navigate your
code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This is an example script showing good use of comments and sectioning }

\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{source}\NormalTok{(}\FunctionTok{here}\NormalTok{(}\StringTok{"src"}\NormalTok{,}\StringTok{"helper\_functions"}\NormalTok{,}\StringTok{"rolling\_mean.R"}\NormalTok{))}

\CommentTok{\#=============================================================================== }
\CommentTok{\# Major Section on Comments {-}{-}{-}{-}}
\CommentTok{\#===============================================================================}

\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\#  Minor Section on inline comments {-}{-}{-}{-} }
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10} \CommentTok{\# this is an inline comment}

\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\#  Minor Section on full line comments {-}{-}{-}{-} }
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\FunctionTok{rolling\_mean}\NormalTok{(x)}
\CommentTok{\# This is an full line comment}
\CommentTok{\# Use 80 characters max for readability}
\end{Highlighting}
\end{Shaded}

\subsection{Objects are Nouns}\label{objects-are-nouns}

\begin{itemize}
\item
  Object names should use only lowercase letters, numbers, and
  \texttt{\_}.
\item
  Use underscores (\texttt{\_}) to separate words within a name.
  (\texttt{snake\_case})
\item
  Use nouns, preferring singular over plural names.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good}
\NormalTok{day\_one}
\NormalTok{day\_1}

\CommentTok{\# Bad}
\NormalTok{first\_day\_of\_the\_month}
\NormalTok{DayOne}
\NormalTok{dayone}
\NormalTok{djm1}
\end{Highlighting}
\end{Shaded}

When creating and naming objects a strong guideline in that objects
should be named using short but meaningful nouns. Names should not
include any special characters and should use underscores to separate
words within the object name.

This is similar to our file naming guide, but note that hyphens can't be
used in object names because this conflicts with the subtraction
operator.

When naming objects, as far as possible use singular nouns. The main
reason for this is that the plurisation rules in English are complex and
will eventually trip up either you or a user of your code.

\subsection{Functions are Verbs}\label{functions-are-verbs}

\begin{itemize}
\item
  Function names should use only lower-case letters, numbers, and
  \texttt{\_}.
\item
  Use underscores (\texttt{\_}) to separate words within a name.
  (\texttt{snake\_case})
\item
  Suggest imperative mood, as in a recipe.
\item
  Break long functions over multiple lines, using 4 rather than the
  usual 2 spaces for indent.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good}
\FunctionTok{add\_row}\NormalTok{()}
\FunctionTok{permute}\NormalTok{()}

\CommentTok{\# Bad}
\FunctionTok{row\_adder}\NormalTok{()}
\FunctionTok{permutation}\NormalTok{()}

\NormalTok{long\_function\_name }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
    \AttributeTok{a =} \StringTok{"a long argument"}\NormalTok{,}
    \AttributeTok{b =} \StringTok{"another argument"}\NormalTok{,}
    \AttributeTok{c =} \StringTok{"another long argument"}\NormalTok{) \{}
  \CommentTok{\# As usual code is indented by two spaces.}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The guidelines for naming functions are broadly similar, with the advice
that functions should be verbs rather than nouns.

Functions should be named in the imperative mood, like in a recipe. This
is again for consistency; having function names in a range of moods and
tenses leads to coding nightmares.

As with object names you should aim to give your functions and their
arguments short, evocative names. For functions with many arguments or a
long name, you might not be able to fit the function definition on a
single line. In this case you can should place each argument on its own
double indented line and the function body on a single indented line.

\subsection{Casing Consistantly}\label{casing-consistantly}

As we have mentioned already, we have many options for separating words
within names:

\begin{itemize}
\tightlist
\item
  \texttt{CamelCase}
\item
  \texttt{pascalCase}
\item
  \texttt{snakecase}
\item
  \texttt{underscore\_separated} 
\item
  \texttt{hyphen-separated}
\item
  \texttt{point.separated} 
\end{itemize}

For people used to working in Python it is tempting to use point
separation in function names, in the spirit of methods from object
oriented programming. Indeed, some base R functions even use this
convention.

However, the reason that we advise against it is because it is already
used for methods in some of R's inbuilt OOP functionality. We will use
underscore separation in our work.

\subsection{Style Guide Summary}\label{style-guide-summary}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use comments to structure your code
\item
  Objects = Nouns
\item
  Functions = Verbs
\item
  Use snake case and consistant grammar
\end{enumerate}

\section{Further Tips for Friendly
Coding}\label{further-tips-for-friendly-coding}

In addition to naming conventions the style guide gives lots of other
guidance on writing code in a way that is kind to future readers of that
code.

I'm not going to go repeat all of that guidance here, but the motivation
for all of these can be boiled down into the following points.

\begin{itemize}
\item
  Write your code to be easily understood by humans.
\item
  Use informative names, typing is cheap.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ dmt) \{}
  \FunctionTok{print}\NormalTok{(i)}
\NormalTok{\}}

\CommentTok{\# Good}
\ControlFlowTok{for}\NormalTok{ (temperature }\ControlFlowTok{in}\NormalTok{ daily\_max\_temperature) \{}
  \FunctionTok{print}\NormalTok{(temperature)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Divide your work into logical stages, human memory is expensive.
\end{itemize}

When writing your code, keep that future reader in mind. This means
using names that are informative and reasonably short, it also means
adding white space, comments and formatting to aid comprehension. Adding
this sort of structure to your code also helps to reduce the cognitive
burden that you are placing on the human reading your code.

Informative names are more important than short names. This is
particularly true when using flow controls, which are things like for
loops and while loops. Which of these for loops would you like to
encounter when approaching a deadline or urgently fixing a bug? Almost
surely the second one, where context is immediately clear.

A computer doesn't care if you call a variable by only a single letter,
by a random key smash (like \texttt{aksnbioawb}) or by an informative
name. A computer also doesn't care if you include no white space your
code - the script will still run. However, doing these things are
friendly practices that can help yourself when debugging and your
co-workers when collaborating.

\section{Reduce, Reuse, Recycle}\label{reduce-reuse-recycle}

In this final section, we'll look at how you can make your workflow more
efficient by reducing the amount of code you write, as well as reusing
and recycling code that you've already written.

\subsection{DRY Coding}\label{dry-coding}

This idea of making your workflow more efficient by reducing, reusing
and recycling your code is summarised by the DRY acronym: don't repeat
yourself.

This can be boiled down to three main points:

\begin{quote}
\begin{itemize}
\tightlist
\item
  if you do something twice in a single script, then write a function to
  do that thing;
\item
  if you want to use your function elsewhere \emph{within} your project,
  then save it in a separate script;
\item
  If you want to use your function \emph{across} projects, then add it
  to a package.
\end{itemize}
\end{quote}

Of course, like with scoping projects in the first place, this requires
some level of clairvoyance: you have to be able to look into the future
and see whether you'll use a function in another script or project. This
is difficult, bordering on impossible. So in practice, this is done
retrospectively - you find a second script or project that needs a
function then pull it out its own separate file or include it in a
package.

As a rule of thumb, if you are having to consider whether or not to make
the function more widely available then you should do it. It takes much
less effort to do this work now, while it's fresh in your mind, than to
have to re-familiarise yourself with the code in several years time.

Let's now look at how to implement those sub-bullet points: ``when you
write a function, document it'' and ``when you write a function, test
it''.

\subsection{Rememer how to use your own
code}\label{rememer-how-to-use-your-own-code}

When you come to use a function written by somebody else, you likely
have to refer to their documentation to teach or to remind yourself of
things like what the expected inputs are and how exactly the method is
implemented.

When writing your own functions you should create documentation that
fills the same need. Even if the function is just for personal use, over
time you'll forget exactly how it works.

\begin{quote}
\emph{When you write a function, document it.}
\end{quote}

But what should that documentation contain?

\begin{itemize}
\tightlist
\item
  Inputs
\item
  Outputs
\item
  Example use cases
\item
  Author (if not obvious or working in a team)
\end{itemize}

Your documentation should describe the inputs and outputs of your
function, some simple example uses. If you are working in a large team,
the documentation should also indicate who wrote the function and who's
responsible for maintaining it over time.

\subsection{\texorpdfstring{\texttt{\{roxygen2\}} for
documentation}{\{roxygen2\} for documentation}}\label{roxygen2-for-documentation}

In the same way that we used the \texttt{\{here\}} package to simplify
our file path problems, we'll use the \texttt{\{roxygen2\}} package to
simplify our testing workflow.

The \{roxygen2\} package gives us an easily insert-able temple for
documenting our functions. This means we don't have to waste our time
and energy typing out and remembering boilerplate code. It also puts our
documentation in a format that allows us to get hints and
auto-completion for our own functions, just like the functions we use
from packages that are written by other people.

To use Roxygen, you only need to install it once - it doesn't need to be
loaded with a library call at the top of your script. After you've done
this, and with your cursor inside a function definition, you can then
insert skeleton code to document that function in one of two ways: you
can either use the Rstudio menu or the keyboard short cut for your
operating system.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{install.packages("roxygen2")}
\item
  With cursor inside function: Code \textgreater{} Insert Roxygen
  Skeleton
\item
  Keyboard shortcut: \emph{cmd + option + shift + r} or \emph{crtl +
  option + shift + r}
\item
  Fill out relevant fields
\end{enumerate}

\subsection{\texorpdfstring{An \texttt{\{roxygen2\}}
example}{An \{roxygen2\} example}}\label{an-roxygen2-example}

Below, we've got an example of an Roxygen skeleton to document a
function that calculates the geometric mean of a vector. Here, the hash
followed by an apostrophe is a special type of comment. It indicates
that this is function documentation rather than just a regular comment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} Title}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param x }
\CommentTok{\#\textquotesingle{} @param remove\_NA }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @return}
\CommentTok{\#\textquotesingle{} @export}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @examples}
\NormalTok{geometric\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{)\{}
  \CommentTok{\# Function body goes here}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We'll fill in all of the fields in this skeleton apart from export,
which we'll remove. If we put this function in a R package, then the
export field makes it available to users of that package. Since this is
just a standalone function we won't need the export field, though
keeping it wouldn't actually cause us any problems either.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} Calculate the geometric mean of a numeric vector}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param x numeric vector}
\CommentTok{\#\textquotesingle{} @param remove\_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @return the geometric mean of the values in \textasciigrave{}x\textasciigrave{}, a numeric scalar value. }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @examples}
\CommentTok{\#\textquotesingle{} geometric\_mean(x = 1:10)}
\CommentTok{\#\textquotesingle{} geometric\_mean(x = c(1:10, NA), remove\_NA = TRUE)}
\CommentTok{\#\textquotesingle{} }
\NormalTok{geometric\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{)\{}
  \CommentTok{\# Function body goes here}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Once we have filled in the skeleton documentation it might look
something like this. We have described what the function does, what the
expected inputs are and what the user can expect as an output. We've
also given an few simple examples of how the function can be used.

For more on Roxygen, see the \href{https://roxygen2.r-lib.org/}{package
documentation} or the chapter of R packages on
\href{https://r-pkgs.org/man.html}{function documentation}.

\subsection{Checking Your Code}\label{checking-your-code}

\begin{quote}
\emph{If you write a function, test it.}
\end{quote}

Testing code has two main purposes:

\begin{itemize}
\tightlist
\item
  To warn or prevent user misuse (e.g.~strange inputs),
\item
  To catch edge cases.
\end{itemize}

On top of explaining how our functions \emph{should} work, we really
ought to check that they \emph{do} work. This is the job of unit
testing.

Whenever you write a function you should test that it works as you
intended it to. Additionally, you should test that your function is
robust to being misused by the user. Depending on the context, this
might be accidental or malicious misuse. Finally, you should check that
the function behaves properly for strange, but still valid, inputs.
These are known as edge cases.

Testing can be a bit of a brutal process, you've just created a
beautiful function and now you're job is to do your best to break it!

\subsection{An Informal Testing
Workflow}\label{an-informal-testing-workflow}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function
\item
  Experiment with the function in the console, try to break it
\item
  Fix the break and repeat.
\end{enumerate}

\textbf{Problems:} Time consuming and not reproducible.

An informal approach to testing your code might be to first write a
function and then play around with it in the console to check that it
behaves well when you give it obvious inputs, edge cases and
deliberately wrong inputs. Each time you manage to break the function,
you edit it to fix the problem and then start the process all over
again.

This \emph{is} testing the code, but only informally. There's no record
of how you have tried to break your code already. The problem with this
approach is that when you return to this code to add a new feature,
you'll probably have forgotten at least one of the informal tests you
ran the first time around. This goes against our efforts towards
reproducibility and automation. It also makes it very easy to break code
that used to work just fine.

\subsection{A Formal Testing Workflow}\label{a-formal-testing-workflow}

We can formalise this testing workflow by writing our tests in their own
R script and saving them for future reference. Remember from the first
lecture that these should be saved in the \texttt{tests/} directory, the
structure of which should mirror that of the \texttt{src/} directory for
your project. All of the tests for one function should live in a single
file, which is named after that function.

One way of writing these tests is to use lots of if statements. The
\texttt{\{testthat\}} can do some of that syntactic heavy lifting for
us. It has lots of helpful functions to test that the output of your
function is what you expect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{geometric\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x , }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{)\{}\FunctionTok{prod}\NormalTok{(x)}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(x))\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testthat}\SpecialCharTok{::}\FunctionTok{expect\_equal}\NormalTok{(}
  \AttributeTok{object =} \FunctionTok{geometric\_mean}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{),}
  \AttributeTok{expected =} \ConstantTok{NA}\NormalTok{)}

\CommentTok{\# Error: geometric\_mean(x = c(1, NA), remove\_NA = FALSE) not equal to NA.}
\CommentTok{\# Types not compatible: double is not logical}
\end{Highlighting}
\end{Shaded}

In this example, we have an error because our function returns a logical
\texttt{NA} rather than a double \texttt{NA}. Yes, R really does have
different types of \texttt{NA} for different types of missing data, it
usually just handles these nicely in the background for you.

This subtle difference is probably not something that you would have
spotted on your own, until it caused you trouble much further down the
line. This rigorous approach is one of the benefits of of using the
\texttt{\{testthat\}} functions.

To fix this test we change out expected output to \texttt{NA\_real\_}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testthat}\SpecialCharTok{::}\FunctionTok{expect\_equal}\NormalTok{(}
  \AttributeTok{object =} \FunctionTok{geometric\_mean}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\ConstantTok{NA}\NormalTok{), }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{),}
  \AttributeTok{expected =} \ConstantTok{NA\_real\_}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We'll revisit the \texttt{\{testthat\}} package in the live session this
week, when we will learn how to use it to test functions within our own
packages.

\section{Summary}\label{summary-1}

\begin{itemize}
\tightlist
\item
  Functional and Object Oriented Programming
\item
  Structuring your scripts
\item
  Styling your code
\item
  Reduce, reuse, recycle
\item
  Documenting and testing
\end{itemize}

Let's wrap up by summarising what we have learned in this chapter.

We started out with a discussion on the differences between functional
and object oriented programming. While R is capable of both, data
science work tends to have more of a functional flavour to it.

We've then described how to structure your scripts and style your code
to make it as human-friendly and easy to debug as possible.

Finally, we discussed how to write DRY code that is well documented and
tested.

\section{Session Information}\label{session-information-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pander}\SpecialCharTok{::}\FunctionTok{pander}\NormalTok{(}\FunctionTok{sessionInfo}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{other attached packages:} magrittr(v.2.0.3)

\textbf{loaded via a namespace (and not attached):}
\emph{crayon(v.1.5.3)}, \emph{vctrs(v.0.6.5)}, \emph{cli(v.3.6.3)},
\emph{knitr(v.1.49)}, \emph{rlang(v.1.1.4)}, \emph{xfun(v.0.50)},
\emph{stringi(v.1.8.4)}, \emph{purrr(v.1.0.2)}, \emph{pkgload(v.1.4.0)},
\emph{generics(v.0.1.3)}, \emph{assertthat(v.0.2.1)},
\emph{jsonlite(v.1.8.9)}, \emph{glue(v.1.8.0)},
\emph{rprojroot(v.2.0.4)}, \emph{htmltools(v.0.5.8.1)},
\emph{brio(v.1.1.5)}, \emph{rmarkdown(v.2.29)}, \emph{pander(v.0.6.5)},
\emph{emo(v.0.0.0.9000)}, \emph{evaluate(v.1.0.3)},
\emph{fastmap(v.1.2.0)}, \emph{yaml(v.2.3.10)},
\emph{lifecycle(v.1.0.4)}, \emph{stringr(v.1.5.1)},
\emph{compiler(v.4.4.2)}, \emph{Rcpp(v.1.0.14)},
\emph{testthat(v.3.2.3)}, \emph{timechange(v.0.3.0)},
\emph{rstudioapi(v.0.17.1)}, \emph{digest(v.0.6.37)},
\emph{R6(v.2.5.1)}, \emph{pillar(v.1.10.1)}, \emph{tools(v.4.4.2)},
\emph{lubridate(v.1.9.4)} and \emph{desc(v.1.4.3)}

\chapter*{Workflows Checklist}\label{workflows-checklist}
\addcontentsline{toc}{chapter}{Workflows Checklist}

\markboth{Workflows Checklist}{Workflows Checklist}

\section*{Videos / Chapters}\label{videos-chapters}
\addcontentsline{toc}{section}{Videos / Chapters}

\markright{Videos / Chapters}

\begin{itemize}
\tightlist
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=eb93df23-751e-4f79-8397-af72013634d0}{Organising
  your work} (30 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/01-01-organising-your-work/01-01-organising-your-work.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=0f676fbc-3de6-490a-ac38-af7200ee1396}{Naming
  Files} (20 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/01-02-naming-files/01-02-naming-files.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=572c25c0-4cac-4260-97fe-af7200ee1358}{Organising
  your code} (27 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/01-03-organising-your-code/01-03-organising-your-code.pdf}{{[}slides{]}}
\end{itemize}

\section*{Reading}\label{reading}
\addcontentsline{toc}{section}{Reading}

\markright{Reading}

Use the \hyperref[workflows-reading]{workflows section} of the reading
list to support and guide your exploration of this week's materials.
Note that these texts are divided into core reading, reference materials
and materials of interest.

\section*{Tasks}\label{tasks}
\addcontentsline{toc}{section}{Tasks}

\markright{Tasks}

\emph{Short:}

\begin{itemize}
\item
  When downloading papers from a journal webpage or ArXiV, they often
  have unhelpful file names like \texttt{2310.12711.pdf}. Come up with
  your own naming convention that will keep your downloaded papers
  organised and easy to search. Write a short paragraph explaining and
  justifying your naming convention for someone else.
\item
  When downloading data from websites you can encounter similary
  uninformative names. Use the
  \href{https://earthquake.usgs.gov/earthquakes/search/}{US Geological
  Survey website} to download a csv file of all earthqakes in the
  conterminous US exceeding magnitude 3.0 during 2023. Note the default
  name of this file and suggest your own, improved name for this file.
\item
  Consider simulating a fire evacuation of the Huxley building taking an
  object oriented approach. List the classes of object might you define
  and what methods and fields would give to each of these. (There is no
  expectation to code this simulation)
\end{itemize}

\emph{Core:}

\begin{itemize}
\item
  Find 3 data science projects on Github and explore how they organise
  their work. Write a post on the EdStem forum that links to all three,
  and in a couple of paragraphs describe the content and structure of
  one project.
\item
  Create your own project directory (or directories) for this course and
  its assignments.
\item
  Write two of your own R functions. The first should calculate the
  geometric mean of a numeric vector. The second should calculate the
  rolling arithmetic mean of a numeric vector.
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\item
  Re-factor an old project to match the project organisation and coding
  guides for this course. This might be a small research project, class
  notes or a collection of homework assignments. Use an R-based project
  if possible. If you only have python projects, then either translate
  these to R or apply the \href{https://peps.python.org/pep-0008/}{PEP8}
  style guide. Take care to select a suitably sized project so that this
  is a meaningful exercise but does not take more than a few hours.
\item
  If you are able to do so, host your re-factored project publicly and
  share it with the rest of the class on the EdStem Discussion forum.
\end{itemize}

\section*{Live Session}\label{live-session}
\addcontentsline{toc}{section}{Live Session}

\markright{Live Session}

In the live session we will begin with a discussion of this week's
tasks. We will then create a minimal R package to organise and test the
functions you have written.

\begin{itemize}
\item
  Please come to the live session prepared to discuss the following
  points:

  \begin{itemize}
  \item
    Did you make the assignment projects as subdirectories or as their
    stand alone projects? Why?
  \item
    What were some terms that you had not met before during the
    readings? How did you find their meanings?
  \item
    What did you have to consider when writing your rolling mean
    function?
  \end{itemize}
\item
  If you would like to follow along in the live session then you should
  prepare by:

  \begin{itemize}
  \item
    Installling/Updating \href{https://cran.r-project.org/}{R},
    \href{https://posit.co/download/rstudio-desktop/}{RStudio} and
    \href{https://quarto.org/}{Quarto}.
  \item
    Follow the \href{https://happygitwithr.com/}{Happy git with R}
    instructions to link RStudio and Git and Github (1-14).
  \item
    Write a \texttt{username/README.md} introducing yourself on your
    GitHub profile. (See e.g.:
    \href{https://github.com/StatsRhian/StatsRhian}{StatsRhian},
    \href{https://github.com/nrennie/nrennie}{nrennie})
  \end{itemize}
\end{itemize}

\part{Acquiring and Sharing Data}

\section*{Introduction}\label{data-introduction}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\emph{Data can be difficult to acquire and gnarly when you get it.}

The raw material that you work with as a data scientist is,
unsurprisingly, data. In this part of the course we will focus on the
different ways in which data can be stored, distributed and obtained.

Being able to obtain and read a dataset is often a surprisingly large
hurdle in getting a new data science project off the ground. The skill
of being able to source and read data from many locations is usually
sanitised during a statistics programme: you're given a ready-to-go,
cleaned CSV file and all focus is placed on modelling. This week aims to
remedy that by equipping you with the skills to acquire and manage your
own data.

We will begin this week by explore different file types. This dictates
what type of information you can store, who can access that information
and how they read that it into R. We will then turn our attention to the
case when data are not given to you directly. We will learn how to
obtain data from a raw webpage and how to request data that via a
service known as an API.

\chapter{Tabular Data}\label{data-tabular}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-important-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-important-color-frame, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
undergoing heavy restructuring and may be confusing or incomplete.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{Loading Tabular Data}\label{loading-tabular-data}

Recall that simpler, open source formats improve accessibility and
reproducibility. We will begin by reading in three open data formats for
tabular data.

\begin{itemize}
\item
  \texttt{random-data.csv}
\item
  \texttt{random-data.tsv}
\item
  \texttt{random-data.txt}
\end{itemize}

Each of these data sets contains 26 observations of 4 variables:

\begin{itemize}
\tightlist
\item
  \texttt{id}, a Roman letter identifier;
\item
  \texttt{gaussian}, standard normal random variates;
\item
  \texttt{gamma}, gamma(1,1) random variates;
\item
  \texttt{uniform}, uniform(0,1) random variates.
\end{itemize}

\subsection{Base R}\label{base-r}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random\_df }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}random{-}data.csv\textquotesingle{}}\NormalTok{)}
\FunctionTok{print}\NormalTok{(random\_df)}
\CommentTok{\#\textgreater{}    id    gaussian      gamma    uniform}
\CommentTok{\#\textgreater{} 1   a {-}1.20706575 0.98899970 0.22484576}
\CommentTok{\#\textgreater{} 2   b  0.27742924 0.03813386 0.08498474}
\CommentTok{\#\textgreater{} 3   c  1.08444118 1.09462335 0.63729826}
\CommentTok{\#\textgreater{} 4   d {-}2.34569770 1.49301101 0.43101637}
\CommentTok{\#\textgreater{} 5   e  0.42912469 5.40361248 0.07271609}
\CommentTok{\#\textgreater{} 6   f  0.50605589 1.72386539 0.80240202}
\CommentTok{\#\textgreater{} 7   g {-}0.57473996 1.95357133 0.32527830}
\CommentTok{\#\textgreater{} 8   h {-}0.54663186 0.07807803 0.75728904}
\CommentTok{\#\textgreater{} 9   i {-}0.56445200 0.21198194 0.58427152}
\CommentTok{\#\textgreater{} 10  j {-}0.89003783 0.20803673 0.70883941}
\CommentTok{\#\textgreater{} 11  k {-}0.47719270 2.08607862 0.42697577}
\CommentTok{\#\textgreater{} 12  l {-}0.99838644 0.49463708 0.34357270}
\CommentTok{\#\textgreater{} 13  m {-}0.77625389 0.77171305 0.75911999}
\CommentTok{\#\textgreater{} 14  n  0.06445882 0.37216648 0.42403021}
\CommentTok{\#\textgreater{} 15  o  0.95949406 1.88207991 0.56088725}
\CommentTok{\#\textgreater{} 16  p {-}0.11028549 0.76622568 0.11613577}
\CommentTok{\#\textgreater{} 17  q {-}0.51100951 0.50488585 0.30302180}
\CommentTok{\#\textgreater{} 18  r {-}0.91119542 0.22979791 0.47880269}
\CommentTok{\#\textgreater{} 19  s {-}0.83717168 0.75637275 0.34483055}
\CommentTok{\#\textgreater{} 20  t  2.41583518 0.62435969 0.60071414}
\CommentTok{\#\textgreater{} 21  u  0.13408822 0.64638373 0.07608332}
\CommentTok{\#\textgreater{} 22  v {-}0.49068590 0.11247545 0.95599261}
\CommentTok{\#\textgreater{} 23  w {-}0.44054787 0.11924307 0.02220682}
\CommentTok{\#\textgreater{} 24  x  0.45958944 4.91805535 0.84171063}
\CommentTok{\#\textgreater{} 25  y {-}0.69372025 0.60282666 0.63244245}
\CommentTok{\#\textgreater{} 26  z {-}1.44820491 0.64446571 0.31009417}
\end{Highlighting}
\end{Shaded}

Output is a \texttt{data.frame} object. (List of vectors with some nice
methods)

\subsection{\texorpdfstring{\texttt{\{readr\}}}{\{readr\}}}\label{readr}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random\_tbl }\OtherTok{\textless{}{-}}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}random{-}data.csv\textquotesingle{}}\NormalTok{)}
\CommentTok{\#\textgreater{} Rows: 26 Columns: 4}
\CommentTok{\#\textgreater{} {-}{-} Column specification {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Delimiter: ","}
\CommentTok{\#\textgreater{} chr (1): id}
\CommentTok{\#\textgreater{} dbl (3): gaussian, gamma, uniform}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} i Use \textasciigrave{}spec()\textasciigrave{} to retrieve the full column specification for this data.}
\CommentTok{\#\textgreater{} i Specify the column types or set \textasciigrave{}show\_col\_types = FALSE\textasciigrave{} to quiet this message.}
\FunctionTok{print}\NormalTok{(random\_tbl)}
\CommentTok{\#\textgreater{} \# A tibble: 26 x 4}
\CommentTok{\#\textgreater{}   id    gaussian  gamma uniform}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 a       {-}1.21  0.989   0.225 }
\CommentTok{\#\textgreater{} 2 b        0.277 0.0381  0.0850}
\CommentTok{\#\textgreater{} 3 c        1.08  1.09    0.637 }
\CommentTok{\#\textgreater{} 4 d       {-}2.35  1.49    0.431 }
\CommentTok{\#\textgreater{} 5 e        0.429 5.40    0.0727}
\CommentTok{\#\textgreater{} 6 f        0.506 1.72    0.802 }
\CommentTok{\#\textgreater{} \# i 20 more rows}
\end{Highlighting}
\end{Shaded}

Output is a \texttt{tibble} object. (List of vectors with some nicer
methods)

\subsubsection{\texorpdfstring{Benefits of
\texttt{readr::read\_csv()}}{Benefits of readr::read\_csv()}}\label{benefits-of-readrread_csv}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Increased speed (approx. 10x) and progress bar.
\item
  Strings are not coerced to factors. No more
  \texttt{stringsAsFactors\ =\ FALSE}
\item
  No row names and nice column names.
\item
  Reproducibility bonus: does not depend on operating system.
\end{enumerate}

\subsection{WTF: Tibbles}\label{wtf-tibbles}

\subsubsection{Printing}\label{printing}

\begin{itemize}
\item
  Default to first 10 rows and as many columns as will comfortably fit
  on your screen.
\item
  Can adjust this behaviour in the print call:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# print first three rows and all columns}
\FunctionTok{print}\NormalTok{(random\_tbl, }\AttributeTok{n =} \DecValTok{3}\NormalTok{, }\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 26 x 4}
\CommentTok{\#\textgreater{}   id    gaussian  gamma uniform}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 a       {-}1.21  0.989   0.225 }
\CommentTok{\#\textgreater{} 2 b        0.277 0.0381  0.0850}
\CommentTok{\#\textgreater{} 3 c        1.08  1.09    0.637 }
\CommentTok{\#\textgreater{} \# i 23 more rows}
\end{Highlighting}
\end{Shaded}

\textbf{Bonus:} Colour formatting in IDE and each column tells you it's
type.

\subsubsection{Subsetting}\label{subsetting}

Subsetting tibbles will always return another tibble.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Row Subsetting}
\NormalTok{random\_tbl[}\DecValTok{1}\NormalTok{, ] }\CommentTok{\# returns tibble}
\NormalTok{random\_df[}\DecValTok{1}\NormalTok{, ]  }\CommentTok{\# returns data.frame}

\CommentTok{\# Column Subsetting}
\NormalTok{random\_tbl[ , }\DecValTok{1}\NormalTok{]      }\CommentTok{\# returns tibble}
\NormalTok{random\_df[ , }\DecValTok{1}\NormalTok{]       }\CommentTok{\# returns vector}

\CommentTok{\# Combined Subsetting}
\NormalTok{random\_tbl[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]      }\CommentTok{\# returns 1x1 tibble}
\NormalTok{random\_df[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]       }\CommentTok{\# returns single value}
\end{Highlighting}
\end{Shaded}

This helps to avoids edge cases associated with working on data frames.

\subsection{\texorpdfstring{Other \texttt{\{readr\}}
functions}{Other \{readr\} functions}}\label{other-readr-functions}

See \texttt{\{readr\}}
\href{https://readr.tidyverse.org/}{documentation}, there are lots of
useful additional arguments that can help you when reading messy data.

Functions for reading and writing other types of tabular data work
analogously.

\subsubsection{Reading Tabular Data}\label{reading-tabular-data}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{read\_tsv}\NormalTok{(}\StringTok{"random{-}data.tsv"}\NormalTok{)}
\FunctionTok{read\_delim}\NormalTok{(}\StringTok{"random{-}data.txt"}\NormalTok{, }\AttributeTok{delim =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Writing Tabular Data}\label{writing-tabular-data}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(random\_tbl, }\StringTok{"random{-}data{-}2.csv"}\NormalTok{)}
\FunctionTok{write\_tsv}\NormalTok{(random\_tbl, }\StringTok{"random{-}data{-}2.tsv"}\NormalTok{)}
\FunctionTok{write\_delim}\NormalTok{(random\_tbl, }\StringTok{"random{-}data{-}2.tsv"}\NormalTok{, }\AttributeTok{delim =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Need for Speed}\label{need-for-speed}

Some times you have to load \emph{lots of large data sets}, in which
case a 10x speed-up might not be sufficient.

If each data set still fits inside RAM, then check out
\texttt{data.table::fread()} which is optimised for speed. (Alternatives
exist for optimal memory usage and data too large for working memory,
but not covered here.)

\textbf{Note:} While it can be much faster, the resulting data.table
object lacks the consistancy properties of a tibble so be sure to check
for edge cases, where the returned value is not what you might expect.

\section{Tidy Data}\label{tidy-data}

\subsection{Wide vs.~Tall Data}\label{wide-vs.-tall-data}

\subsubsection{Wide Data}\label{wide-data}

\begin{itemize}
\item
  First column has unique entries
\item
  Easier for humans to read and compute on
\item
  Harder for machines to compute on
\end{itemize}

\subsubsection{Tall Data}\label{tall-data}

\begin{itemize}
\item
  First column has repeating entries
\item
  Harder for humans to read and compute on
\item
  Easier for machines to compute on
\end{itemize}

\subsubsection{Examples}\label{examples}

\textbf{Example 1 (Wide)}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
\textbf{Person } & \textbf{Age } & \textbf{Weight } & \textbf{Height
} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bob & 32 & 168 & 180 \\
Alice & 24 & 150 & 175 \\
Steve & 64 & 144 & 165 \\
\end{longtable}

\textbf{Example 1 (Tall)}

\begin{longtable}[]{@{}ccc@{}}
\toprule\noalign{}
\textbf{Person } & \textbf{Variable } & \textbf{Value } \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bob & Age & 32 \\
Bob & Weight & 168 \\
Bob & Height & 180 \\
Alice & Age & 24 \\
Alice & Weight & 150 \\
Alice & Height & 175 \\
Steve & Age & 64 \\
Steve & Weight & 144 \\
Steve & Height & 165 \\
\end{longtable}

{[}Source: Wikipedia - Wide and narrow data{]}

\textbf{Example 2 (Wide)}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Team & Points & Assists & Rebounds \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A & 88 & 12 & 22 \\
B & 91 & 17 & 28 \\
C & 99 & 24 & 30 \\
D & 94 & 28 & 31 \\
\end{longtable}

\textbf{Example 2 (Tall)}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Team & Variable & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A & Points & 88 \\
A & Assists & 12 \\
A & Rebounds & 22 \\
B & Points & 91 \\
B & Assists & 17 \\
B & Rebounds & 28 \\
C & Points & 99 \\
C & Assists & 24 \\
C & Rebounds & 30 \\
D & Points & 94 \\
D & Assists & 28 \\
D & Rebounds & 31 \\
\end{longtable}

{[}Source: Statology - Long vs wide data{]}

\subsubsection{Pivoting Wider and
Longer}\label{pivoting-wider-and-longer}

\begin{itemize}
\item
  Error control at input and analysis is format-dependent.
\item
  Switching between long and wide formats useful to control errors.
\item
  Easy with the \texttt{\{tidyr\}} package functions
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{()}
\NormalTok{tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsection{Tidy What?}\label{tidy-what}

\begin{figure}[H]

{\centering \includegraphics{images/201-tabular-data/tidy-1.png}

}

\caption{{[}Image: R4DS - Chapter 12{]}}

\end{figure}%

\emph{Tidy Data} is an opinionated way to store tabular data.

Image Source: Chapter 12 of R for Data Science.

\begin{itemize}
\tightlist
\item
  Each column corresponds to a exactly one measured variable
\item
  Each row corresponds to exactly one observational unit
\item
  Each cell contains exactly one value.
\end{itemize}

\textbf{Benefits of tidy data}

\begin{itemize}
\item
  \emph{Consistent data format:} Reduces cognitive load and allows
  specialised tools (functions) to efficiently work with tabular data.
\item
  \emph{Vectorisation}: Keeping variables as columns allows for very
  efficient data manipulation. (this goes back to data frames and
  tibbles being lists of vectors)
\end{itemize}

\subsection{Example - Tidy Longer}\label{example---tidy-longer}

Consider trying to plot these data as time series. The \texttt{year}
variable is trapped in the column names!

\begin{verbatim}
#> # A tibble: 3 x 3
#>   country     `1999` `2000`
#>   <chr>        <dbl>  <dbl>
#> 1 Afghanistan    745   2666
#> 2 Brazil       37737  80488
#> 3 China       212258 213766
\end{verbatim}

To tidy this data, we need to \texttt{pivot\_longer()}. We will turn the
column names into a new \texttt{year} variable and retaining cell
contents as a new variable called \texttt{cases}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}

\NormalTok{countries }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{1999}\StringTok{\textasciigrave{}}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{2000}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"cases"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   country     year   cases}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Afghanistan 1999     745}
\CommentTok{\#\textgreater{} 2 Afghanistan 2000    2666}
\CommentTok{\#\textgreater{} 3 Brazil      1999   37737}
\CommentTok{\#\textgreater{} 4 Brazil      2000   80488}
\CommentTok{\#\textgreater{} 5 China       1999  212258}
\CommentTok{\#\textgreater{} 6 China       2000  213766}
\end{Highlighting}
\end{Shaded}

Much better!

\subsection{Example - Tidy Wider}\label{example---tidy-wider}

There are other times where we might have to widen our data to tidy it.

This example is not tidy. Why not?

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Team & Variable & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A & Points & 88 \\
A & Assists & 12 \\
A & Rebounds & 22 \\
B & Points & 91 \\
B & Assists & 17 \\
B & Rebounds & 28 \\
C & Points & 99 \\
C & Assists & 24 \\
C & Rebounds & 30 \\
D & Points & 94 \\
D & Assists & 28 \\
D & Rebounds & 31 \\
\end{longtable}

The observational unit here is a team. However, each variable should be
a stored in a separate column, with cells containing their values.

To tidy this data we first generate it as a tibble. We use the
\texttt{tribble()} function, which allows us to create a tibble row-wise
rather than column-wise.

We can then tidy it by creating new columns for each value of the
current \texttt{Variable} column and taking the values for these from
the current \texttt{Value} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tournament }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}
    \AttributeTok{id\_cols =} \StringTok{"Team"}\NormalTok{, }
    \AttributeTok{names\_from =} \StringTok{"Variable"}\NormalTok{,}
    \AttributeTok{values\_from =} \StringTok{"Value"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 4}
\CommentTok{\#\textgreater{}   Team  Points Assists Rebounds}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 A         88      12       22}
\CommentTok{\#\textgreater{} 2 B         91      17       28}
\CommentTok{\#\textgreater{} 3 C         99      24       30}
\CommentTok{\#\textgreater{} 4 D         94      28       31}
\end{Highlighting}
\end{Shaded}

\subsection{Other helpful functions}\label{other-helpful-functions}

The \texttt{pivot\_*()} family of functions resolve issues with rows
(too many observations per row or rows per observation).

There are similar helper functions to solve column issues:

\begin{itemize}
\item
  Multiple variables per column: \texttt{tidyr::separate()},
\item
  Multiple columns per variable: \texttt{tidyr::unite()}.
\end{itemize}

\subsection{Missing Data}\label{missing-data}

In tidy data, every cell contains a value. Including cells with missing
values.

\begin{itemize}
\item
  Missing values are coded as \texttt{NA} (generic) or a type-specific
  \texttt{NA}, such as \texttt{NA\_character\_}.
\item
  The \texttt{\{readr\}} family of \texttt{read\_*()} function have good
  defaults and helpful \texttt{na} argument.
\item
  Explicitly code \texttt{NA} values when collecting data, avoid
  ambiguity: '' ``, -999 or worst of all 0.
\item
  More on missing values in EDA videos\ldots{}
\end{itemize}

\section{Wrapping Up}\label{wrapping-up-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reading in tabular data by a range of methods
\item
  Introduced the \texttt{tibble} and tidy data (+ tidy not always best)
\item
  Tools for tidying messy tabular data
\end{enumerate}

\section{Session Information}\label{session-information-3}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{other attached packages:} magrittr(v.2.0.3)

\textbf{loaded via a namespace (and not attached):}
\emph{crayon(v.1.5.3)}, \emph{vctrs(v.0.6.5)}, \emph{cli(v.3.6.3)},
\emph{knitr(v.1.49)}, \emph{rlang(v.1.1.4)}, \emph{xfun(v.0.50)},
\emph{purrr(v.1.0.2)}, \emph{generics(v.0.1.3)},
\emph{jsonlite(v.1.8.9)}, \emph{glue(v.1.8.0)}, \emph{bit(v.4.5.0.1)},
\emph{htmltools(v.0.5.8.1)}, \emph{hms(v.1.1.3)},
\emph{rmarkdown(v.2.29)}, \emph{pander(v.0.6.5)},
\emph{evaluate(v.1.0.3)}, \emph{tibble(v.3.2.1)}, \emph{tzdb(v.0.4.0)},
\emph{fastmap(v.1.2.0)}, \emph{yaml(v.2.3.10)},
\emph{lifecycle(v.1.0.4)}, \emph{compiler(v.4.4.2)},
\emph{dplyr(v.1.1.4)}, \emph{Rcpp(v.1.0.14)}, \emph{pkgconfig(v.2.0.3)},
\emph{tidyr(v.1.3.1)}, \emph{rstudioapi(v.0.17.1)},
\emph{digest(v.0.6.37)}, \emph{R6(v.2.5.1)}, \emph{utf8(v.1.2.4)},
\emph{readr(v.2.1.5)}, \emph{tidyselect(v.1.2.1)},
\emph{parallel(v.4.4.2)}, \emph{vroom(v.1.6.5)},
\emph{pillar(v.1.10.1)}, \emph{withr(v.3.0.2)}, \emph{tools(v.4.4.2)}
and \emph{bit64(v.4.5.2)}

\chapter{Web Scraping}\label{data-webscraping}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{Scraping webpage data using
\{rvest\}}\label{scraping-webpage-data-using-rvest}

You can't always rely on tidy, tabular data to land on your desk.
Sometimes you are going to have to go out and gather data for yourself.

I'm not suggesting you will need to do this manually, but you will
likely need to get data from the internet that's been made publicly or
privately available to you.

This might be information from a webpage that you gather yourself, or
data shared with you by a collaborator using an API.

In this chapter we will cover the basics of scraping webpages, following
the
\href{https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html\#fnref3}{vignette}
for the \{rvest\} package.

\section{What is a webpage?}\label{what-is-a-webpage}

Before we can even hope to get data from a webpage, we first need to
understand \emph{what} a webpage is.

Webpages are written in a similar way to LaTeX: the content and styling
of webpages are handled separately and are coded using plain text files.

In fact, websites go one step further than LaTeX. The content and
styling of websites are written in different files and in different
languages. HTML (HyperText Markup Language) is used to write the content
and then CSS (Cascading Style Sheets) are used to control the appearance
of that content when it's displayed to the user.

\section{HTML}\label{html}

A basic HTML page with no styling applied might look something like
this:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{\textless{}}\KeywordTok{html}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}}\KeywordTok{head}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{title}\DataTypeTok{\textgreater{}}\NormalTok{Page title}\DataTypeTok{\textless{}/}\KeywordTok{title}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}/}\KeywordTok{head}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}}\KeywordTok{body}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{h1}\OtherTok{ id}\OperatorTok{=}\StringTok{\textquotesingle{}first\textquotesingle{}}\DataTypeTok{\textgreater{}}\NormalTok{A level 1 heading}\DataTypeTok{\textless{}/}\KeywordTok{h1}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{p}\DataTypeTok{\textgreater{}}\NormalTok{Hello World!}\DataTypeTok{\textless{}/}\KeywordTok{p}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{p}\DataTypeTok{\textgreater{}}\NormalTok{Here is some plain text }\DecValTok{\&amp;} \DataTypeTok{\textless{}}\KeywordTok{b}\DataTypeTok{\textgreater{}}\NormalTok{some bold text.}\DataTypeTok{\textless{}/}\KeywordTok{b}\DataTypeTok{\textgreater{}\textless{}/}\KeywordTok{p}\DataTypeTok{\textgreater{}}
  \DataTypeTok{\textless{}}\KeywordTok{img}\OtherTok{ src}\OperatorTok{=}\StringTok{\textquotesingle{}myimg.png\textquotesingle{}}\OtherTok{ width}\OperatorTok{=}\StringTok{\textquotesingle{}100\textquotesingle{}}\OtherTok{ height}\OperatorTok{=}\StringTok{\textquotesingle{}100\textquotesingle{}}\DataTypeTok{\textgreater{}}
\DataTypeTok{\textless{}/}\KeywordTok{body}\DataTypeTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\subsection{HTML elements}\label{html-elements}

Just like XML data files, HTML has a hierarchical structure. This
structure is crafted using HTML elements. Each HTML element is made up
of of a start tag, optional attributes, an end tag.

We can see each of these in the first level header, where
\texttt{\textless{}h1\textgreater{}} is the opening tag,
\texttt{id=\textquotesingle{}first\textquotesingle{}} is an additional
attribute and \texttt{\textless{}/h1\textgreater{}} is the closing tag.
Everything between the opening and closing tag are the contents of that
element. There are also some special elements that consist of only a
single tag and its optional attributes. An example of this is the
\texttt{\textless{}img\textgreater{}} tag.

Since \texttt{\textless{}} and \texttt{\textgreater{}} are used for
start and end tags, you can't write them directly in an HTML document.
Instead, you have to use escape characters. This sounds fancy, but it's
just an alternative way to write characters that serve some special
function within a language.

You can write greater than \texttt{\&gt;} and less than as
\texttt{\&lt;}. You might notice that those escapes use an ampersand
(\&). This means that if you want a literal ampersand on your webpage,
you have to escape too using \texttt{\&amp;}.

There are a wide range of possible HTML tags and escapes. We'll cover
the most common tags in this lecture and you don't need to worry about
escapes too much because \texttt{\{rvest\}} will automatically handle
them for you.

\subsection{Important HTML Elements}\label{important-html-elements}

In all, there are in excess of 100 HTML elements. The most important
ones for you to know about are:

\begin{itemize}
\item
  The \texttt{\textless{}html\textgreater{}} element, that must enclose
  every HTML page. The \texttt{\textless{}html\textgreater{}} element
  must have two child elements within it. The
  \texttt{\textless{}head\textgreater{}} element contains metadata about
  the document, like the page title that is shown in the browser tab and
  the CSS style sheet that should be applied. The
  \texttt{\textless{}body\textgreater{}} element then contains all of
  the content that you see in the browser.
\item
  Block elements are used to give structure to the page. These are
  elements like headings, sub-headings and so on from
  \texttt{\textless{}h1\textgreater{}} all the way down to
  \texttt{\textless{}h6\textgreater{}}. This category also contains
  paragraph elements \texttt{\textless{}p\textgreater{}}, ordered lists
  \texttt{\textless{}ol\textgreater{}} unordered lists
  \texttt{\textless{}ul\textgreater{}}.
\item
  Finally, inline tags like \texttt{\textless{}b\textgreater{}} for
  bold, \texttt{\textless{}i\textgreater{}} for italics, and
  \texttt{\textless{}a\textgreater{}} for hyperlinks are used to format
  text inside block elements.
\end{itemize}

When you come across a tag that you've never seen before, you can find
out what it does with just a little bit of googling. A good resource
here is the \href{https://developer.mozilla.org/en-US/docs/Web/HTML}{MDN
Web Docs} which are produced by Mozilla, the company that makes the
Firefox web browser. The
\href{https://www.w3schools.com/html/default.asp}{W3schools website} is
another great resource for web development and coding resources more
generally.

\section{HTML Attributes}\label{html-attributes}

We've seen one example of a header with an additional attribute. More
generally, all tags can have named attributes. These attributes are
contained within the opening tag and look something like:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{\textless{}}\KeywordTok{tag}\OtherTok{ attribute1}\OperatorTok{=}\StringTok{\textquotesingle{}value1\textquotesingle{}}\OtherTok{ attribute2}\OperatorTok{=}\StringTok{\textquotesingle{}value2\textquotesingle{}}\DataTypeTok{\textgreater{}}\NormalTok{element contents}\DataTypeTok{\textless{}/}\KeywordTok{tag}\DataTypeTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

Two of the most important attributes are \texttt{id} and \texttt{class}.
These attributes are used in conjunction with the CSS file to control
the visual appearance of the page. These are often very useful to
identify the elements that you are interested in when scraping data off
a page.

\section{CSS Selectors}\label{css-selectors}

The Cascading Style Sheet is used to describe how your HTML content will
be displayed. To do this, CSS has it's own system for selecting elements
of a webpage, called CSS selectors.

CSS selectors define patterns for locating the HTML elements that a
particular style should be applied to. A happy side-effect of this is
that they can sometimes be very useful for scraping, because they
provide a concise way of describing which elements you want to extract.

CSS Selectors can work on the level of an element type, a class, or a
tag and these can be used in a nested (or \emph{cascading}) way.

\begin{itemize}
\item
  The \texttt{p} selector will select all paragraph
  \texttt{\textless{}p\textgreater{}} elements.
\item
  The \texttt{.title} selector will select all elements with class
  \texttt{title}.
\item
  The \texttt{p.special} selector will select
  all\texttt{\textless{}p\textgreater{}} elements with class
  \texttt{special}.
\item
  The \texttt{\#title} selector will select the element with the id
  attribute \texttt{title}.
\end{itemize}

When you want to select a single element \texttt{id} attributes are
particularly useful because that \emph{must} be unique within a html
document. Unfortunately, this is only helpful if the developer added an
\texttt{id} attribute to the element(s) you want to scrape!

If you want to learn more CSS selectors I recommend starting with the
fun \href{https://flukeout.github.io/}{CSS diner tutorial} to build a
base of knowledge and then using the
\href{https://www.w3schools.com/css/default.asp}{W3schools resources} as
a reference to explore more webpages in the wild.

\section{Which Attributes and Selectors Do You
Need?}\label{which-attributes-and-selectors-do-you-need}

To scrape data from a webpage, you first have to identify the tag and
attribute combinations that you are interested in gathering.

To find your elements of interest, you have three options. These go from
hardest to easiest but also from most to least robust.

\begin{itemize}
\tightlist
\item
  right click + ``inspect page source'' (F12)
\item
  right click + ``inspect''
\item
  Rvest
  \href{https://rvest.tidyverse.org/articles/selectorgadget.html}{Selector
  Gadget} (very useful but fallible)
\end{itemize}

Inspecting the source of some familiar websites can be a useful way to
get your head around these concepts. Beware though that sophisticated
webpages can be quite intimidating. A good place to start is with
simpler, static websites such as personal websites, rather than the
dynamic webpages of online retailers or social media platforms.

\section{\texorpdfstring{Reading HTML with
\texttt{\{rvest\}}}{Reading HTML with \{rvest\}}}\label{reading-html-with-rvest}

With \texttt{\{rvest\}}, reading a html page can be as simple as loading
in tabular data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\OtherTok{\textless{}{-}}\NormalTok{ rvest}\SpecialCharTok{::}\FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://www.zakvarty.com/teaching.html"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{class} of the resulting object is an \texttt{xml\_document}.
This type of object is from the low-level package \texttt{\{xml2\}},
which allows you to read xml files into R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(html)}
\CommentTok{\#\textgreater{} [1] "xml\_document" "xml\_node"}
\end{Highlighting}
\end{Shaded}

We can see that this object is split into several components: first is
some metadata on the type of document we have scraped, followed by the
head and then the body of that html document.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html}
\CommentTok{\#\textgreater{} \{html\_document\}}
\CommentTok{\#\textgreater{} \textless{}html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}head\textgreater{}\textbackslash{}n\textless{}meta http{-}equiv="Content{-}Type" content="text/html; charset=UT ...}
\CommentTok{\#\textgreater{} [2] \textless{}body class="nav{-}fixed fullcontent"\textgreater{}\textbackslash{}n\textbackslash{}n\textless{}div id="quarto{-}search{-}results ...}
\end{Highlighting}
\end{Shaded}

We have several possible approaches to extracting information from this
document.

\section{Extracting HTML elements}\label{extracting-html-elements}

In \texttt{\{rvest\}} you can extract a single element with
\texttt{html\_element()}, or all matching elements with
\texttt{html\_elements()}. Both functions take a document object and one
or more CSS selectors as inputs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}

\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"h1"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (1)\}}
\CommentTok{\#\textgreater{} [1] \textless{}h1\textgreater{}Teaching\textless{}/h1\textgreater{}}
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"h2"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (1)\}}
\CommentTok{\#\textgreater{} [1] \textless{}h2 class="anchored" data{-}anchor{-}id="my{-}courses"\textgreater{}My Courses\textless{}/h2\textgreater{}}
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (35)\}}
\CommentTok{\#\textgreater{}  [1] \textless{}p\textgreater{}I am fortunate to have had the opportunity to teach in a variety o ...}
\CommentTok{\#\textgreater{}  [2] \textless{}p\textgreater{}Developing and teaching modules in statistics, data science and da ...}
\CommentTok{\#\textgreater{}  [3] \textless{}p\textgreater{}Designing courses for both in{-}person and remote learning, predomin ...}
\CommentTok{\#\textgreater{}  [4] \textless{}p\textgreater{}Supervising undergraduate, postgraduate and doctoral research proj ...}
\CommentTok{\#\textgreater{}  [5] \textless{}p\textgreater{}Adapting and leading short courses on scientific writing and commu ...}
\CommentTok{\#\textgreater{}  [6] \textless{}p\textgreater{}Running workshops and computer labs for undergraduate and postgrad ...}
\CommentTok{\#\textgreater{}  [7] \textless{}p\textgreater{}Speaking at university open days and providing one{-}to{-}one tuition  ...}
\CommentTok{\#\textgreater{}  [8] \textless{}p\textgreater{}I am an associate fellow of the Higher Education Academy, which yo ...}
\CommentTok{\#\textgreater{}  [9] \textless{}p\textgreater{}\textless{}img src="assets/course{-}logos/EDS{-}logo.png" alt="Effective Data Sc ...}
\CommentTok{\#\textgreater{} [10] \textless{}p\textgreater{}\textless{}a href="https://eds{-}notes.zakvarty.com" class="btn btn{-}outline{-}pr ...}
\CommentTok{\#\textgreater{} [11] \textless{}p\textgreater{}Model building and evaluation are necessary but not sufficient ski ...}
\CommentTok{\#\textgreater{} [12] \textless{}p\textgreater{}During this module you will critically explore how to:\textless{}/p\textgreater{}}
\CommentTok{\#\textgreater{} [13] \textless{}p\textgreater{}This interdisciplinary course draws from fields including statisti ...}
\CommentTok{\#\textgreater{} [14] \textless{}p\textgreater{}\textless{}img src="assets/course{-}logos/ethics{-}logo.png" alt="MLDS Ethics lo ...}
\CommentTok{\#\textgreater{} [15] \textless{}p\textgreater{}\textless{}a class="btn btn{-}outline{-}primary m{-}1" href="https://www.zakvarty. ...}
\CommentTok{\#\textgreater{} [16] \textless{}p\textgreater{}Data{-}driven decision making is now pervasive and impacts us all. Y ...}
\CommentTok{\#\textgreater{} [17] \textless{}p\textgreater{}The ways in which these predictive models can fail \textless{}em\textgreater{}mathematica ...}
\CommentTok{\#\textgreater{} [18] \textless{}p\textgreater{}To prevent this harm, the ethical impacts of using data to make de ...}
\CommentTok{\#\textgreater{} [19] \textless{}p\textgreater{}The course takes a practical and technical approach to identifying ...}
\CommentTok{\#\textgreater{} [20] \textless{}p\textgreater{}\textless{}img src="assets/course{-}logos/point{-}process{-}logo.png" alt="Stochas ...}
\CommentTok{\#\textgreater{} ...}
\end{Highlighting}
\end{Shaded}

You can also combine and nest these selectors. For example you might
want to extract all links that are within paragraphs \emph{and} all
second level headers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p a,h2"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (13)\}}
\CommentTok{\#\textgreater{}  [1] \textless{}a href="https://www.advance{-}he.ac.uk/fellowship/associate{-}fellowship ...}
\CommentTok{\#\textgreater{}  [2] \textless{}h2 class="anchored" data{-}anchor{-}id="my{-}courses"\textgreater{}My Courses\textless{}/h2\textgreater{}}
\CommentTok{\#\textgreater{}  [3] \textless{}a href="https://eds{-}notes.zakvarty.com" class="btn btn{-}outline{-}prima ...}
\CommentTok{\#\textgreater{}  [4] \textless{}a href="https://youtube.com/playlist?list=PLBVD{-}YN3U17AiyuMwW77yGnVu ...}
\CommentTok{\#\textgreater{}  [5] \textless{}a href="https://github.com/zakvarty/effective{-}data{-}science{-}resources ...}
\CommentTok{\#\textgreater{}  [6] \textless{}a class="btn btn{-}outline{-}primary m{-}1" href="https://www.zakvarty.com ...}
\CommentTok{\#\textgreater{}  [7] \textless{}a href="https://github.com/zakvarty/ethics{-}one{-}resources" class="btn ...}
\CommentTok{\#\textgreater{}  [8] \textless{}a href="https://github.com/zakvarty/ethics{-}two{-}resources" class="btn ...}
\CommentTok{\#\textgreater{}  [9] \textless{}a href="https://github.com/zakvarty/ethics{-}three{-}resources" class="b ...}
\CommentTok{\#\textgreater{} [10] \textless{}a href="https://m1r.zakvarty.com" class="btn btn{-}outline{-}primary m{-}1 ...}
\CommentTok{\#\textgreater{} [11] \textless{}a href="https://youtube.com/playlist?list=PLBVD{-}YN3U17BybIN\_6drVs9LL ...}
\CommentTok{\#\textgreater{} [12] \textless{}a href="https://extremes{-}notes.zakvarty.com" class="btn btn{-}outline{-} ...}
\CommentTok{\#\textgreater{} [13] \textless{}a href="https://youtube.com/playlist?list=PLBVD{-}YN3U17Ce7tO3bkJqgJT\_ ...}
\end{Highlighting}
\end{Shaded}

\section{Extracting Data From HTML
Elements}\label{extracting-data-from-html-elements}

Now that we've got the elements we care about extracted from the
complete document. But how do we get the data we need out of those
elements?

You'll usually get the data from either the contents of the HTML element
or else from one of it's attributes. If you're really lucky, the data
you need will already be formatted for you as a HTML table or list.

\subsection{Extracting text}\label{extracting-text}

The functions \texttt{rvest::html\_text()} and
\texttt{rvest::html\_text2()} can be used to extract the plain text
contents of an HTML element.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"\#teaching li"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{}  [1] "Developing and teaching modules in statistics, data science and data ethics."                      }
\CommentTok{\#\textgreater{}  [2] "Designing courses for both in{-}person and remote learning, predominantly at the postgraduate level."}
\CommentTok{\#\textgreater{}  [3] "Supervising undergraduate, postgraduate and doctoral research projects."                           }
\CommentTok{\#\textgreater{}  [4] "Adapting and leading short courses on scientific writing and communication."                       }
\CommentTok{\#\textgreater{}  [5] "Running workshops and computer labs for undergraduate and postgraduate modules."                   }
\CommentTok{\#\textgreater{}  [6] "Speaking at university open days and providing one{-}to{-}one tuition to high school students."        }
\CommentTok{\#\textgreater{}  [7] "effectively scope and manage a data science project;"                                              }
\CommentTok{\#\textgreater{}  [8] "work openly and reproducibly;"                                                                     }
\CommentTok{\#\textgreater{}  [9] "efficiently acquire, manipulate, and present data;"                                                }
\CommentTok{\#\textgreater{} [10] "interpret and explain your work for a variety of stakeholders;"                                    }
\CommentTok{\#\textgreater{} [11] "ensure that your work can be put into production;"                                                 }
\CommentTok{\#\textgreater{} [12] "assess the ethical implications of your work as a data scientist."}
\end{Highlighting}
\end{Shaded}

The difference between \texttt{html\_text()} and \texttt{html\_text2()}
is in how they handle whitespace. In HTML whitespace and line breaks
have very little influence over how the code is interpreted by the
computer (this is similar to R but very different from Python).
\texttt{html\_text()} will extract the text as it is in the raw html,
while \texttt{html\_text2()} will do its best to extract the text in a
way that gives you something similar to what you'd see in the browser.

\subsection{Extracting Attributes}\label{extracting-attributes}

Attributes are also used to record information that you might like to
collect. For example, the destination of links are stored in the
\texttt{href} attribute and the source of images is stored in the
\texttt{src} attribute.

As an example of this, consider trying to extract the twitter link from
the icon in the page footer. This is quite tricky to locate in the html
source, so I used the
\href{https://rvest.tidyverse.org/articles/selectorgadget.html}{Selector
Gadget} to help find the correct combination of elements.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{".compact:nth{-}child(1) .nav{-}link"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{html\_node\}}
\CommentTok{\#\textgreater{} \textless{}a class="nav{-}link" href="https://www.twitter.com/zakvarty"\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}i class="bi bi{-}twitter" role="img"\textgreater{}\textbackslash{}n\textless{}/i\textgreater{}}
\CommentTok{\#\textgreater{} [2] \textless{}span class="menu{-}text"\textgreater{}\textless{}/span\textgreater{}}
\end{Highlighting}
\end{Shaded}

To extract the \texttt{href} attribute from the scraped element, we use
the \texttt{rvest::html\_attr()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{".compact:nth{-}child(1) .nav{-}link"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "https://www.twitter.com/zakvarty" "https://www.twitter.com/zakvarty"}
\end{Highlighting}
\end{Shaded}

\textbf{Note:} \texttt{rvest::html\_attr()} will always return a
character string (or list of character strings). If you are extracting
an attribute that describes a quantity, such as the width of an image,
you'll need to convert this from a string to your required data type.
For example, of the width is measures in pixels you might use
\texttt{as.integer()}.

\subsection{Extracting tables}\label{extracting-tables}

HTML tables are composed in a similar, nested manner to LaTeX tables.

There are four main elements to know about that make up an HTML table:

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}table\textgreater{}},
\item
  \texttt{\textless{}tr\textgreater{}} (table row),
\item
  \texttt{\textless{}th\textgreater{}} (table heading),
\item
  \texttt{\textless{}td\textgreater{}} (table data).
\end{itemize}

Here's our simple example data, formatted as an HTML table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html\_2 }\OtherTok{\textless{}{-}} \FunctionTok{minimal\_html}\NormalTok{(}\StringTok{"}
\StringTok{  \textless{}table\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}th\textgreater{}Name\textless{}/th\textgreater{}}
\StringTok{      \textless{}th\textgreater{}Number\textless{}/th\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}td\textgreater{}A\textless{}/td\textgreater{}}
\StringTok{      \textless{}td\textgreater{}1\textless{}/td\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}td\textgreater{}B\textless{}/td\textgreater{}}
\StringTok{      \textless{}td\textgreater{}2\textless{}/td\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}td\textgreater{}C\textless{}/td\textgreater{}}
\StringTok{      \textless{}td\textgreater{}3\textless{}/td\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{  \textless{}/table\textgreater{}}
\StringTok{  "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since tables are a common way to store data, \texttt{\{rvest\}} includes
a useful function \texttt{html\_table()} that converts directly from an
HTML table into a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html\_2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"table"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_table}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   Name  Number}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 A          1}
\CommentTok{\#\textgreater{} 2 B          2}
\CommentTok{\#\textgreater{} 3 C          3}
\end{Highlighting}
\end{Shaded}

Applying this to our real scraped data we can easily extract the table
of taught courses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"table"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_table}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 27 x 3}
\CommentTok{\#\textgreater{}   Year      Course                                            Role    }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}                                             \textless{}chr\textgreater{}   }
\CommentTok{\#\textgreater{} 1 "2021{-}25" Data Science Notes Videos                         Lecturer}
\CommentTok{\#\textgreater{} 2 ""        Ethics in Data Science I, II and III Reading List Lecturer}
\CommentTok{\#\textgreater{} 3 ""        Data Ethics for Digital Chemistry                 Lecturer}
\CommentTok{\#\textgreater{} 4 ""        Y1 research projects: point process models Notes  Lecturer}
\CommentTok{\#\textgreater{} 5 "2021{-}22" Supervised Learning Videos                        Lecturer}
\CommentTok{\#\textgreater{} 6 ""                                                               }
\CommentTok{\#\textgreater{} \# i 21 more rows}
\end{Highlighting}
\end{Shaded}

\section{Tip for Building Tibbles}\label{tip-for-building-tibbles}

When scraping data from a webpage, your end-goal is typically going to
be constructing a data.frame or a tibble.

If you are following our description of tidy data, you'll want each row
to correspond some repeated unit on the HTML page. In this case, you
should

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use \texttt{html\_elements()} to select the elements that contain each
  observation unit;
\item
  Use \texttt{html\_element()} to extract the variables from each of
  those observations.
\end{enumerate}

Taking this approach guarantees that you'll get the same number of
values for each variable, because \texttt{html\_element()} always
returns the same number of outputs as inputs. This is vital when you
have missing data - when not every observation unit has a value for
every variable of interest.

As an example, consider this extract of text about the
\href{https://dplyr.tidyverse.org/reference/starwars.html\#ref-examples}{starwars
dataset}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars\_html }\OtherTok{\textless{}{-}} \FunctionTok{minimal\_html}\NormalTok{(}\StringTok{"}
\StringTok{  \textless{}ul\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}C{-}3PO\textless{}/b\textgreater{} is a \textless{}i\textgreater{}droid\textless{}/i\textgreater{} that weighs \textless{}span class=\textquotesingle{}weight\textquotesingle{}\textgreater{}167 kg\textless{}/span\textgreater{}\textless{}/li\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}R2{-}D2\textless{}/b\textgreater{} is a \textless{}i\textgreater{}droid\textless{}/i\textgreater{} that weighs \textless{}span class=\textquotesingle{}weight\textquotesingle{}\textgreater{}96 kg\textless{}/span\textgreater{}\textless{}/li\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}Yoda\textless{}/b\textgreater{} weighs \textless{}span class=\textquotesingle{}weight\textquotesingle{}\textgreater{}66 kg\textless{}/span\textgreater{}\textless{}/li\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}R4{-}P17\textless{}/b\textgreater{} is a \textless{}i\textgreater{}droid\textless{}/i\textgreater{}\textless{}/li\textgreater{}}
\StringTok{  \textless{}/ul\textgreater{}}
\StringTok{  "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is an unordered list where each list item corresponds to one
observational unit (one character from the starwars universe). The name
of the character is given in bold, the character species is specified in
italics and the weight of the character is denoted by the
\texttt{.weight} class. However, some characters have only a subset of
these variables defined: for example Yoda has no species entry.

If we try to extract each element directly, our vectors of variable
values are of different lengths. We don't know where the missing values
should be, so we can't line them back up to make a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "C{-}3PO"  "R2{-}D2"  "Yoda"   "R4{-}P17"}
\NormalTok{starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"i"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "droid" "droid" "droid"}
\NormalTok{starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{".weight"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "167 kg" "96 kg"  "66 kg"}
\end{Highlighting}
\end{Shaded}

What we should do instead is start by extracting all of the list item
elements using \texttt{html\_elements()}. Once we have done this, we can
then use \texttt{html\_element()} to extract each variable for all
characters. This will pad with NAs, so that we can collate them into a
tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars\_characters }\OtherTok{\textless{}{-}}\NormalTok{ starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"li"}\NormalTok{)}

\NormalTok{starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "C{-}3PO"  "R2{-}D2"  "Yoda"   "R4{-}P17"}
\NormalTok{starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"i"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "droid" "droid" NA      "droid"}
\NormalTok{starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{".weight"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "167 kg" "96 kg"  "66 kg"  NA}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\SpecialCharTok{::}\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{name =}\NormalTok{ starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{(),}
  \AttributeTok{species =}\NormalTok{ starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"i"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{(),}
  \AttributeTok{weight =}\NormalTok{ starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{".weight"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{}   name   species weight}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}   \textless{}chr\textgreater{} }
\CommentTok{\#\textgreater{} 1 C{-}3PO  droid   167 kg}
\CommentTok{\#\textgreater{} 2 R2{-}D2  droid   96 kg }
\CommentTok{\#\textgreater{} 3 Yoda   \textless{}NA\textgreater{}    66 kg }
\CommentTok{\#\textgreater{} 4 R4{-}P17 droid   \textless{}NA\textgreater{}}
\end{Highlighting}
\end{Shaded}

\section{Session Information}\label{session-information-4}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{other attached packages:} rvest(v.1.0.4)

\textbf{loaded via a namespace (and not attached):}
\emph{vctrs(v.0.6.5)}, \emph{httr(v.1.4.7)}, \emph{cli(v.3.6.3)},
\emph{knitr(v.1.49)}, \emph{rlang(v.1.1.4)}, \emph{xfun(v.0.50)},
\emph{stringi(v.1.8.4)}, \emph{jsonlite(v.1.8.9)}, \emph{glue(v.1.8.0)},
\emph{selectr(v.0.4-2)}, \emph{htmltools(v.0.5.8.1)},
\emph{rmarkdown(v.2.29)}, \emph{pander(v.0.6.5)},
\emph{evaluate(v.1.0.3)}, \emph{tibble(v.3.2.1)},
\emph{fastmap(v.1.2.0)}, \emph{yaml(v.2.3.10)},
\emph{lifecycle(v.1.0.4)}, \emph{stringr(v.1.5.1)},
\emph{compiler(v.4.4.2)}, \emph{Rcpp(v.1.0.14)},
\emph{pkgconfig(v.2.0.3)}, \emph{rstudioapi(v.0.17.1)},
\emph{digest(v.0.6.37)}, \emph{R6(v.2.5.1)}, \emph{utf8(v.1.2.4)},
\emph{pillar(v.1.10.1)}, \emph{curl(v.6.1.0)}, \emph{magrittr(v.2.0.3)},
\emph{tools(v.4.4.2)} and \emph{xml2(v.1.3.6)}

\chapter{APIs}\label{data-apis}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{Aquiring Data Via an API}\label{aquiring-data-via-an-api}

We've already established that you can't always rely on tidy, tabular
data to land on your desk.

Sometimes you are going to have to go out and gather data for yourself.
We have already seen how to scrape information directly from the HTML
source of a webpage. But surely there has to be an easer way.
Thankfully, there often is!

In this chapter we will cover the basics of obtaining data via an API.
This material draws together the
\href{https://zapier.com/learn/apis/}{Introduction to APIs} book by
Brian Cooksey and the
\href{https://stat545.com/diy-web-data.html\#interacting-with-an-api}{DIY
web data} section of STAT545 at the University of British Columbia.

\section{Why do I need to know about
APIs?}\label{why-do-i-need-to-know-about-apis}

\begin{quote}
An API, or application programming interface, is a set of rules that
allows different software applications to communicate with each other.
\end{quote}

As a data scientist, you will often need to access data that is stored
on remote servers or in cloud-based services. APIs provide a convenient
way for data scientists to programmatically retrieve this data, without
having to manually download data sets or and process them locally on
their own computer.

This has multiple benefits including automation and standardisation of
data sharing.

\begin{itemize}
\item
  \textbf{Automation:} It is much faster for a machine to process a data
  request than a human. Having a machine handling data requests also
  scales much better as either the number or the complexity of data
  requests grows. Additionally, there is a lower risk of introducing
  human error. For example, a human might accidentally share the wrong
  data, which can have serious legal repercussions.
\item
  \textbf{Standardisation:} Having a machine process data requests
  requires the format of these requests and the associated responses to
  be standardised. This allows data sharing and retrieval to become a
  reproducible and programmatic aspect of our work.
\end{itemize}

\section{What is an API?}\label{what-is-an-api}

So then, if APIs are so great, what exactly are they?

In human-to-human communication, the set of rules governing acceptable
behaviour is known as etiquette. Depending on when or where you live,
social etiquette can be rather strict. The rules for
computer-to-computer communication take this to a whole new level,
because with machines there can be no room left for interpretation.

The set of rules governing interactions between computers or programmes
is known as a \textbf{protocol}.

APIs provide a standard protocol for different programs to interact with
one another. This makes it easier for developers to build complex
systems by leveraging the functionality of existing services and
platforms. The benefits of working in a standardised and modular way
apply equally well to sharing data as they do to writing code or
organising files.

There are two sides to communication and when \emph{machines}
communicate these are known as the \textbf{server} and the
\textbf{client}.

Servers can seem intimidating, because unlike your laptop or mobile
phone they don't have their own input and output devices; they have no
keyboard, no monitor, and no a mouse. Despite this, servers are just
regular computers that are designed to store data and run programmes.
Servers don't have their own input or output devices because they are
intended to be used \emph{remotely}, via another computer. There is no
need for a screen or a mouse if the user is miles away. Nothing scary
going on here!

People often find clients much less intimidating - they are simply any
other computer or application that might contact the sever.

\section{HTTP}\label{http}

This leads us one step further down the rabbit-hole. An API is a
protocol that defines the rules of how applications communicate with one
another. But how does this communication happen?

HTTP (Hypertext Transfer Protocol) is the dominant mode communication on
the World Wide Web. You can see the secure version of HTTP, HTTPS, at
the start of most web addresses up at the top of your browser. For
example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{https://www.zakvarty.com/blog}
\end{Highlighting}
\end{Shaded}

HTTP is the foundation of data communication on the web and is used to
transfer files (such as text, images, and videos) between web servers
and clients.

To understand HTTP communications, I find it helpful to imagine the
client and the server as being a customer and a waiter at a restaurant.
The client makes some request to the server, which then tries to comply
before giving a response. The server might respond to confirm that the
request was completed successfully. Alternatively, the server might
respond with an error message, which is (hopefully) informative about
why the request could not be completed.

This request-response model is the basis for HTTP, the communication
system used by the majority of APIs.

\section{HTTP Requests}\label{http-requests}

An HTTP request consists of:

\begin{itemize}
\tightlist
\item
  Uniform Resource Locator (URL) {[}unique identifier for a thing{]}
\item
  Method {[}tells server the type of action requested by client{]}
\item
  Headers {[}meta-information about request, e.g.~device type{]}
\item
  Body {[}Data the client wants to send to the server{]}
\end{itemize}

\subsection{URL}\label{url}

The URL in a HTTP request specifies where that request is going to be
made, for example \texttt{http://example.com}.

\subsection{Method}\label{method}

The action that the client wants to take is indicated by a set of
well-defined methods or HTTP verbs. The most common HTTP verbs are
\texttt{GET}, \texttt{POST}, \texttt{PUT}, \texttt{PATCH}, and
\texttt{DELETE}.

The \texttt{GET} verb is used to retrieve a resource from the server,
such as a web page or an image. The \texttt{POST} verb is used to send
data to the server, such as when submitting a form or uploading a file.
The \texttt{PUT} verb is used to replace a resource on the server with a
new one, while the \texttt{PATCH} verb is used to update a resource on
the server without replacing it entirely. Finally, the \texttt{DELETE}
verb is used to delete a resource from the server.

In addition to these common HTTP verbs, there are also several less
frequently used verbs. These are used for specialized purposes, such as
requesting only the headers of a resource, or testing the connectivity
between the client and the server.

\subsection{Header}\label{header}

The request headers contain meta-information about the request. This is
where information about the device type would be included within the
request.

\subsection{Body}\label{body}

Finally, the body of the request contains the data that the client is
providing to the server.

\section{HTTP Responses}\label{http-responses}

When the server receives a request it will attempt to fulfil it and then
send a response back to the client.

A response has a similar structure to a request apart from:

\begin{itemize}
\tightlist
\item
  responses \textbf{do not have} a URL,
\item
  responses \textbf{do not have} a method,
\item
  responses \textbf{have} a status code.
\end{itemize}

\subsection{Status Codes}\label{status-codes}

The status code is a 3 digit number, each of which has a specific
meaning. Some common error codes that you might (already have) come
across are:

\begin{itemize}
\tightlist
\item
  200: Success,
\item
  404: Page not found (all 400s are errors),
\item
  503: Page down.
\end{itemize}

In a data science context, a successful response will return the
requested data within the data field. This will most likely be given in
JSON or XML format.

\section{Authentication}\label{authentication}

Now that we know \emph{how} applications communicate, you might ask how
we can control who has access to the API and what types of request they
can make. This can be done by the server setting appropriate permissions
for each client. But then how does the server verify that the client is
really who is claims to be?

\textbf{Authentication} is a way to ensure that only authorized clients
are able to access an API. This is typically done by the server
requiring each client to provide some secret information that uniquely
identifies them, whenever they make requests to the API. This
information allows the API server to validate the authenticity this user
before it authorises the request.

\subsection{Basic Authentication}\label{basic-authentication}

There are various ways to implement API authentication.

Basic authentication involves each legitimate client having a username
and password. An encrypted version of these is included in the
\texttt{Authorization} header of the HTTP request. If the hear matches
with the server's records then the request is processed. If not, then a
special status code (401) is returned to the client.

Basic authentication is dangerous because it does not put any
restrictions on what a client can do once they are authorised.
Additional, individualised restrictions can be added by using an
alternative authentication scheme.

\subsection{API Key Authentication}\label{api-key-authentication}

An API key is long, random string of letters and numbers that is
assigned to each authorised user. An API key is distinct from the user's
password and keys are typically issued by the service that provides an
API. Using keys rather than basic authentication allows the API provider
to track and limit the usage of their API.

For example, a provider may issue a unique API key to each developer or
organization that wants to use the API. The provider can then limit
access to certain data. They could also limit the number of requests
that each key can make in a given time period or prevent access to
certain administrative functions, like changing passwords or deleting
accounts.

Unlike Basic Authentication, there is no standard way of a client
sharing a key with the server. Depending on the API this might be in the
\texttt{Authorization} field of the header, at the end of the URL
(\texttt{http://example.com?api\_key=my\_secret\_key}), or within the
body of the data.

\section{API wrappers}\label{api-wrappers}

We've learned a lot about how the internet works. Fortunately, a lot of
the time we won't have to worry about all of that new information other
than for debugging purposes.

In the best case scenario, a very kind developer has written a
``wrapper'' function for the API. These wrappers are functions in R that
will construct the HTTP request for you. If you are particularly lucky,
the API wrapper will also format the response for you, converting it
from XML or JSON back into an R object that is ready for immediate use.

\section{\texorpdfstring{\texttt{\{geonames\}}
wrapper}{\{geonames\} wrapper}}\label{geonames-wrapper}

\href{https://ropensci.org/}{rOpenSci} has a curated list of many
wrappers for accessing scientific data using R. We will focus on the
\href{https://www.geonames.org/}{GeoNames API}, which gives open access
to a geographical database. To access this data, we will use wrapper
functions provided by the \texttt{\{geonames\}}
\href{https://docs.ropensci.org/geonames/}{package}.

The aim here is to illustrate the important steps of getting started
with a new API.

\subsection{Set up}\label{set-up}

Before we can get any data from the GeoNames API, we first need to do a
little bit of set up.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Install and load \texttt{\{geonames\}} from CRAN
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("geonames")}
\FunctionTok{library}\NormalTok{(geonames)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Create a \href{https://www.geonames.org/login}{user account} for the
  GeoNames API
\end{enumerate}

\begin{center}
\includegraphics[width=0.8\textwidth,height=\textheight]{images/203-data-apis/sign-up.png}
\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Activate the account (see activation email)
\end{enumerate}

\begin{center}
\includegraphics[width=0.8\textwidth,height=\textheight]{images/203-data-apis/confirmation-email.png}
\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Enable the free web services for your GeoNames account by logging in
  at this \href{http://www.geonames.org/enablefreewebservice}{link}.
\item
  Tell R your credentials for GeoNames.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-warning-color-frame, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

We could use the following code to tell R our credentials, \textbf{but
we absolutely should not}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{geonamesUsername =} \StringTok{"example\_username"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This would save our username as an environment variable, but it
\emph{also} puts our API credentials directly into the script. If we
share the script with others (internally, externally or publicly) we
would be sharing our credentials too. Not good!

\end{tcolorbox}

\subsection{Keep it Secret, Keep it
Safe}\label{keep-it-secret-keep-it-safe}

The solution to this problem is to add our credentials as environment
variables in our \texttt{.Rprofile} rather than in this script. The
\texttt{.Rprofile} is an R script that is run at the start of every
session. It can be created and edited directly, but can also be created
and edited from within R.

To make/open your \texttt{.Rprofile} use the \texttt{edit\_r\_profile()}
function from the \texttt{\{usethis\}} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}
\NormalTok{usethis}\SpecialCharTok{::}\FunctionTok{edit\_r\_profile}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Within this file, add
\texttt{options(geonamesUsername="example\_username")} on a new line,
remembering to replace \texttt{example\_username} with your own GeoNames
username.

The final step is to \textbf{check this this file ends with a blank
line}, save it and restart R. Then we are all set to start using
\texttt{\{geonames\}}.

This set up procedure is indicative of most API wrappers, but of course
the details will vary between each API. This is why good documentation
is important!

If you are using \texttt{\{renv\}} to track the versions of R and the
packages you are using take care. For \texttt{\{renv\}} to be effective
you have to place your project level \texttt{.Rprofile} under version
control - this means you might accidentally share your API credentials.

A workaround for this problem is to create an R file that you
\texttt{source()} from within the project level \texttt{.Rprofile} but
is included in \texttt{.gitignore}, so that your credentials remain
secret.

\subsection{\texorpdfstring{Using
\texttt{\{geonames\}}}{Using \{geonames\}}}\label{using-geonames}

GeoNames has a whole host of
\href{http://www.geonames.org/export/ws-overview.html}{different
geo-datasets} that you can explore. As a first example, let's get all of
the geo-tagged wikipedia articles that are within 1km of Imperial
College London.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imperial\_coords }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{lat =} \FloatTok{51.49876}\NormalTok{, }\AttributeTok{lon =} \SpecialCharTok{{-}}\FloatTok{0.1749}\NormalTok{)}
\NormalTok{search\_radius\_km }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{imperial\_neighbours }\OtherTok{\textless{}{-}}\NormalTok{ geonames}\SpecialCharTok{::}\FunctionTok{GNfindNearbyWikipedia}\NormalTok{(}
  \AttributeTok{lat =}\NormalTok{ imperial\_coords}\SpecialCharTok{$}\NormalTok{lat,}
  \AttributeTok{lng =}\NormalTok{ imperial\_coords}\SpecialCharTok{$}\NormalTok{lon, }
  \AttributeTok{radius =}\NormalTok{ search\_radius\_km,}
  \AttributeTok{lang =} \StringTok{"en"}\NormalTok{,                }\CommentTok{\# english language articles}
  \AttributeTok{maxRows =} \DecValTok{500}              \CommentTok{\# maximum number of results to return }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Looking at the structure of \texttt{imperial\_neighbours} we can see
that it is a data frame with one row per geo-tagged wikipedia article.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(imperial\_neighbours)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    204 obs. of  13 variables:}
\CommentTok{\#\textgreater{}  $ summary     : chr  "The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at "| \_\_truncated\_\_ "Imperial College Business School is a global business school located in London. The business school was opened "| \_\_truncated\_\_ "Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est"| \_\_truncated\_\_ "Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one"| \_\_truncated\_\_ ...}
\CommentTok{\#\textgreater{}  $ elevation   : chr  "20" "18" "19" "24" ...}
\CommentTok{\#\textgreater{}  $ feature     : chr  "edu" "edu" "landmark" "edu" ...}
\CommentTok{\#\textgreater{}  $ lng         : chr  "{-}0.1746" "{-}0.1748" "{-}0.17425" "{-}0.1757" ...}
\CommentTok{\#\textgreater{}  $ distance    : chr  "0.0335" "0.0494" "0.0508" "0.0558" ...}
\CommentTok{\#\textgreater{}  $ rank        : chr  "81" "91" "90" "96" ...}
\CommentTok{\#\textgreater{}  $ lang        : chr  "en" "en" "en" "en" ...}
\CommentTok{\#\textgreater{}  $ title       : chr  "Department of Mechanical Engineering, Imperial College London" "Imperial College Business School" "Exhibition Road" "Imperial College School of Medicine" ...}
\CommentTok{\#\textgreater{}  $ lat         : chr  "51.498524" "51.4992" "51.4989722222222" "51.4987" ...}
\CommentTok{\#\textgreater{}  $ wikipediaUrl: chr  "en.wikipedia.org/wiki/Department\_of\_Mechanical\_Engineering\%2C\_Imperial\_College\_London" "en.wikipedia.org/wiki/Imperial\_College\_Business\_School" "en.wikipedia.org/wiki/Exhibition\_Road" "en.wikipedia.org/wiki/Imperial\_College\_School\_of\_Medicine" ...}
\CommentTok{\#\textgreater{}  $ countryCode : chr  NA "AE" NA "GB" ...}
\CommentTok{\#\textgreater{}  $ thumbnailImg: chr  NA NA NA NA ...}
\CommentTok{\#\textgreater{}  $ geoNameId   : chr  NA NA NA NA ...}
\end{Highlighting}
\end{Shaded}

To confirm we have the correct location we can inspect the title of the
first five neighbours.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imperial\_neighbours}\SpecialCharTok{$}\NormalTok{title[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] "Department of Mechanical Engineering, Imperial College London"             }
\CommentTok{\#\textgreater{} [2] "Imperial College Business School"                                          }
\CommentTok{\#\textgreater{} [3] "Exhibition Road"                                                           }
\CommentTok{\#\textgreater{} [4] "Imperial College School of Medicine"                                       }
\CommentTok{\#\textgreater{} [5] "Department of Civil and Environmental Engineering, Imperial College London"}
\end{Highlighting}
\end{Shaded}

Nothing too surprising here, mainly departments of the college and
Exhibition Road, which runs along one side of the campus. These sorts of
check are important - I initially forgot the minus in the longitude and
was getting results in East London!

\section{What if there is no
wrapper?}\label{what-if-there-is-no-wrapper}

If there is not a wrapper function, we can still access APIs fairly
easilty using the \texttt{\{httr\}} package.

We will look at an example using \href{http://www.omdbapi.com/}{OMDb},
which is an open source version of \href{https://www.imdb.com/}{IMDb},
to get information about the movie Mean Girls.

To use the OMDB API you will once again need to
\href{http://www.omdbapi.com/apikey.aspx}{request a free API key},
follow a verification link and add your API key to your
\texttt{.Rprofile}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add this to .Rprofile, pasting in your own API key}
\FunctionTok{options}\NormalTok{(}\AttributeTok{OMDB\_API\_Key =} \StringTok{"PASTE YOUR KEY HERE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can then restart R and safely access your API key from within your R
session.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load your API key into the current R session}
\NormalTok{ombd\_api\_key }\OtherTok{\textless{}{-}} \FunctionTok{getOption}\NormalTok{(}\StringTok{"OMDB\_API\_Key"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using the documentation for the API, requests have URLs of the following
form, where terms in angular brackets should be replaced by you.

\begin{verbatim}
http://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>
\end{verbatim}

With a little bit of effort, we can write a function that composes this
type of request URL for us. We will using the \texttt{\{glue\}} package
to help us join strings together.

Running the example we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_girls\_request }\OtherTok{\textless{}{-}} \FunctionTok{omdb\_url}\NormalTok{(}
  \AttributeTok{title =} \StringTok{"mean+girls"}\NormalTok{,}
  \AttributeTok{year =}  \StringTok{"2004"}\NormalTok{,}
  \AttributeTok{plot =} \StringTok{"short"}\NormalTok{,}
  \AttributeTok{format =}  \StringTok{"json"}\NormalTok{,}
  \AttributeTok{api\_key =}  \FunctionTok{getOption}\NormalTok{(}\StringTok{"OMDB\_API\_Key"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We can then use the \texttt{\{httr\}} package to construct our request
and store the response we get.

\begin{verbatim}
#> [1] 200
\end{verbatim}

Thankfully it was a success! If you get a 401 error code here, check
that you have clicked the activation link for your API key.

The full structure of the response is quite complicated, but we can
easily extract the requested data using \texttt{content()}

\begin{verbatim}
#> $Title
#> [1] "Mean Girls"
#> 
#> $Year
#> [1] "2004"
#> 
#> $Rated
#> [1] "PG-13"
#> 
#> $Released
#> [1] "30 Apr 2004"
#> 
#> $Runtime
#> [1] "97 min"
#> 
#> $Genre
#> [1] "Comedy"
#> 
#> $Director
#> [1] "Mark Waters"
#> 
#> $Writer
#> [1] "Rosalind Wiseman, Tina Fey"
#> 
#> $Actors
#> [1] "Lindsay Lohan, Jonathan Bennett, Rachel McAdams"
#> 
#> $Plot
#> [1] "Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George."
#> 
#> $Language
#> [1] "English, German, Vietnamese, Swahili"
#> 
#> $Country
#> [1] "United States, Canada"
#> 
#> $Awards
#> [1] "7 wins & 25 nominations total"
#> 
#> $Poster
#> [1] "https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg"
#> 
#> $Ratings
#> $Ratings[[1]]
#> $Ratings[[1]]$Source
#> [1] "Internet Movie Database"
#> 
#> $Ratings[[1]]$Value
#> [1] "7.1/10"
#> 
#> 
#> $Ratings[[2]]
#> $Ratings[[2]]$Source
#> [1] "Rotten Tomatoes"
#> 
#> $Ratings[[2]]$Value
#> [1] "84%"
#> 
#> 
#> $Ratings[[3]]
#> $Ratings[[3]]$Source
#> [1] "Metacritic"
#> 
#> $Ratings[[3]]$Value
#> [1] "66/100"
#> 
#> 
#> 
#> $Metascore
#> [1] "66"
#> 
#> $imdbRating
#> [1] "7.1"
#> 
#> $imdbVotes
#> [1] "454,106"
#> 
#> $imdbID
#> [1] "tt0377092"
#> 
#> $Type
#> [1] "movie"
#> 
#> $DVD
#> [1] "N/A"
#> 
#> $BoxOffice
#> [1] "$86,058,055"
#> 
#> $Production
#> [1] "N/A"
#> 
#> $Website
#> [1] "N/A"
#> 
#> $Response
#> [1] "True"
\end{verbatim}

\section{Wrapping up}\label{wrapping-up-2}

We have learned a bit more about how the internet works, the benefits of
using an API to share data and how to request data from Open APIs.

When obtaining data from the internet it's vital that you keep your
credentials safe, and that don't do more work than is needed.

\begin{itemize}
\item
  Keep your API keys out of your code. Store them in your
  \texttt{.Rprofile} (and make sure this is not under version control!)
\item
  Scraping is always a last resort. Is there an API already?
\item
  Writing your own code to access an API can be more painful than
  necessary.
\item
  Don't repeat other people, if a suitable wrapper exists then use it.
\end{itemize}

\section{Session Information}\label{session-information-5}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{other attached packages:} geonames(v.0.999)

\textbf{loaded via a namespace (and not attached):}
\emph{digest(v.0.6.37)}, \emph{R6(v.2.5.1)}, \emph{fastmap(v.1.2.0)},
\emph{xfun(v.0.50)}, \emph{glue(v.1.8.0)}, \emph{knitr(v.1.49)},
\emph{htmltools(v.0.5.8.1)}, \emph{rmarkdown(v.2.29)},
\emph{cli(v.3.6.3)}, \emph{pander(v.0.6.5)}, \emph{compiler(v.4.4.2)},
\emph{httr(v.1.4.7)}, \emph{rstudioapi(v.0.17.1)},
\emph{tools(v.4.4.2)}, \emph{curl(v.6.1.0)}, \emph{evaluate(v.1.0.3)},
\emph{Rcpp(v.1.0.14)}, \emph{yaml(v.2.3.10)}, \emph{rlang(v.1.1.4)} and
\emph{jsonlite(v.1.8.9)}

\chapter*{Checklist}\label{data-checklist}
\addcontentsline{toc}{chapter}{Checklist}

\markboth{Checklist}{Checklist}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section*{Videos / Chapters}\label{videos-chapters-1}
\addcontentsline{toc}{section}{Videos / Chapters}

\markright{Videos / Chapters}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d80e9045-22e7-4a0e-a0fc-af8100d3e727}{Tabular
  Data} (27 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-01-tabular-data-and-csvs/02-01-tabular-data.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=239ba39e-8a06-4e7b-a6c1-af7200f91d2b}{Web
  Scraping} (22 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-02-webscraping/02-02-web-scraping.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=e1ed8e4f-cbaa-40c2-8c44-af7200ee2e9f}{APIs}
  (25 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-03-apis/02-03-apis.pdf}{{[}slides{]}}
\end{itemize}

\section*{Reading}\label{reading-1}
\addcontentsline{toc}{section}{Reading}

\markright{Reading}

Use the \hyperref[data-reading]{Acquiring and Sharing Data section} of
the reading list to support and guide your exploration of this week's
topics. Note that these texts are divided into core reading, reference
materials and materials of interest.

\section*{Tasks}\label{tasks-1}
\addcontentsline{toc}{section}{Tasks}

\markright{Tasks}

\emph{Core:}

\begin{itemize}
\item
  Revisit the Projects that you explored on Github last week. This time
  look for any data or documentation files.

  \begin{itemize}
  \tightlist
  \item
    Are there any file types that are new to you?
  \item
    If so, are there packages or helper function that would let you read
    this data into R?
  \item
    Why might you not find many data files on Github?
  \end{itemize}
\item
  Play \href{https://flukeout.github.io/}{CSS Diner} to familiarise
  yourself with some CSS selectors.
\item
  Identify 3 APIs that give access to data on topics that interest you.
  Write a post on the discussion forum describing the APIs and use one
  of them to load some data into R.
\item
  Scraping Book Reviews:

  \begin{itemize}
  \tightlist
  \item
    Visit the Amazon page for R for Data Science. Write code to scrape
    the percentage of customers giving each ``star'' rating (5,
    \ldots, 1).
  \item
    Turn your code into a function that will return a tibble of the
    form:
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0877}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1316}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1316}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1316}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1316}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1316}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1404}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
product
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_reviews
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
percent\_5\_star
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
percent\_4\_star
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
percent\_3\_star
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
percent\_2\_star
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
percent\_1\_star
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
url
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
example\_name & 1000 & 20 & 20 & 20 & 20 & 20 & www.example.com \\
\end{longtable}

\begin{itemize}
\item
  Generalise your function to work for other Amazon products, where the
  function takes as input a vector of product names and an associated
  vector of URLs.
\item
  Use your function to compare the reviews of the following three books:
  \href{https://www.amazon.com/Data-Science-Transform-Visualize-Model/dp/1491910399/ref=sr_1_1?keywords=r+for+data+science&qid=1674145765&s=books&sprefix=R+for+data+\%2Cstripbooks-intl-ship\%2C157&sr=1-1}{R
  for Data Science},
  \href{https://www.amazon.com/Packages-Organize-Test-Document-Share/dp/1491910593/ref=sr_1_1?crid=XWR8O7WPKZS9&keywords=R+packages&qid=1674145743&s=books&sprefix=r+package\%2Cstripbooks-intl-ship\%2C158&sr=1-1}{R
  packages} and
  \href{https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/331924275X/ref=sr_1_1?crid=24WRUZ93PL2E6&keywords=ggplot2&qid=1674145703&s=books&sprefix=ggplot2\%2Cstripbooks-intl-ship\%2C190&sr=1-1}{ggplot2}.
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\tightlist
\item
  Add this function to the R package you made last week, remembering to
  add tests and documentation.
\end{itemize}

\section*{Live Session}\label{live-session-1}
\addcontentsline{toc}{section}{Live Session}

\markright{Live Session}

In the live session we will begin with a discussion of this week's
tasks. We will then work through some examples of how to read data from
non-standard sources.

Please come to the live session prepared to discuss the following
points:

\begin{itemize}
\item
  Roger Peng states that files can be imported and exported using
  \texttt{readRDS()} and \texttt{saveRDS()} for fast and space efficient
  data storage. What is the downside to doing so?
\item
  What data types have you come across (that we have not discussed
  already) and in what context are they used?
\item
  What do you have to give greater consideration to when scraping data
  than when using an API?
\end{itemize}

\part{Data Exploration and Visualisation}

\section*{Introduction}\label{edav-introduction}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

Now that we have read our raw data into R we can start getting our data
science project moving and being to see some initial returns on the time
and effort that we have invested so far.

In this section we will explore how to wrangle, explore and visualise
the data that forms the basis of our projects. These skills are often
overlooked by folks coming into data science as being ``soft skills''
compared to modelling. However, I would argue that this is not the case
because each of these tasks requires its own specialist knowledge and
tools.

Additionally, these task make up the majority of data scientist's work
and are often where we can add the most value to an organisation. At
this stage in a project we turn useless, messy data into a form that can
be used; we derive initial insights from this data while making minimal
assumptions; and we communicate all of this in an accurate and engaging
way, to drive decision making both within and outwith the organisation.

\chapter{Data Wrangling}\label{edav-wrangling}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{What is Data Wrangling?}\label{what-is-data-wrangling}

Okay, so you've got some data. That's a great start!

You might have had it handed to you by a collaborator,
\hyperref[data-apis]{requested it via an API} or
\hyperref[data-webscraping]{scraped it from the raw html of a webpage}.
In the worst case scenario, you're an \emph{actual} scientist (not just
a \emph{data} one) and you spent the last several months of your life
painstakingly measuring flower petals or car parts. Now we really want
to do something useful with that data.

We've seen already how you can load the data into R and pivot between
wider and longer formats, but that probably isn't enough to satisfy your
curiosity. You want to be able to view your data, manipulate and subset
it, create new variables from existing ones and cross-reference your
dataset with others. All of these are things possible in R and are known
under various collective names including data manipulation, data munging
and data wrangling.

I've decided to use the term data wrangling here. That's because data
manipulation sounds boring as heck and data munging is both unpleasant
to say and makes me imagine we are squelching through some sort of
information swamp.

In what follows, I'll give a fly-by tour of tools for data wrangling in
R, showing some examples along the way. I'll focus on some of the most
common and useful operations and link out to some more extensive guides
for wrangling your data in R, that you can refer back to as you need
them.

\section{Example Data Sets}\label{example-data-sets}

\begin{center}
\includegraphics[width=0.6\textwidth,height=\textheight]{images/301-edav-wrangling/palmer-penguins.png}
\end{center}

To demonstrate some standard skills we will use two datasets. The
\texttt{mtcars} data comes built into any R installation. The second
data set we will look at is the \texttt{penguins} data from
\texttt{\{palmerpenguins\}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(palmerpenguins)}
\NormalTok{penguins }\OtherTok{\textless{}{-}}\NormalTok{ palmerpenguins}\SpecialCharTok{::}\NormalTok{penguins}
\NormalTok{cars }\OtherTok{\textless{}{-}}\NormalTok{ datasets}\SpecialCharTok{::}\NormalTok{mtcars}
\end{Highlighting}
\end{Shaded}

\section{Viewing Your Data}\label{viewing-your-data}

\subsection{\texorpdfstring{\texttt{View()}}{View()}}\label{view}

The \texttt{View()} function can be used to create a spreadsheet-like
view of your data. In RStudio this will open as a new tab.

\texttt{View()} will work for any ``matrix-like'' R object, such as a
tibble, data frame, vector or matrix. Note the capital letter - the
function is called \texttt{View()}, not \texttt{view()}.

\begin{center}
\includegraphics[width=0.9\textwidth,height=\textheight]{images/301-edav-wrangling/view-penguins-screenshot.png}
\end{center}

\subsection{\texorpdfstring{\texttt{head()}}{head()}}\label{head}

For large data sets, you might not want (or be able to) view it all at
once. You can then use \texttt{head()} to view the first few rows. The
integer argument \texttt{n} specifies the number of rows you would like
to return.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(}\AttributeTok{x =}\NormalTok{ penguins, }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 8}
\CommentTok{\#\textgreater{}   species island   bill\_length\_mm bill\_depth\_mm flipper\_length\_mm body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}             \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}             \textless{}int\textgreater{}       \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgers\textasciitilde{}           39.1          18.7               181        3750}
\CommentTok{\#\textgreater{} 2 Adelie  Torgers\textasciitilde{}           39.5          17.4               186        3800}
\CommentTok{\#\textgreater{} 3 Adelie  Torgers\textasciitilde{}           40.3          18                 195        3250}
\CommentTok{\#\textgreater{} \# i 2 more variables: sex \textless{}fct\textgreater{}, year \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{str()}}{str()}}\label{str}

An alternative way to view a data set that is large or has a complicated
format is to examine its structure using \texttt{str()}. This is a
useful way to inspect list-like objects with a nested structure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(penguins)}
\CommentTok{\#\textgreater{} tibble [344 x 8] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ species          : Factor w/ 3 levels "Adelie","Chinstrap",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{\#\textgreater{}  $ island           : Factor w/ 3 levels "Biscoe","Dream",..: 3 3 3 3 3 3 3 3 3 3 ...}
\CommentTok{\#\textgreater{}  $ bill\_length\_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...}
\CommentTok{\#\textgreater{}  $ bill\_depth\_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...}
\CommentTok{\#\textgreater{}  $ flipper\_length\_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...}
\CommentTok{\#\textgreater{}  $ body\_mass\_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...}
\CommentTok{\#\textgreater{}  $ sex              : Factor w/ 2 levels "female","male": 2 1 1 NA 1 2 1 2 NA NA ...}
\CommentTok{\#\textgreater{}  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{names()}}{names()}}\label{names}

If you just want to access the variable names you can do so with the
\texttt{names()} function from base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(penguins)}
\CommentTok{\#\textgreater{} [1] "species"           "island"            "bill\_length\_mm"   }
\CommentTok{\#\textgreater{} [4] "bill\_depth\_mm"     "flipper\_length\_mm" "body\_mass\_g"      }
\CommentTok{\#\textgreater{} [7] "sex"               "year"}
\end{Highlighting}
\end{Shaded}

Similarly, you can explicitly access the row and column names of a data
frame or tibble using \texttt{colnames()} or \texttt{rownames()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(cars)}
\CommentTok{\#\textgreater{}  [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"}
\CommentTok{\#\textgreater{} [11] "carb"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(cars)}
\CommentTok{\#\textgreater{}  [1] "Mazda RX4"           "Mazda RX4 Wag"       "Datsun 710"         }
\CommentTok{\#\textgreater{}  [4] "Hornet 4 Drive"      "Hornet Sportabout"   "Valiant"            }
\CommentTok{\#\textgreater{}  [7] "Duster 360"          "Merc 240D"           "Merc 230"           }
\CommentTok{\#\textgreater{} [10] "Merc 280"            "Merc 280C"           "Merc 450SE"         }
\CommentTok{\#\textgreater{} [13] "Merc 450SL"          "Merc 450SLC"         "Cadillac Fleetwood" }
\CommentTok{\#\textgreater{} [16] "Lincoln Continental" "Chrysler Imperial"   "Fiat 128"           }
\CommentTok{\#\textgreater{} [19] "Honda Civic"         "Toyota Corolla"      "Toyota Corona"      }
\CommentTok{\#\textgreater{} [22] "Dodge Challenger"    "AMC Javelin"         "Camaro Z28"         }
\CommentTok{\#\textgreater{} [25] "Pontiac Firebird"    "Fiat X1{-}9"           "Porsche 914{-}2"      }
\CommentTok{\#\textgreater{} [28] "Lotus Europa"        "Ford Pantera L"      "Ferrari Dino"       }
\CommentTok{\#\textgreater{} [31] "Maserati Bora"       "Volvo 142E"}
\end{Highlighting}
\end{Shaded}

In the \texttt{cars} data, the car model are stored as the row names.
This doesn't really jive with our idea of tidy data - we'll see how to
fix that shortly.

\section{Renaming Variables}\label{renaming-variables}

\subsection{\texorpdfstring{\texttt{colnames()}}{colnames()}}\label{colnames}

The function \texttt{colnames()} can be used to set, as well as to
retrieve, column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars }
\FunctionTok{colnames}\NormalTok{(cars\_renamed)[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"miles\_per\_gallon"}
\FunctionTok{colnames}\NormalTok{(cars\_renamed)}
\CommentTok{\#\textgreater{}  [1] "miles\_per\_gallon" "cyl"              "disp"            }
\CommentTok{\#\textgreater{}  [4] "hp"               "drat"             "wt"              }
\CommentTok{\#\textgreater{}  [7] "qsec"             "vs"               "am"              }
\CommentTok{\#\textgreater{} [10] "gear"             "carb"}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{dplyr::rename()}}{dplyr::rename()}}\label{dplyrrename}

We can also use functions from \texttt{\{dplyr\}} to rename columns.
Let's alter the second column name.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(}\AttributeTok{.data =}\NormalTok{ cars\_renamed, }\AttributeTok{cylinders =}\NormalTok{ cyl)}
\FunctionTok{colnames}\NormalTok{(cars\_renamed)}
\CommentTok{\#\textgreater{}  [1] "miles\_per\_gallon" "cylinders"        "disp"            }
\CommentTok{\#\textgreater{}  [4] "hp"               "drat"             "wt"              }
\CommentTok{\#\textgreater{}  [7] "qsec"             "vs"               "am"              }
\CommentTok{\#\textgreater{} [10] "gear"             "carb"}
\end{Highlighting}
\end{Shaded}

This could be done as part of a pipe if we were making many alterations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{displacement =}\NormalTok{ disp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{horse\_power =}\NormalTok{ hp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{rear\_axel\_ratio =}\NormalTok{ drat)}

\FunctionTok{colnames}\NormalTok{(cars\_renamed)}
\CommentTok{\#\textgreater{}  [1] "miles\_per\_gallon" "cylinders"        "displacement"    }
\CommentTok{\#\textgreater{}  [4] "horse\_power"      "rear\_axel\_ratio"  "wt"              }
\CommentTok{\#\textgreater{}  [7] "qsec"             "vs"               "am"              }
\CommentTok{\#\textgreater{} [10] "gear"             "carb"}
\end{Highlighting}
\end{Shaded}

When using the \texttt{\{dplyr\}} function \texttt{rename()}, you have
to remember the format \texttt{new\_name\ =\ old\_name}. This matches
the format used to create a data frame or tibble, but is the opposite
order to the python function of the same name and often catches people
out.

In the section on \hyperref[creating-new-variables]{creating new
variables}, we will see an alternative way of doing this by copying the
column and then deleting the original.

\section{Subsetting}\label{subsetting-1}

\subsection{Base R}\label{base-r-1}

In base R you can extract rows, columns and combinations thereof using
index notation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First row}
\NormalTok{penguins[}\DecValTok{1}\NormalTok{, ]}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}   species island   bill\_length\_mm bill\_depth\_mm flipper\_length\_mm body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}             \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}             \textless{}int\textgreater{}       \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgers\textasciitilde{}           39.1          18.7               181        3750}
\CommentTok{\#\textgreater{} \# i 2 more variables: sex \textless{}fct\textgreater{}, year \textless{}int\textgreater{}}

\CommentTok{\# First Column }
\NormalTok{penguins[ , }\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 1}
\CommentTok{\#\textgreater{}   species}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Adelie }
\CommentTok{\#\textgreater{} 2 Adelie }
\CommentTok{\#\textgreater{} 3 Adelie }
\CommentTok{\#\textgreater{} 4 Adelie }
\CommentTok{\#\textgreater{} 5 Adelie }
\CommentTok{\#\textgreater{} 6 Adelie }
\CommentTok{\#\textgreater{} \# i 338 more rows}

\CommentTok{\# Rows 2{-}3 of columns 1, 2 and 4}
\NormalTok{penguins[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)]}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}   species island    bill\_depth\_mm}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}             \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen          17.4}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen          18}
\end{Highlighting}
\end{Shaded}

Using negative indexing you can remove rows or columns

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop all but first row}
\NormalTok{penguins[}\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{344}\NormalTok{), ]}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}   species island   bill\_length\_mm bill\_depth\_mm flipper\_length\_mm body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}             \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}             \textless{}int\textgreater{}       \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgers\textasciitilde{}           39.1          18.7               181        3750}
\CommentTok{\#\textgreater{} \# i 2 more variables: sex \textless{}fct\textgreater{}, year \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop all but first column }
\NormalTok{penguins[ , }\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{8}\NormalTok{)]}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 1}
\CommentTok{\#\textgreater{}   species}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Adelie }
\CommentTok{\#\textgreater{} 2 Adelie }
\CommentTok{\#\textgreater{} 3 Adelie }
\CommentTok{\#\textgreater{} 4 Adelie }
\CommentTok{\#\textgreater{} 5 Adelie }
\CommentTok{\#\textgreater{} 6 Adelie }
\CommentTok{\#\textgreater{} \# i 338 more rows}
\end{Highlighting}
\end{Shaded}

You can also select rows or columns by their names. This can be done
using the bracket syntax (\texttt{{[}\ {]}}) or the dollar syntax
(\texttt{\$}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins[ ,}\StringTok{"species"}\NormalTok{]}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 1}
\CommentTok{\#\textgreater{}   species}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Adelie }
\CommentTok{\#\textgreater{} 2 Adelie }
\CommentTok{\#\textgreater{} 3 Adelie }
\CommentTok{\#\textgreater{} 4 Adelie }
\CommentTok{\#\textgreater{} 5 Adelie }
\CommentTok{\#\textgreater{} 6 Adelie }
\CommentTok{\#\textgreater{} \# i 338 more rows}
\NormalTok{penguins}\SpecialCharTok{$}\NormalTok{species}
\CommentTok{\#\textgreater{}   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [344] Chinstrap}
\CommentTok{\#\textgreater{} Levels: Adelie Chinstrap Gentoo}
\end{Highlighting}
\end{Shaded}

Since \texttt{penguins} is a tibble, these return different types of
object. Subsetting a tibble with bracket syntax will return a tibble but
extracting a column using the dollar syntax returns a vector of values.

\subsection{\texorpdfstring{\texttt{filter()} and
\texttt{select()}}{filter() and select()}}\label{filter-and-select}

\texttt{\{dplyr\}} has two functions for subsetting, \texttt{filter()}
subsets by rows and \texttt{select()} subsets by column.

In both functions you list what you would like to retain. Filter and
select calls can be piped together to subset based on row and column
values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g)}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 3}
\CommentTok{\#\textgreater{}   species island    body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}           \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen        3750}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen        3800}
\CommentTok{\#\textgreater{} 3 Adelie  Torgersen        3250}
\CommentTok{\#\textgreater{} 4 Adelie  Torgersen          NA}
\CommentTok{\#\textgreater{} 5 Adelie  Torgersen        3450}
\CommentTok{\#\textgreater{} 6 Adelie  Torgersen        3650}
\CommentTok{\#\textgreater{} \# i 338 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textgreater{}} \DecValTok{6000}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}   species island body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}        \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Gentoo  Biscoe        6300}
\CommentTok{\#\textgreater{} 2 Gentoo  Biscoe        6050}
\end{Highlighting}
\end{Shaded}

Subsetting rows can be inverted by negating the \texttt{filter()}
statement

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textgreater{}} \DecValTok{6000}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 340 x 3}
\CommentTok{\#\textgreater{}   species island    body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}           \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen        3750}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen        3800}
\CommentTok{\#\textgreater{} 3 Adelie  Torgersen        3250}
\CommentTok{\#\textgreater{} 4 Adelie  Torgersen        3450}
\CommentTok{\#\textgreater{} 5 Adelie  Torgersen        3650}
\CommentTok{\#\textgreater{} 6 Adelie  Torgersen        3625}
\CommentTok{\#\textgreater{} \# i 334 more rows}
\end{Highlighting}
\end{Shaded}

and dropping columns can done by selecting all columns except the one(s)
you want to drop.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textgreater{}} \DecValTok{6000}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{c}\NormalTok{(species, island))}
\CommentTok{\#\textgreater{} \# A tibble: 340 x 1}
\CommentTok{\#\textgreater{}   body\_mass\_g}
\CommentTok{\#\textgreater{}         \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1        3750}
\CommentTok{\#\textgreater{} 2        3800}
\CommentTok{\#\textgreater{} 3        3250}
\CommentTok{\#\textgreater{} 4        3450}
\CommentTok{\#\textgreater{} 5        3650}
\CommentTok{\#\textgreater{} 6        3625}
\CommentTok{\#\textgreater{} \# i 334 more rows}
\end{Highlighting}
\end{Shaded}

\section{Creating New Variables}\label{creating-new-variables}

\subsection{Base R}\label{base-r-2}

We can create new variables in base R by assigning a vector of the
correct length to a new column name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed}\SpecialCharTok{$}\NormalTok{weight }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed}\SpecialCharTok{$}\NormalTok{wt}
\end{Highlighting}
\end{Shaded}

If we then drop the original column from the data frame, this gives us
an alternative way of renaming columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed[ ,}\SpecialCharTok{{-}}\FunctionTok{which}\NormalTok{(}\FunctionTok{names}\NormalTok{(cars\_renamed) }\SpecialCharTok{==} \StringTok{"wt"}\NormalTok{)]}
\FunctionTok{head}\NormalTok{(cars\_renamed, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}                   miles\_per\_gallon cylinders displacement horse\_power}
\CommentTok{\#\textgreater{} Mazda RX4                     21.0         6          160         110}
\CommentTok{\#\textgreater{} Mazda RX4 Wag                 21.0         6          160         110}
\CommentTok{\#\textgreater{} Datsun 710                    22.8         4          108          93}
\CommentTok{\#\textgreater{} Hornet 4 Drive                21.4         6          258         110}
\CommentTok{\#\textgreater{} Hornet Sportabout             18.7         8          360         175}
\CommentTok{\#\textgreater{}                   rear\_axel\_ratio  qsec vs am gear carb weight}
\CommentTok{\#\textgreater{} Mazda RX4                    3.90 16.46  0  1    4    4  2.620}
\CommentTok{\#\textgreater{} Mazda RX4 Wag                3.90 17.02  0  1    4    4  2.875}
\CommentTok{\#\textgreater{} Datsun 710                   3.85 18.61  1  1    4    1  2.320}
\CommentTok{\#\textgreater{} Hornet 4 Drive               3.08 19.44  1  0    3    1  3.215}
\CommentTok{\#\textgreater{} Hornet Sportabout            3.15 17.02  0  0    3    2  3.440}
\end{Highlighting}
\end{Shaded}

One thing to be aware of is that this operation does not preserve column
ordering.

Generally speaking, code that relies on columns being in a specific
order is fragile - it breaks easily. If possible, you should try to
write your code in another way that's robust to column reordering. I've
done that here when removing the \texttt{wt} column by looking up the
column index as part of my code, rather than assuming it will always be
the fourth column.

\subsection{\texorpdfstring{\texttt{dplyr::mutate()}}{dplyr::mutate()}}\label{dplyrmutate}

The function from \texttt{\{dplyr\}} to create new columns is
\texttt{mutate()}. Let's create another column that has the car's weight
in kilogrammes rather than tonnes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{weight\_kg =}\NormalTok{ weight }\SpecialCharTok{*} \DecValTok{1000}\NormalTok{)}

\NormalTok{cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(miles\_per\_gallon, cylinders, displacement, weight, weight\_kg) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}                   miles\_per\_gallon cylinders displacement weight weight\_kg}
\CommentTok{\#\textgreater{} Mazda RX4                     21.0         6          160  2.620      2620}
\CommentTok{\#\textgreater{} Mazda RX4 Wag                 21.0         6          160  2.875      2875}
\CommentTok{\#\textgreater{} Datsun 710                    22.8         4          108  2.320      2320}
\CommentTok{\#\textgreater{} Hornet 4 Drive                21.4         6          258  3.215      3215}
\CommentTok{\#\textgreater{} Hornet Sportabout             18.7         8          360  3.440      3440}
\end{Highlighting}
\end{Shaded}

You can also create new columns that are functions of multiple other
columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cylinder\_adjusted\_mpg =}\NormalTok{ miles\_per\_gallon }\SpecialCharTok{/}\NormalTok{ cylinders)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{rownames\_to\_column()}}{rownames\_to\_column()}}\label{rownames_to_column}

One useful example of adding an additional row to a data frame is to
convert its row names to a column of the data fame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =} \FunctionTok{rownames}\NormalTok{(cars\_renamed)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(mpg, cyl, model) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}                    mpg cyl             model}
\CommentTok{\#\textgreater{} Mazda RX4         21.0   6         Mazda RX4}
\CommentTok{\#\textgreater{} Mazda RX4 Wag     21.0   6     Mazda RX4 Wag}
\CommentTok{\#\textgreater{} Datsun 710        22.8   4        Datsun 710}
\CommentTok{\#\textgreater{} Hornet 4 Drive    21.4   6    Hornet 4 Drive}
\CommentTok{\#\textgreater{} Hornet Sportabout 18.7   8 Hornet Sportabout}
\end{Highlighting}
\end{Shaded}

There's a neat function called \texttt{rownames\_to\_column()} in the
\texttt{\{tibble\}} package. This will add the row names as the first
column and remove the row names all in one step.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tibble}\SpecialCharTok{::}\FunctionTok{rownames\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"model"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}               model  mpg cyl disp  hp drat    wt  qsec vs am gear carb}
\CommentTok{\#\textgreater{} 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4}
\CommentTok{\#\textgreater{} 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4}
\CommentTok{\#\textgreater{} 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1}
\CommentTok{\#\textgreater{} 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1}
\CommentTok{\#\textgreater{} 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{rowids\_to\_column()}}{rowids\_to\_column()}}\label{rowids_to_column}

Another function from \texttt{\{tibble\}} adds the row id of each
observation as a new column. This is often useful when ordering or
combining tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tibble}\SpecialCharTok{::}\FunctionTok{rowid\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"row\_id"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}   row\_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb}
\CommentTok{\#\textgreater{} 1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4}
\CommentTok{\#\textgreater{} 2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4}
\CommentTok{\#\textgreater{} 3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1}
\CommentTok{\#\textgreater{} 4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1}
\CommentTok{\#\textgreater{} 5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2}
\end{Highlighting}
\end{Shaded}

\section{Summaries}\label{summaries}

The \texttt{summarise()} function allows you to collapse a data frame
into a single row, which using a summary statistic of your choosing.

We can calculate the average bill length of all penguins in a single
\texttt{summarise()} function call.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarise}\NormalTok{(penguins, }\AttributeTok{average\_bill\_length\_mm =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   average\_bill\_length\_mm}
\CommentTok{\#\textgreater{}                    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1                     NA}
\end{Highlighting}
\end{Shaded}

Since we have missing values, we might instead want to calculate the
mean of the recorded values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarise}\NormalTok{(penguins, }\AttributeTok{average\_bill\_length\_mm =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   average\_bill\_length\_mm}
\CommentTok{\#\textgreater{}                    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1                   43.9}
\end{Highlighting}
\end{Shaded}

We can also use \texttt{summarise()} to gather multiple summaries in a
single data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bill\_length\_mm\_summary }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{min =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_0 =} \FunctionTok{min}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_1 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_2 =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_3 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_4 =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}

\NormalTok{bill\_length\_mm\_summary}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}    mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1  43.9   44.4  59.6  32.1  39.2  44.4  48.5  59.6}
\end{Highlighting}
\end{Shaded}

In all, this isn't overly exciting. You might rightly wonder why you'd
want to use \texttt{summarise()} when we could just use the simpler base
R calls directly.

One benefit of \texttt{summarise()} is that it provides certainty that
the obejct returned will be of a certain class (a tibble) no matter what
summary function is used. However, \texttt{summarise()} really comes
into its own when you want to apply these summaries to distinct
subgroups of the data.

\section{Grouped Operations}\label{grouped-operations}

The real benefit of \texttt{summarise()} comes from its combination with
\texttt{group\_by()}. This allows to you calculate the same summary
statistics for each level of a factor with only one additional line of
code.

Here we're re-calculating the same set of summary statistics we just
found for all penguins, but for each individual species.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{min =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_0 =} \FunctionTok{min}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_1 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_2 =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_3 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_4 =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 9}
\CommentTok{\#\textgreater{}   species    mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  }
\CommentTok{\#\textgreater{} 2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

You can group by multiple factors to calculate summaries for each
distinct combination of levels within your data set. Here we group by
combinations of species and the island to which they belong.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguin\_summary\_stats }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(species, island) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{min =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_0 =} \FunctionTok{min}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_1 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_2 =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_3 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_4 =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}species\textquotesingle{}. You can override using the}
\CommentTok{\#\textgreater{} \textasciigrave{}.groups\textasciigrave{} argument.}

\NormalTok{penguin\_summary\_stats}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 10}
\CommentTok{\#\textgreater{} \# Groups:   species [3]}
\CommentTok{\#\textgreater{}   species   island     mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6}
\CommentTok{\#\textgreater{} 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1}
\CommentTok{\#\textgreater{} 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  }
\CommentTok{\#\textgreater{} 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

\subsection{Ungrouping}\label{ungrouping}

By default, each call to \texttt{summarise()} will undo one level of
grouping. This means that our previous result was still grouped by
species. We can see this in the tibble output above or by examining the
structure of the returned data frame. This tells us that this is an S3
object of class \texttt{grouped\_df}, which inherits its properties from
a \texttt{tbl\_df}, \texttt{tbl}, and \texttt{data.frame} objects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(penguin\_summary\_stats)}
\CommentTok{\#\textgreater{} [1] "grouped\_df" "tbl\_df"     "tbl"        "data.frame"}
\end{Highlighting}
\end{Shaded}

Since we have grouped by two variables, \texttt{\{dplyr\}} expects us to
use two \texttt{summary()} function calls before it will return a data
frame (or tibble) that is not grouped. One way to satisfy this is to
apply a second summary at the species level of grouping.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguin\_summary\_stats }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise\_all}\NormalTok{(mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 10}
\CommentTok{\#\textgreater{}   species   island  mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}      \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2}
\CommentTok{\#\textgreater{} 2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

However, we won't always want to do apply another summary. In that case,
we can undo the grouping using \texttt{ungroup()}. Remembering to
ungroup is a common mistake and cause of confusion when working with
multiple-group summaries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ungroup}\NormalTok{(penguin\_summary\_stats)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 10}
\CommentTok{\#\textgreater{}   species   island     mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6}
\CommentTok{\#\textgreater{} 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1}
\CommentTok{\#\textgreater{} 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  }
\CommentTok{\#\textgreater{} 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

There's an alternative method to achieve the same thing in a single step
when using \texttt{\{dplyr\}} versions 1.0.0 and above. This is to to
set the \texttt{.by} parameter of the \texttt{summarise()} function
call, which determines the grouping that should be applied to the
original data frame, just for this one operation.

\begin{verbatim}
#> # A tibble: 3 x 2
#>   island    mean_bill_length_mm
#>   <fct>                   <dbl>
#> 1 Torgersen                39.0
#> 2 Biscoe                   45.3
#> 3 Dream                    44.2
\end{verbatim}

The \texttt{.by} argument applies to a single operation. This means that
the result of the \texttt{summarise()} call will always be an ungrouped
tibble, regardless of the number of grouping columns.

\begin{verbatim}
#> # A tibble: 5 x 3
#>   island    species   mean_bill_length_mm
#>   <fct>     <fct>                   <dbl>
#> 1 Torgersen Adelie                   39.0
#> 2 Biscoe    Adelie                   39.0
#> 3 Dream     Adelie                   38.5
#> 4 Biscoe    Gentoo                   47.5
#> 5 Dream     Chinstrap                48.8
\end{verbatim}

See \texttt{?dplyr\_by} for more information on using the \texttt{.by}
argument with \texttt{summarise()} and other \texttt{\{dplyr\}} verbs.

\section{Reordering Factors}\label{reordering-factors}

R stored factors as integer values, which it then maps to a set of
labels. Only factor levels that appear in your data will be assigned a
coded integer value and the mapping between factor levels and integers
will depend on the order that the labels appear in your data.

This can be annoying, particularly when your factor levels relate to
properties that aren't numerical but do have an inherent ordering to
them. In the example below, we have the t-shirt size of twelve people.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tshirts }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }
  \AttributeTok{size =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"L"}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"S"}\NormalTok{, }\StringTok{"XS"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"XXL"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"XS"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"S"}\NormalTok{))}
\NormalTok{)}

\FunctionTok{levels}\NormalTok{(tshirts}\SpecialCharTok{$}\NormalTok{size)}
\CommentTok{\#\textgreater{} [1] "L"   "M"   "S"   "XS"  "XXL"}
\end{Highlighting}
\end{Shaded}

Irritatingly, the sizes aren't in order and extra large isn't included
because it's not included in this particular sample. This leads to
awkward looking summary tables and plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tshirts }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(size) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   size  count}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 L         3}
\CommentTok{\#\textgreater{} 2 M         3}
\CommentTok{\#\textgreater{} 3 S         2}
\CommentTok{\#\textgreater{} 4 XS        2}
\CommentTok{\#\textgreater{} 5 XXL       1}
\CommentTok{\#\textgreater{} 6 \textless{}NA\textgreater{}      1}
\end{Highlighting}
\end{Shaded}

We can fix this by creating a new variable with the factors explicitly
coded in the correct order. We also need to specify that we should not
drop empty groups as part of \texttt{group\_by()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tshirt\_levels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"XS"}\NormalTok{, }\StringTok{"S"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"XL"}\NormalTok{, }\StringTok{"XXL"}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}

\NormalTok{tshirts }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{size\_tidy =} \FunctionTok{factor}\NormalTok{(size, }\AttributeTok{levels =}\NormalTok{ tidy\_tshirt\_levels)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(size\_tidy, }\AttributeTok{.drop =} \ConstantTok{FALSE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 2}
\CommentTok{\#\textgreater{}   size\_tidy count}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 XS            2}
\CommentTok{\#\textgreater{} 2 S             2}
\CommentTok{\#\textgreater{} 3 M             3}
\CommentTok{\#\textgreater{} 4 L             3}
\CommentTok{\#\textgreater{} 5 XL            0}
\CommentTok{\#\textgreater{} 6 XXL           1}
\CommentTok{\#\textgreater{} \# i 1 more row}
\end{Highlighting}
\end{Shaded}

\section{Be Aware: Factors}\label{be-aware-factors}

As we have seen a little already, categorical variables can cause issues
when wrangling and presenting data in R. All of these problems are
solvable using base R techniques but the \texttt{\{forcats\}} package
provides tools for the most common of these problems. This includes
functions for changing the order of factor levels or the values with
which they are associated.

Some examples functions from the package include:

\begin{itemize}
\tightlist
\item
  \texttt{fct\_reorder()}: Reordering a factor by another variable.
\item
  \texttt{fct\_infreq()}: Reordering a factor by the frequency of
  values.
\item
  \texttt{fct\_relevel()}: Changing the order of a factor by hand.
\item
  \texttt{fct\_lump()}: Collapsing the least/most frequent values of a
  factor into ``other''.
\end{itemize}

Examples of each of these can be found in the
\href{https://forcats.tidyverse.org/articles/forcats.html}{forcats
vignette} or the \href{(https://r4ds.had.co.nz/factors.html)}{factors
chapter} of R for data science.

\section{Be Aware: Strings}\label{be-aware-strings}

Working with and analysing text data is a skill unto itself. However, it
is useful to be able to do some basic manipulation of character strings
programatically.

Because R was developed as a statistical programming language, it is
well suited to the computational and modelling aspects of working with
text data but the base R string manipulation functions can be a bit
unwieldy at times.

The \texttt{\{stringr\}} package aims to combat this by providing useful
helper functions for a range of text management problems. Even when not
analysing text data these can be useful, such as when removing prefixes
from many column names.

Suppose we wanted to keep only the text following an underscore in these
column names. We could do that by using a regular expression to extract
lower-case or upper-case letters which follow an underscore.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(poorly\_named\_df)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 11}
\CommentTok{\#\textgreater{}   observation\_id   V1\_A   V2\_B     V3\_C   V4\_D   V5\_E   V6\_F   V7\_G    V8\_H}
\CommentTok{\#\textgreater{}            \textless{}int\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1              1  0.215 {-}2.39   1.73    {-}0.611 {-}1.12   0.797 {-}1.03  {-}0.856 }
\CommentTok{\#\textgreater{} 2              2 {-}0.364  1.26   0.981    0.563  0.970 {-}0.209 {-}0.182  2.32  }
\CommentTok{\#\textgreater{} 3              3 {-}0.418  0.474 {-}2.36     1.03   0.196 {-}1.05   1.04  {-}0.0272}
\CommentTok{\#\textgreater{} 4              4 {-}0.949 {-}1.83  {-}0.00179  0.874  0.613 {-}0.846 {-}0.375 {-}2.36  }
\CommentTok{\#\textgreater{} 5              5  0.515 {-}1.07   0.116   {-}0.390 {-}0.457 {-}0.914  0.212 {-}0.736 }
\CommentTok{\#\textgreater{} 6              6 {-}0.574  0.461  0.0208  {-}0.599  0.752 {-}1.59   0.524  2.20  }
\CommentTok{\#\textgreater{} \# i 2 more variables: V9\_I \textless{}dbl\textgreater{}, V10\_J \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stringr}\SpecialCharTok{::}\FunctionTok{str\_extract}\NormalTok{(}\FunctionTok{names}\NormalTok{(poorly\_named\_df), }\AttributeTok{pattern =} \StringTok{"(?\textless{}=\_)([a{-}zA{-}Z]+)"}\NormalTok{)}
\CommentTok{\#\textgreater{}  [1] "id" "A"  "B"  "C"  "D"  "E"  "F"  "G"  "H"  "I"  "J"}
\end{Highlighting}
\end{Shaded}

Alternatively, can avoid using
\href{https://en.wikipedia.org/wiki/Regular_expression}{regular
expressions}. We can use \texttt{stringr::str\_split()} to divide each
column name at the underscore and keep only the second part of each
string.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# split column names at underscores and inspect structure of resuting object}
\NormalTok{split\_strings }\OtherTok{\textless{}{-}}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_split}\NormalTok{(}\FunctionTok{names}\NormalTok{(poorly\_named\_df), }\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(split\_strings)}
\CommentTok{\#\textgreater{} List of 11}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "observation" "id"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V1" "A"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V2" "B"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V3" "C"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V4" "D"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V5" "E"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V6" "F"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V7" "G"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V8" "H"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V9" "I"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V10" "J"}

\CommentTok{\# keep only the second element of each character vector in the list}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{map\_chr}\NormalTok{(split\_strings, }\ControlFlowTok{function}\NormalTok{(x)\{x[}\DecValTok{2}\NormalTok{]\})}
\CommentTok{\#\textgreater{}  [1] "id" "A"  "B"  "C"  "D"  "E"  "F"  "G"  "H"  "I"  "J"}
\end{Highlighting}
\end{Shaded}

Unless you plan to work extensively with text data, I would recommend
that you look up such string manipulations as you need them. The
\href{https://r4ds.had.co.nz/strings.html\#strings}{strings} section of
R for Data Science is a useful starting point.

\section{Be Aware: Date-Times}\label{be-aware-date-times}

Remember \hyperref[naming-things-date-order]{all the fuss} we made about
storing dates in the ISO standard format? That was because dates and
times are complicated enough to work on without adding extra ambiguity.

\[ \text{YYYY} - \text{MM} - \text{DD}\] Dates, times and time intervals
have to reconcile two factors: the physical orbit of the Earth around
the Sun and the social and geopolitical mechanisms that determine how we
measure and record the passing of time. This makes the history of date
and time records fascinating but also make working with this type of
data complicated.

Moving from larger to smaller time spans: leap years alter the number of
days in a year, months are of variable length (with February's length
changing from year to year). If your data are measured in a place that
uses daylight saving, then one day a year will be 23 hours long and
another will be 25 hours long. To make things worse, the dates and the
hour at which the clocks change are not uniform across countries, which
might span multiple time zones and those time-zone boundaries can shift
over time.

Even at the level of minutes and seconds we aren't safe - since the
Earth's orbit is gradually slowing down and a leap second is added
approximately every 21 months. Nor are things any better when looking at
longer time scales or across cultures, where we might have to account
for different calendars: months are added removed and altered over time,
other calendar systems still take different approaches to measuring time
and using different units and origin points.

With all of these issues you have to be very careful when working with
date and time data. Functions to help you with this can be found in the
\texttt{\{lubridate\}} package, with examples in the
\href{https://r4ds.had.co.nz/dates-and-times.html\#dates-and-times}{dates
and times} chapter of R for data science.

\section{Be Aware: Relational Data}\label{be-aware-relational-data}

When the data you need are stored across two or more data frames you
need to be able to cross-reference those and match up values for
observational unit. This sort of data is know as relational data, and is
used extensively in data science.

The variables you use to match observational units across data frames
are known as \emph{keys}. The primary key belongs to the first table and
the foreign key belongs to the secondary table. There are various ways
to join these data frames, depending on if you want to retain.

\subsubsection{Join types}\label{join-types}

You might want to keep only observational units that have key variables
values in both data frames, this is known as an inner join.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{images/301-edav-wrangling/join-inner.png}

}

\caption{Inner join diagram. Source: R for Data Science}

\end{figure}%

You might instead want to keep all units from the primary table but pad
with NAs where there is not a corresponding foreign key in the second
table. This results in an \textbf{(outer) left-join}.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{images/301-edav-wrangling/join-left-right-full.png}

}

\caption{Diagram for left, right and outer joins. Source: R for Data
Science}

\end{figure}%

Conversely, you might keep all units from the second table but pad with
NAs where there is not a corresponding foreign key in the primary table.
This is imaginatively named an \textbf{(outer) right-join}.

In the \textbf{(outer) full join}, all observational units from either
table are retained and all missing values are padded with NAs.

Things get more complicated when keys don't uniquely identify
observational units in either one or both of the tables. I'd recommend
you start exploring these ideas with the
\href{https://r4ds.had.co.nz/relational-data.html}{relational data}
chapter of R for Data Science.

\subsubsection{Why and where to learn
more}\label{why-and-where-to-learn-more}

Working with relational data is essential to getting any data science up
and running out in the wilds of reality. This is because businesses and
companies don't store all of their data in a huge single csv file. For
one this isn't very efficient, because most cells would be empty.
Secondly, it's not a very secure approach, since you can't grant partial
access to the data. That's why information is usually stored in many
data frames (more generically known as tables) within one or more
databases.

These data silos are created, maintained, accessed and destroyed using a
relational data base management system. These management systems use
code to manage and access the stored data, just like we have seen in the
dplyr commands above. You might well have heard of the SQL programming
language (and its many variants), which is a popular language for data
base management and is the inspiration for the dplyr package and verbs.

If you'd like to learn more then there are many excellent introductory
SQL books and courses, I'd recommend picking one that focuses on data
analysis or data science unless you really want to dig into efficient
storage and querying of databases.

\section{Wrapping up}\label{wrapping-up-3}

We have:

\begin{itemize}
\item
  Learned how to wrangle tabular data in R with \texttt{\{dplyr\}}
\item
  Met the idea of relational data and \texttt{\{dplyr\}}'s relationship
  to SQL
\item
  Become aware of some tricky data types and packages that can help.
\end{itemize}

\section{Session Information}\label{session-information-6}

\textbf{R version 4.4.2 (2024-10-31)}

\textbf{Platform:} x86\_64-apple-darwin20

\textbf{locale:}
en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8\textbar\textbar C\textbar\textbar en\_US.UTF-8\textbar\textbar en\_US.UTF-8

\textbf{attached base packages:} \emph{stats}, \emph{graphics},
\emph{grDevices}, \emph{utils}, \emph{datasets}, \emph{methods} and
\emph{base}

\textbf{other attached packages:} \emph{dplyr(v.1.1.4)} and
\emph{palmerpenguins(v.0.1.1)}

\textbf{loaded via a namespace (and not attached):}
\emph{vctrs(v.0.6.5)}, \emph{cli(v.3.6.3)}, \emph{knitr(v.1.49)},
\emph{rlang(v.1.1.4)}, \emph{xfun(v.0.50)}, \emph{stringi(v.1.8.4)},
\emph{purrr(v.1.0.2)}, \emph{generics(v.0.1.3)},
\emph{jsonlite(v.1.8.9)}, \emph{glue(v.1.8.0)},
\emph{htmltools(v.0.5.8.1)}, \emph{rmarkdown(v.2.29)},
\emph{pander(v.0.6.5)}, \emph{evaluate(v.1.0.3)},
\emph{tibble(v.3.2.1)}, \emph{fastmap(v.1.2.0)}, \emph{yaml(v.2.3.10)},
\emph{lifecycle(v.1.0.4)}, \emph{stringr(v.1.5.1)},
\emph{compiler(v.4.4.2)}, \emph{Rcpp(v.1.0.14)},
\emph{pkgconfig(v.2.0.3)}, \emph{rstudioapi(v.0.17.1)},
\emph{digest(v.0.6.37)}, \emph{R6(v.2.5.1)}, \emph{tidyselect(v.1.2.1)},
\emph{utf8(v.1.2.4)}, \emph{pillar(v.1.10.1)}, \emph{magrittr(v.2.0.3)},
\emph{tools(v.4.4.2)} and \emph{withr(v.3.0.2)}

\chapter{Exploratory Data Analysis}\label{edav-analysis}

\section{Introduction}\label{introduction-3}

Exploratory data analysis is an essential stage in any data science
project. It allows you to become familiar with the data you are working
with while also to identify potential strategies for progressing the
project and flagging any areas of concern.

In this chapter we will look at three different perspectives on
exploratory data analysis: its purpose for you as a data scientist, its
purpose for the broader team working on the project and finally its
purpose for the project itself.

\section{Get to know your data}\label{get-to-know-your-data}

Let's first focus on an exploratory data analysis from our own point of
view, as data scientists.

Exploratory data analysis (or EDA) is a process of examining a data set
to understand its overall structure, contents, and the relationships
between the variables it contains. EDA is an iterative process that's
often done before building a model or making other data-driven decisions
within a data science project.

\begin{quote}
Exploratory Data Analysis: quick and simple exerpts, summaries and plots
to better understand a data set.
\end{quote}

One key aspect of EDA is generating quick and simple summaries and plots
of the data. These plots and summary statistics can help to quickly
understand the distribution of and relationships between the recorded
variables. Additionally, during an exploratory analysis you will
familiarise yourself with the structure of the data you're working with
and how that data was collected.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)                     }
\FunctionTok{library}\NormalTok{(}\StringTok{"GGally"}\NormalTok{)}

\NormalTok{colours }\OtherTok{\textless{}{-}}\NormalTok{ colorspace}\SpecialCharTok{::}\FunctionTok{adjust\_transparency}\NormalTok{(}
  \AttributeTok{col =}\NormalTok{ PrettyCols}\SpecialCharTok{::}\FunctionTok{prettycols}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Fun"}\NormalTok{, }\AttributeTok{n =} \DecValTok{3}\NormalTok{),}
  \AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{)}

\FunctionTok{ggpairs}\NormalTok{(iris,}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ Species)) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ colours) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ colours) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{302-edav-analysis_files/figure-pdf/iris-plot-1.pdf}

}

\caption{Investigating marginal and pairwise relationships in the Iris
dataset.}

\end{figure}%

Since EDA is an initial and iterative process, it's rare that any
component of the analysis will be put into production. Instead, the goal
is to get a general understanding of the data that can inform the next
steps of the analysis.

In terms of workflow, this means that using one or more notebooks is
often an effective way of organising your work during an exploratory
analysis. This allows for rapid iteration and experimentation, while
also providing a level of reproducibility and documentation. Since
notebooks allow you to combine code, plots, tables and text in a single
document, this makes it easy to share your initial findings with
stakeholders and project managers.

\section{Start a conversation}\label{start-a-conversation}

\begin{quote}
An effective EDA sets a precedent for open communication with the
stakeholder and project manager.
\end{quote}

We've seen the benefits of an EDA for you as a data scientist, but this
isn't the only perspective.

One key benefit of an EDA is that it can kick-start your communication
with subject matter experts and project managers. You can build rapport
and trust early in a project's life cycle by sharing your preliminary
findings with these stakeholders . This can lead to a deeper
understanding of both the available data and the problem being addressed
for everyone involved. If done well, it also starts to build trust in
your work before you even begin the modelling stage of a project.

\subsection{Communicating with
specialists}\label{communicating-with-specialists}

Sharing an exploratory analysis will inevitably require a time
investment. The graphics, tables, and summaries you produce need to be
presented to a higher standard and explained in a way that is clear to a
non-specialist. However, this time investment will often pay dividends
because of the additional contextual knowledge that the domain-expert
can provide. They have a deep understanding of the business or technical
domain surrounding the problem. This can provide important insights that
aren't in the data itself, but which are vital to the project's success.

As an example, these stakeholder conversations often reveal important
features in the data generating or measurement process that should be
accounted for when modelling. These details are usually left out of the
data documentation because they would be immediately obvious to any
specialist in that field.

\subsection{Communicating with project
manager}\label{communicating-with-project-manager}

An EDA can sometimes allow us to identify cases where the strength of
signal within the available data is clearly insufficient to answer the
question of interest. By clearly communicating this to the project
manager, the project can be postponed while different, better quality or
simply more data are collected. It's important to note that this data
collection is not trivial and can have a high cost in terms of both time
and capital. It might be that collecting the data needed to answer a
question will cost more than we're likely to gain from knowing that
answer. Whether the project is postponed or cancelled, this constitutes
a successful outcome for the project; the aim is to to produce insight
or profit - not to fit models for their own sake.

\section{Scope Your Project}\label{scope-your-project}

\begin{quote}
EDA is an initial assessment of whether the available data measure the
correct values, in sufficient quality and quantity, to answer a
particular question.
\end{quote}

A third view on EDA is as an initial assessment of whether the available
data measure the correct values, with sufficient quality and quantity,
to answer a particular question. In order for EDA to be successful, it's
important to take a few key steps.

First, it's important to formulate a specific question of interest or
line of investigation and agree on it with the stakeholder. By having a
clear question in mind, it will be easier to focus the analysis and
identify whether the data at hand can answer it.

Next, it's important to make a record (if one doesn't already exist) of
how the data were collected, by whom it was collected, what each
recorded variable represents and the units in which they are recorded.
This meta-data is often known as a data sheet or data dictionary. Having
this information in written form is crucial when adding a new
collaborator to a project, so that they can understand the data
generating and measurement processes, and are aware of the quality and
accuracy of the recorded values.The
\href{https://help.osf.io/article/217-how-to-make-a-data-dictionary}{Open
Science Foundation} give a concise description of the meta-data you
might list in a data dictionary.

\section{Investigate Your Data}\label{investigate-your-data}

\begin{quote}
EDA is an opportunitiy to quantify data completeness and investigate the
possibility of informative missingness.
\end{quote}

In addition, it's essential to investigate and document the structure,
precision, completeness and quantity of data available. This includes
assessing the degree of measurement noise or misclassification in the
data, looking for clear linear or non-linear dependencies between any of
the variables, and identifying if any data are missing or if there's any
structure to this missingness. Other data features to be aware of are
the presence of any censoring or whether some values tend to be missing
together.

Furthermore, a more advanced EDA might include a simulation study to
estimate the amount of data needed to detect the smallest meaningful
effect. This is more in-depth than a typical EDA but if you suspect that
the signals within your data are weak relative to measurement noise, can
help to demonstrate the limitations of the current line of enquiry with
the information that is currently available.

\section{What is not EDA?}\label{what-is-not-eda}

It's important to understand that an exploratory data analysis is not
the same thing as modelling. In particular is \emph{not} the
construction of your baseline model, which is sometimes called initial
data analysis. Though it might inform the choice of baseline model, EDA
is usually not model based. In an EDA, simple plots and summaries are
used to identify patterns in the data that inform how you approach the
rest of the project.

Some degree of statistical rigour can be added to EDA through the use of
non-parametric techniques; methods like rolling averages, smoothing or
partitioning can to help identify trends or patterns while making
minimal assumptions about the data generating process.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ismev)}
\FunctionTok{library}\NormalTok{(cowplot)}

\FunctionTok{data}\NormalTok{(}\StringTok{"dowjones"}\NormalTok{, }\AttributeTok{package =} \StringTok{"ismev"}\NormalTok{) }

\NormalTok{theme\_dow\_jones }\OtherTok{\textless{}{-}} \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{9}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{,  }\AttributeTok{hjust =} \FloatTok{0.98}\NormalTok{, }\AttributeTok{vjust =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{),}
    \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{)}
\NormalTok{  )}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ dowjones }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{index\_change =}\NormalTok{ Index }\SpecialCharTok{{-}} \FunctionTok{lag}\NormalTok{(Index)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(index\_change)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Date, }\AttributeTok{y =}\NormalTok{ index\_change)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{col =} \FunctionTok{rgb}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{0.2}\NormalTok{), }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1998{-}06{-}01"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Daily change in Dow Jones Index"}\NormalTok{, }\StringTok{"with smoothed estimate of mean change"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  theme\_dow\_jones}

\NormalTok{p2 }\OtherTok{\textless{}{-}}\NormalTok{ dowjones }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{index\_change =}\NormalTok{ Index }\SpecialCharTok{{-}} \FunctionTok{lag}\NormalTok{(Index)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(index\_change)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Date, }\AttributeTok{y =}\NormalTok{ index\_change)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{col =} \FunctionTok{rgb}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}
    \AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }
    \AttributeTok{fill =}\NormalTok{ colorspace}\SpecialCharTok{::}\FunctionTok{adjust\_transparency}\NormalTok{(}\StringTok{"darkorange"}\NormalTok{, }\FloatTok{0.05}\NormalTok{),}
    \AttributeTok{lwd =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1998{-}06{-}01"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{  theme\_dow\_jones}

\FunctionTok{plot\_grid}\NormalTok{(p1,p2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{302-edav-analysis_files/figure-pdf/dow-jones-rolling-mean-plot-1.pdf}

}

\caption{Daily change in Dow Jones Index with smoothed estimate of mean
and 95\% confidence interval.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ dowjones }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{index\_change =}\NormalTok{ Index }\SpecialCharTok{{-}} \FunctionTok{lag}\NormalTok{(Index)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{after\_june\_98 =}\NormalTok{ Date }\SpecialCharTok{\textgreater{}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1998{-}06{-}01"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(after\_june\_98) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(index\_change, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(index\_change, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)),}
  \AttributeTok{caption =} \StringTok{"Mean and standard deviation of daily change in Dow Jones Index, before and after 1st of June 1998."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\caption{Mean and standard deviation of daily change in Dow Jones Index,
before and after 1st of June 1998.}\tabularnewline
\toprule\noalign{}
after\_june\_98 & mean & sd \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
after\_june\_98 & mean & sd \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
FALSE & 5.916798 & 65.19093 \\
TRUE & 3.972929 & 119.56067 \\
\end{longtable}

Though the assumptions in an EDA are often minimal it can help to make
them explicit. For example, in this plot a moving averages is shown with
a confidence band, but the construction of this band makes the implicit
assumption that, at least locally, our observations have the same
distribution and so are exchangeable.

Finally, EDA is not a prescriptive process. While I have given a lot of
suggestions on what you might usually want to consider, there is no
correct way to go about an EDA because it is so heavily dependent on the
particular dataset, its interpretation and the task you want to achieve
with it. This is one of the parts of data science that make it a craft
that you hone with experience, rather than an algorithmic process. When
you work in a particular area for a long time you develop
domain-specific knowledge of common data quirks, which may or may not
translate to other applications.

Now that we have a better idea of what is and what is not EDA, let's
talk about the issue that an EDA tries to resolve and the other issues
that it generates.

\section{Issue: Forking Paths}\label{issue-forking-paths}

In any data science project you have to make a sequence of very many
decisions, each with many potential options. It is almost impossible to
anticipate all of the decisions you will have to make ahead of time and
even more difficult do decide how to proceed with each decision \emph{a
priori}.

\includegraphics{images/302-data-exploration/forking-paths.jpg}

Focusing in on only one small part of the data science process, we might
consider picking a null or baseline model that we will then try and
improve on. Should that null model make constant predictions,
incorporate a simple linear trend or is something more flexible
obviously needed? Do you have the option to try all of these or are you
working under time constraints? Is there any domain knowledge that rules
some of these null models out on contextual grounds?

An EDA lets you narrow down your options by looking at your data and
helps you to decide what might be reasonable modelling approaches.

\includegraphics{images/302-data-exploration/supervise_learning_schematic.png}

The problem that sneaks in here is data leakage. Formally this is where
training data is included in test set but this sort of information leak
can happen informally too. Usually this is because you've seen the data
you're trying to model or predict and then selected your modelling
approach based on that information.

Standard, frequentist statistical methods for estimation and testing
assume no ``peeking'' of this type has occurred. If we use these methods
without acknowledging that we have already observed our data then we
will artificially inflate the significance of our findings. For example,
we might be comparing two models: the first of which makes constant
predictions with regard to a predictor, while the second includes a
linear trend. We will of course use a statistical test to confirm that
what we are seeing is unlikely by chance. However, we must be aware this
test was only performed because we had previously examined at the data
and noticed what looked to be a trend.

Similar issues arise in Bayesian approaches, particularly when
constructing or eliciting prior distributions for our model parameters.
One nice thing that we can do in the Bayesian setting is to simulate
data from the prior predictive distribution and then get an expert to
check that these datasets seem seem reasonable. However, it is often the
case this expert is also the person who collected the data we will soon
be modelling. It's very difficult for them to ignore what they have
seen, which leads to similar, subtle leakage problems. (For further
discussion see the relevant sections of
\href{https://doi.org/10.48550/arXiv.2011.01808}{Gelman et al.~(2020)}
or
\href{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html}{Betancourt
(2020)})

\section{Correction Methods}\label{correction-methods}

There are various methods or corrections that we can apply during our
testing and estimation procedures to ensure that our error rates or
confidence intervals account for our previous ``peeking'' during EDA.

Examples of these corrections have been developed across many fields of
statistics. In medical statistics we have approaches like the Bonferroni
correction, to account for carrying out multiple hypothesis tests. In
the change-point literature there are techniques for estimating a change
location given that a change has been detected somewhere in a time
series. While in the extreme value literature there are methods to
estimate the required level of protection against rare events, given
that the analysis was triggered by the current protections having been
compromised.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate data}
\NormalTok{date }\OtherTok{\textless{}{-}} \FunctionTok{seq.Date}\NormalTok{(}
  \AttributeTok{from =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2018{-}03{-}01"}\NormalTok{),}
  \AttributeTok{to =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}05{-}03"}\NormalTok{),}
  \AttributeTok{by =} \StringTok{"day"}\NormalTok{)}

\NormalTok{sim\_size }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(date)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{surge\_height }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sim\_size) }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, sim\_size }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\NormalTok{surface\_height }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sim\_size) }\SpecialCharTok{+} \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{), }\AttributeTok{times =} \FunctionTok{c}\NormalTok{(}\DecValTok{1234}\NormalTok{, sim\_size }\SpecialCharTok{{-}} \DecValTok{1234}\NormalTok{))}

\NormalTok{waves }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tibble}\NormalTok{(date, surge\_height, surface\_height)}

\CommentTok{\# Extreme Value Plot}
\NormalTok{p3 }\OtherTok{\textless{}{-}}\NormalTok{ waves }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_extreme =}\NormalTok{ surge\_height }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ surge\_height)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ is\_extreme), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"FALSE"} \OtherTok{=} \StringTok{"grey40"}\NormalTok{, }\StringTok{"TRUE"} \OtherTok{=} \StringTok{"darkorange"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Wave surge height (m)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{9}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{,  }\AttributeTok{hjust =} \FloatTok{0.98}\NormalTok{, }\AttributeTok{vjust =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{),}
    \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Changepoint Plot}
\NormalTok{p4 }\OtherTok{\textless{}{-}}\NormalTok{ waves }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ surface\_height)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"grey40"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2021{-}07{-}17"}\NormalTok{),}
             \AttributeTok{colour =} \StringTok{"darkorange"}\NormalTok{,}
             \AttributeTok{linewidth =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Sea surface height (m)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{9}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{,  }\AttributeTok{hjust =} \FloatTok{0.98}\NormalTok{, }\AttributeTok{vjust =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{),}
    \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{plot\_grid}\NormalTok{(p3,p4,}\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{302-edav-analysis_files/figure-pdf/eva-and-changepoint-plots-1.pdf}

}

\caption{Example sea-height datasets where an analysis has been
triggered by an extreme value (above) or a visually identified change in
mean (below).}

\end{figure}%

All of these corrections require us to make assumptions about the nature
of the peeking. They are either very specific about the process that has
occurred, or else are very pessimistic about how much information has
been leaked. Developing such corrections to account for EDA isn't really
possible, given its adaptive and non-prescriptive nature.

In addition to being either highly specific or pessimistic, these
corrections can also be hard to derive and complicated to implement.
This is why in settings where the power of tests or level of estimation
is critically important, the entire analysis is pre-registered. In
clinical trials, for example, every step of the analysis is specified
before any data are collected. In data science this rigorous approach is
rarely taken.

As statistically trained data scientists, it is important for us to
remain humble about our potential findings and to suggest follow up
studies to confirm the presence of any relationships we do find.

\section{Learning More}\label{learning-more}

In this chapter we have acknowledged that exploratory analyses are an
important part of the data science workflow; this is true not only for
us as data scientists, but also for the other people who are involved
with our projects. We've also seen that an exploratory analysis can help
to guide the progression of our projects, but that in doing so we must
take care to prevent and acknowledge the risk of data leakage.

If you want to explore this topic further, it can be quite challenging.
Examples of good, exploratory data analyses can be difficult to come by
because, unlike the papers and technical reports that they precede,
exploratory analyses are not often made publicly available.
Additionally, they are often kept out of public repositories because
they are not as ``polished'' as the rest of the project. Personally, I
think this is a shame and the culture on this is slowly changing.

For now, your best approach to learning about what makes a good
exploratory analysis is to do lots of your own and to talk to you
colleagues about their approaches. There are lots of list-articles out
there claiming to give you a comprehensive list of steps for any
exploratory analysis. These can be good for inspiration, but I strongly
advise you against falling into the trap of treating these as
prescriptive or foolproof.

Despite the name of the chapter,
\href{https://bookdown.org/rdpeng/exdata/exploratory-data-analysis-checklist.html\#follow-up-questions}{Roger
Peng's EDA check-list} gives an excellent worked example of an
exploratory analysis in R. In discussion article
``\href{http://www.stat.columbia.edu/~gelman/research/published/p755.pdf}{Exploratory
Data Analysis for Complex Models}'', Andrew Gelman makes a more abstract
discussion of both exploratory analyses (which happen before modelling)
and confirmatory analyses (which happen afterwards).

\chapter{Data Visualisation}\label{edav-visualisation}

\section{Introduction}\label{introduction-4}

\subsection{More than a pretty
picture}\label{more-than-a-pretty-picture}

Data visualisation is an integral part of your work as a data scientist.

\begin{figure}[H]

{\centering \includegraphics{images/303-data-visualisation/the-climate-book-jacket.jpg}

}

\caption{Warming stripes graphic on the cover of ``The Climate Book''}

\end{figure}%

You'll use visualisations to rapidly explore new data sets, to
understand their structure and to establish which types of model might
be suitable for the task at hand. Visualisation is also vital during
model evaluation and when check the validity of the assumptions on which
that model is based. These are relatively technical uses of
visualisation, but graphics have a much broader role within your work as
an effective data scientist.

When well designed, plots, tables and animations can tell compelling
stories that were once trapped within your data. They can also
intuitively communicate the strength of evidence for your findings and
draw attention to the most salient parts your argument.

Data visualisation is an amalgamation of science, statistics, graphic
design and storytelling. It's multi-disciplinary nature means that we
have to draw on all of our skills to ensure success. While there are
certainly many ways to go wrong when visualising data, there are many
more ways to get it right.

This chapter won't be a a step-by-step tutorial of how to visualise any
specific type of data. Nor will it be a line-up of visualisations gone
wrong. Instead, I hope to pose some questions that'll get you thinking
critically about exactly what you want from each graphic that you
produce.

There are (at least) five things that you should think about when
producing any sort of data visualisation. We will consider each of these
in turn.

\section{Your Tools }\label{your-tools}

\subsection{Picking the right tool for the
job}\label{picking-the-right-tool-for-the-job}

When you think of data visualisation, you might immediately think of
impressive animations or complex, interactive dashboards that allow
users to explore relationships within the data for themselves.

Such tools are no doubt impressive but they are by no means necessary
for an effective data visualisation. In many cases there is no
technology is needed at all. The history of data visualisation vastly
pre-dates that of computers and some of the most effective
visualisations remain analogue creations.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{images/303-data-visualisation/coffee-2.png}

}

\caption{Coffee consumption, visualised. Jaime Serra Palou.}

\end{figure}%

This visualisation of a year's coffee consumption is an ideal example.
Displaying the number of cups of coffee in a bar chart or line graph
would have been a more accurate way to collect and display this data,
but that wouldn't have the same resonance or impact and it certainly
wouldn't have been as memorable.

\subsection{Analogue or Digital}\label{analogue-or-digital}

\subsubsection{Analogue Data Viz}\label{analogue-data-viz}

Here we have another example of an analogue data visualisation that is
created as part of data collection. Each member of the department is
invited to place a Lego brick on a grid to indicate how much caffeine
they have consumed and how much sleep they have had. The beauty of using
Lego bricks here is that they are stackable and so create a bar plot
over two dimensions.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{images/303-data-visualisation/lego-coffee.jpg}

}

\caption{Caffeination vs sleep, shown in lego. Elsie Lee-Robbins}

\end{figure}%

A third example can be found next to the tills in many supermarkets.
Each customer is given a token as they pay for their goods. They can
then drop this token into one of three large perspex containers as they
leave the shop, each representing a different charity. At the end of the
month 10,000 is split between the charities in proportion to the number
of tokens. Because the containers are made from a transparent material
you can see how the tokens are distributed, giving a visualisation of
the level of support for each of the charities.

There are many other way of constructing a physical, analogue
visualisation of your data and this doesn't need to be done as part of
the data collection process. The simplest and perhaps most obvious most
obvious is to create a plot of tabular data using a pen and paper.

\subsubsection{Digital Data Viz}\label{digital-data-viz}

When it comes to digital tools for data visualisation you have a
plethora of options. The most similar to pen-and-paper plotting is to
draw your visualisations using a Photoshop, or an open source equivalent
like Inkscape. The benefit here is that if you misplace a line or dot
you can correct this small error without having to start all over again.

There are then more data-focused tools that have point-and-click
interfaces. These are things like Excel's chart tools, or specialist
visualisation software like Tableau. These are great because they scale
with the quantity of data, so that you can plot larger amounts of raw
data values that you wouldn't have the time or patience to do by hand -
whether that's in an analogue or digital format.

Analogue and point-and-click approaches to visualisation have the shared
limitation of not being reproducible, at least not without extensive
documentation explaining how the graphic was created.

Using code to create your visualisations can resolve this
reproducibility issue, and includes visualisation as a part of your
larger, reproducible workflow for data science. Scripted visualisations
also scale easily to large data sets and are easy to alter if any
changes are required. The downside here is that there is a relatively
steep learning curve to creating such visualisations, which is exactly
what point-and-click methods are trying to avoid.

No matter how you produce your visualisations, the time spent on
developing your skills in that medium is what buys you the ability to
control and customise your creations. This upfront time investment will
also often make you faster at producing future graphics in that medium.

Whenever you approach a new visualisation problem, you should pick your
tools and medium judiciously. You have to balance your immediate needs
for speed, accuracy and reproducibility against your current skill level
and improving those skills in the medium to long term. Unfortunately,
the only way to make good visualisations is to make lots of bad ones and
even more mediocre ones first.

\subsection{\texorpdfstring{\texttt{ggplot2}}{ggplot2}}\label{ggplot2}

If your aim is to produce a wide range of high quality data
visualisation using R, then the \texttt{\{ggplot2\}} package is one of
the most versatile and well documented tools available to you.

The g's at the start of the package name stand for grammar of graphics.
This is an opinionated, abstract approach to constructing data
visualisations programmatically, by building them up slowly and adding
additional plot elements one layer at a time.

\includegraphics{images/303-data-visualisation/grammar-of-graphics-abstract.png}

This idea of a ``grammar of Graphics'' was originally introduced by
Leland Wilkinson. The paper shown by Hadley Wickham, and the associated
\texttt{\{ggplot2\}} package popularised this approach within the R
community. Like many of the tidyverse collection of packages that we
have met already, \texttt{\{ggplot2\}} provides simple but specialised
modular functions that can be composed to create complex visualisations.

If you'd like to learn how to use \texttt{\{ggplot2\}}, I wouldn't
recommend starting with the paper nor would I recommend trying to get
started with the docs alone. Instead, I would suggest you work through
an introductory tutorial
(e.g.~\href{https://www.cedricscherer.com/2019/05/17/the-evolution-of-a-ggplot/}{evolution
of a ggplot} or
\href{https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/}{beautiful
plotting in R}), or one of the resources linked within the
\href{https://ggplot2.tidyverse.org/\#learning-ggplot2}{package
documentation}.

Once you have a grasp of the basic principles \texttt{\{ggplot2\}}
(geometries, aesthetics, themes) the best way to improve is to make your
own plots, using reference texts and other people's work as a guide. A
great source of inspiration here is the
\href{https://github.com/rfordatascience/tidytuesday}{Tidy Tuesday} data
visualisation challenge. You can
\href{https://github.com/search?q=tidy\%20tuesday&type=repositories}{search
for the challenge on Github} to inspect both the plots made by other
people and the code that was used to make them.

\section{Your Medium }\label{your-medium}

\subsection{Where is your visualisation
going?}\label{where-is-your-visualisation-going}

The second aspect that I recommend you think about before starting a
data visualisation is where that graphic is going to be used. The
intended location for your visualisation will influence both the
composition of your graphic and also the amount of effort that you
dedicate to it.

\includegraphics{images/303-data-visualisation/visualisation-purposes.png}

For example, consider making an exploratory plot at the start of a
project to improve your own understanding of the data and its structure.
In this case you don't need to spend hours worrying about refining axis
labels, colour schemes or which file format to save your work in.

If you are developing a figure to share in a daily stand-up meeting with
your team then you should take a little more care to ensure that your
work can be clearly understood by others. For example you might spend
some time to ensure that the legend and axis labels are both large and
sufficiently informative.

Further refinement will be required if you plan to use your
visualisation in external presentation. Is the message of the
visualisation immediately clear? Will the graphic it still be clear when
displayed in a boardroom or conference hall, or will it pixellate?
Finally, how long will the audience have to interpret the visualisation
while you are speaking? Even if slide decks are made available, very few
audience members will actually refer to them before or after the
presentation.

The opposing consideration has to be made when preparing a visualisation
for a report or scientific paper. In print, plots and tables can be very
small - particularly within two-column or grid layouts. You then have to
be wary about the legibility of your smallest text (think values on
axes) to ensure that your visualisation can be clearly understood,
whether the document is being read zoomed-in on a computer screen or
printed out in black and white.

\subsection{File Types}\label{file-types}

\begin{figure}[H]

{\centering \includegraphics{images/303-data-visualisation/dalle-pixellated-image.png}

}

\caption{A low resoloution bitmap image.}

\end{figure}%

To ensure that your graphics are suitable for the intended medium it is
helpful to know a little bit about image file types.

There are two dominant types of image file: vector graphics and bitmap
graphics.

\textbf{Bitmap graphics} store images as a grid of little squares and
each of these pixels takes a single, solid colour. If you make a bitmap
image large enough, either by zooming in or by using a really big
screen, then these individual pixels become visible. Usually this isn't
going to be your intention, so you need to ensure that the resolution of
your graphic (its dimensions counted in pixels) is sufficiently large.

\textbf{Vector graphics} create images using continuous paths and fill
the areas that these enclose with flat colour. These vector images can
be enlarged as much as you like without the image quality becoming
compromised. This is great for simple simple designs like logos, which
have to be clear when used on both letterhead and billboards.

However, these vector graphics are more memory intensive than bitmap
images, particularly when there are many distinct colours or objects
within the image. This can be a particular problem in data science, for
example when creating a scatter plot with many thousands of data points.

It can often be useful to save both a bitmap and vector version of your
most important graphics. This way you can use bitmap when you need small
files that load quickly (like when loading a webpage) and vector
graphics when you need your visualisation to stay sharp when enlarged
(like when creating a poster or giving a presentation in an auditorium).

\section{Your Audience }\label{your-audience}

\subsection{Know Your Audience}\label{know-your-audience}

Data visualisations are a tool for communicating information. To make
this communication as effective as possible, you have to target your
delivery to the intended audience.

\begin{itemize}
\item
  Who is the intended audience for your visualisation?
\item
  What knowledge do they bring with them?
\item
  What assumptions and biases do they hold?
\end{itemize}

Creating \emph{personas} for distinct user groups can be a helpful way
to answer these questions, particularly when the user population are
heterogeneous.

\includegraphics{images/303-data-visualisation/user-groups.png}

To know \emph{how} to target your delivery to a particular audience, you
fist have to identify exactly who that is.

To make a compelling data visualisation you have to have some idea of
the background knowledge that the viewer brings. Are they a specialist
in statistics and data science or does their expertise the area of
application? Are the findings that you're presenting going to come as a
surprise to the viewer or will they validate their pre-existing
knowledge?

It's worth considering these prior beliefs and how strongly they are
held when constructing your visualisation. Take the time to consider how
this existing knowledge could alter or influence the viewer's
interpretation of what you're showing to them. If your conclusions go
against their experience or knowledge then you need to design your
visualisation to be as clear and persuasive as possible. On the
flip-side, you might be presenting information on a topic that the
viewer is ambivalent about, is actively bored by or is wilfully trying
to ignore. In that case, you can take special care to compose engaging
visualisations to capture and hold the attention of your audience.

\subsection{Preattentive Attributes}\label{preattentive-attributes}

When crafting a visualisation we want to require as little work as
possible from the viewer. We can do this by using pre-attentive
attributes such as colour, shape size and position to encode our data
values.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{images/303-data-visualisation/preattentive-attributes.png}

}

\caption{Examples of preattentive attributes}

\end{figure}%

These preattentive attributes are properties of lines and shapes that
provide immediate visual contrast without requiring active thought from
the viewer. As we will see, care needs to be taken here to ensure that
we are don't mislead the viewer with how we use these attributes.

\subsection{Example: First Impressions
Count}\label{example-first-impressions-count}

\begin{figure}[H]

{\centering \includegraphics{images/303-data-visualisation/average-male-height.jpg}

}

\caption{Issues with scales, area and perspective}

\end{figure}%

This figure presents a bar chart of the mean height of males in several
countries, but has swapped out the bars for human outlines. While the
visualisation has an attractive, minimal design and a pleasant colour
scheme, it doesn't do a good job of immediately conveying the relevant
information to the viewer.

The three main issues with this plot arise from swapping the bars of
this plot for male silhouettes and are caused by the difference in how
humans perceive lengths and areas. Typically, we make immediate
pre-attentive comparisons based on area but can draw more accurate,
considered comparisons when comparing lengths.

By replacing bars with human outlines this plot breaks the
proportionality of length and area that is inherent in a bar plot. This
causes dissonance between immediate and considered interpretation of
this plot. An additional issue is that the silhouettes overlap, creating
a forced perspective that makes it seem like some the outlines are
further back and therefore even larger if this forced perspective is
taken into account.

These three issues are important to consider when constructing your own
visualisations. Are you showing all the values that the data could take,
or focusing on a smaller interval to provide better contrast? If you are
using the size of a circle to represent a value, are you changing the
diameter or area in proportion to the data value? Finally, if you are
making a plot that appears three-dimensional, have you done so on
purpose? If so, could one of those dimensions be better represented by
an attribute other than position?

\subsection{Visual Perception}\label{visual-perception}

When reducing the dimensionality of your plot you may wish to represent
a data value using colour rather than position. When deciding on how to
use colour, you should keep your audience in mind.

Is your aim to two or more categories? In that case, you'll need to
select your finite set of colours and ensure that these can be
distinguished.

Are you are representing a data value that is continuous or has an
associated ordering? Then you will again have to select your palette to
provide sufficient contrast to all viewers of your work.

If you are representing a measurement that has a \emph{reference value}
(for example 0 for temperature in centigrade) then a diverging colour
palette can be used to represent data that are above or below this
reference point. This requires some cultural understanding of how the
colours will be interpreted, for example you are likely to cause
confusion if you an encoding of red for cold and blue for hot.

For colour scales without such a reference point then a gradient in a
single colour is likely the best option. In either case, it is important
to check that a unit change in data value represents a consistent change
in colour across all values. This is not the case for the rainbow
palette here (which is neither a single gradient nor a diverging colour
scale).

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{images/303-data-visualisation/saturated-colour-scales.png}

}

\caption{Some default colour scales in R}

\end{figure}%

To improve accessibility of your designs, I would recommend one of the
many on-line tools to simulate colour vision deficiency or using a
pre-made palette where this has been considered for you. A good,
low-tech rule of thumb is to design your visualisations so that they're
still easily understood when printed in greyscale. This can mean picking
appropriate colours or additionally varying the point shape, line width
or line types used.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{images/303-data-visualisation/desaturated-colour-scales.png}

}

\caption{Desatureated colour scales in R}

\end{figure}%

For a practical guide on setting colours see this
\href{https://bookdown.org/rdpeng/exdata/plotting-and-color-in-r.html}{chapter}
of exploratory data analysis by Roger Peng.

\subsection{Alt-text, Titles and
Captions}\label{alt-text-titles-and-captions}

\begin{quote}
\textbf{Captions} describe a figure or table so that it may be
identified in a list of figures and to add additional detail (where
appropriate).

\textbf{Alternative text} describes the content of an image for a person
who cannot view it.
(\href{https://myaccessible.website/infographic/images/images-accessibility-alt-text}{Guide
to writing alt-text})

\textbf{Titles} give additional context or identify key findings. Active
titles are preferable.
\end{quote}

When visualisations are included in a report, article or website, they
are often accompanied by three pieces of text. The title, the caption
and the alt-text all help the audience to understand a visualisation but
each serves a distinct purpose.

\subsubsection{Captions}\label{captions}

A caption is short description of a visualisation. Captions usually
displayed directly above or below the figure or table that they
describe. These captions serve two purposes: in a report, the caption
can be used to look up the visualisation from a list of figures or
tables. The second purpose of a caption is to add additional detail that
you don't want to add to the plot directly. For example caption might be
``Time series of GDP in the United States of America, 2017-2022. Lines
show daily (solid), monthly (dashed) and five-year (dotted) mean
values.''

\subsubsection{Alt-text}\label{alt-text}

Alt text or alternative text is used to describe the content of an image
to a person who can't view it. This text is helpful for people with a
visual impairment, particularly those who use a screen reader. Screen
reading software reads digital text out loud but can't interpret images.
Such software replaces the image with the provided alternative text. Alt
text is also valuable in cases when the image can't be found or loaded,
for example because of an incorrect file path or a slow internet
connection, because it will be displayed in place of the image.

The purpose of alt-text is different from a caption. It's designed as a
replacement for the image, not just a shorthand or to provide additional
information. If there is an important trend or conclusion to be drawn
from the visualisation (that is not already mentioned in the main text)
this should be identified in the alt-text. This sort of interpretation
is a key aspect of alt-text that shouldn't be included in a caption.

\subsubsection{Titles}\label{titles}

Titles give additional context that is not conveyed by the axis labels
or chart annotations. Alternatively the title can be used like a
newspaper headline to deliver the key findings of the visualisation. One
example of this might be when looking at a visualisation that is
composed of many smaller plots, each showing the GDP for a US state over
the last five years. Title for each smaller plot would identify the
state it is describing, while the overall title might be something like
``All US states have increased GDP in the period 2017-2022''.

If you are including this type of interpretive title, make sure that the
same interpretation is clear in the alt-text.

\section{Your Story }\label{your-story}

The fourth aspect of a successful data visualisation is that it must
tell a story. This story doesn't need to be a multi-generational novel
or even a captivating novella. If a picture speaks a thousand words,
really what your aiming for is the visual equivalent of an engaging
anecdote.

Your visualisation should be something the grabs viewers attention and
through its composition or content alters their knowledge or view of
world in some way.

Telling effective stories requires planning. How you construct your
narrative depends on what effect you want to have on your audience. I'd
encourage you to think like a data journalist and go about your work
with its intended effect clear in your mind. Is your purpose to inform
them of a fact, to persuade them to use a different methodology or
entertain them by presenting dull or commonplace data in a fresh and
engaging way?

\begin{figure}[H]

{\centering \includegraphics{images/303-data-visualisation/entertain-persuade-inform.png}

}

\caption{Three goals of data visualisation: to entertain, persuade and
perform}

\end{figure}%

In reality your goal will be some mixture of these, at an interior point
of this triangle. Clearly identifying this point will help you to
present your visual story in a way that works towards your aims, rather
than against them.

On the point of presentation, it is important to realise that there is
no neutral way to present information. In creating a visualisation
you're choosing which aspects of the data to emphasise, what gets
summarised and what is not presented at all. This is how you construct a
plot that tells a clear and coherent story. However, there is more than
one story that you could tell from a single dataset.

As an example of this, let's consider a time-series showing the price of
two stocks and in particular the choice of scale on the y-axis. Suppose
the two stock have values fluctuating around \$100 per share. Choosing a
scale that goes from \$90 to \$120 would emphasise the differences
between the two stocks. Setting the lower limit to \$0 would instead
emphasise that these variations are small relative to the overall value
of the stocks. Both are valid approaches but tell different stories. Be
clear and be open about which of these you are telling and why you have
chosen that over the alternative.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(cowplot)}

\CommentTok{\# Set simulation parameters}
\NormalTok{n\_out }\OtherTok{\textless{}{-}} \DecValTok{365} 
\NormalTok{initial\_value }\OtherTok{\textless{}{-}} \DecValTok{100}

\CommentTok{\# Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{007}\NormalTok{)}

\NormalTok{price }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA\_real\_}\NormalTok{, n\_out)}
\NormalTok{price[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ initial\_value}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{n\_out) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.01}\NormalTok{)}
\NormalTok{  b }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  price[i] }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{*}\NormalTok{ price[i}\DecValTok{{-}1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ b}
\NormalTok{\}}

\NormalTok{stock }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{date =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2023{-}01{-}01"}\NormalTok{), }\AttributeTok{length.out =}\NormalTok{ n\_out, }\AttributeTok{by =} \StringTok{"day"}\NormalTok{),}
  \AttributeTok{price =}\NormalTok{ price}
\NormalTok{)}

\CommentTok{\# Construct plot}
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ stock }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Stock Price, USD"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{9}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{,  }\AttributeTok{hjust =} \FloatTok{0.98}\NormalTok{, }\AttributeTok{vjust =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{),}
    \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.02}\NormalTok{)}
\NormalTok{  )}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{125}\NormalTok{)}
\NormalTok{p2 }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{125}\NormalTok{)}

\FunctionTok{plot\_grid}\NormalTok{(p1, p2, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{303-edav-visualisation_files/figure-pdf/unnamed-chunk-1-1.pdf}

A final cross-over from data journalism is that your visualisations will
be competing for your viewers attention. You have to compete against
everything else that is going on in their lives. Establish a clear
``hook'' within your visualisation to attract your viewer's attention
and immediately deliver the core message. This might be done with a
contrasting trend-line or an intriguing title. Lead their attention
first to the key message and then the supporting evidence.

\section{Your Guidelines }\label{your-guidelines}

\subsection{Standardise and Document}\label{standardise-and-document}

The final consideration when creating visualisations is to reduce the
number of considerations that you have to make in the future. This is
done by thinking carefully about each of the decisions that you make and
writing guidelines so that you make these choices consistently.

The choices that go into making an effective data visualisation are
important and deserve careful consideration. However, this consideration
comes at a cost. To the employer this is the literal, financial cost of
paying for your time. More broadly this is the opportunity cost of all
the other things that you could have been doing instead.

To be efficient in our visualisation design, we should extend our DRY
coding principles the design processes. Make choices carefully and
document your decisions to externalise the cognitive work required of
you in the future.

Many companies aware of these financial and opportunity costs and
provide style guides for visualisations in a similar manner to a coding
or writing style guide. This not only externalises and formalises many
decisions, but it also leads to a more uniform style across
visualisations and the data scientists producing them. This leads to a
unified, house-style for graphic design and a visual brand that is
easily identifiable. This is beneficial for large companies or personal
projects alike.

\subsection{Example Style Guides}\label{example-style-guides}

I'd highly recommend exploring some visualisation guides to get an idea
of how these are constructed and how you might develop your own.

Unsurprisingly some of the best guides come from media outlets and
government agencies. These businesses are used to writing style guides
for text to create and maintain a distinctive style across all of their
writers.

\begin{itemize}
\item
  BBC

  \begin{itemize}
  \tightlist
  \item
    \href{https://www.bbc.co.uk/gel/features/how-to-design-infographics}{Infographics
    Guidelines}
  \item
    \href{https://bbc.github.io/rcookbook/}{R Cookbook}
  \item
    \href{https://github.com/bbc/bbplot}{\texttt{\{bbplot\}}}
  \end{itemize}
\item
  \href{https://design-system.economist.com/documents/CHARTstyleguide_20170505.pdf}{The
  Economist}
\item
  \href{https://style.ons.gov.uk/category/data-visualisation/}{The
  Office for National Statistics}
\item
  \href{https://ec.europa.eu/eurostat/web/products-eurostat-news/-/STYLE-GUIDE_2016}{Eurostat}
\item
  \href{https://urbaninstitute.github.io/graphics-styleguide/}{Urban
  Institute}
\item
  \href{https://pudding.cool/resources/}{The Pudding} (learning
  resources)
\end{itemize}

The level of detail and technicality varies wildly between these
examples. For instance, the BBC do not provide strong guidelines on the
details of the final visualisation but do provide a lot of technical
tools and advice on how to construct those in a consistent way across
the corporation. They've even gone so far as to write their own theme
for ggplot and to publish this as an R package!

\section{Wrapping Up}\label{wrapping-up-4}

 Think about your \emph{tools}.

 Think about your \emph{medium}.

 Think about your \emph{audience}.

 Think about your \emph{story}.

 Think about your \emph{guidelines}.

Data visualisation might seem like a soft skill in comparison to data
acquisition, wrangling or modelling. However, it is often effective
visualisations that have the greatest real world impact.

It is regularly the highly effective figures within reports and
presentations that determine which projects are funded or renewed.
Similarly, visualisations in press releases can determine whether the
result of your study are trusted, correctly interpreted, and remembered
by the wider public.

When constructing visualisations it is important to consider whether
there are existing guidelines that provide helpful constraints to your
work. From there, determine the story that you wish to tell and exactly
who it is that your are telling that story to. Once this is decided you
can select the medium and the tools that you use to craft your
visualisation so that you have the greatest chance of achieving your
intended effect.

\chapter*{Checklist}\label{edav-checklist}
\addcontentsline{toc}{chapter}{Checklist}

\markboth{Checklist}{Checklist}

\section*{Videos / Chapters}\label{videos-chapters-2}
\addcontentsline{toc}{section}{Videos / Chapters}

\markright{Videos / Chapters}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b25a6d35-1e91-4090-b4f8-af9600b39086}{Data
  Wrangling} (20 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/03-01-data-wrangling/03-01-data-wrangling.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a986f50c-90fe-4379-b4a1-af9600ba22fd}{Data
  Exploration} (25 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/03-02-eda/03-02-data-exploration.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=2ebbfa94-0b5d-45e9-b708-af8100d9664a}{Data
  Visualisation} (27 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/03-03-data-visualisation/03-03-data-visualisation.pdf}{{[}slides{]}}
\end{itemize}

\section*{Reading}\label{reading-2}
\addcontentsline{toc}{section}{Reading}

\markright{Reading}

Use the \hyperref[edav-reading]{Data Exploration and Visualisation}
section of the reading list to support and guide your exploration of
this week's topics. Note that these texts are divided into core reading,
reference materials and materials of interest.

\section*{Activities}\label{activities}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\emph{Core:}

\begin{itemize}
\item[$\square$]
  \href{https://normconf.com/}{NormConf} is a conference dedicated to
  the unglamorous but essential aspects of working in the data sciences.
  The conference talks from December 2022 are available as a
  \href{https://www.youtube.com/@normconf/videos}{Youtube Playlist}.
  Find a talk that interests you and watch it, then post a short summary
  to EdStem, describing what you learned from the talk and one thing
  that you still do not understand.
\item[$\square$]
  Work through this ggplot2 tutorial for
  \href{https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r}{beautiful
  plotting in R} by Cdric Scherer, recreating the examples for
  yourself.
\item[$\square$]
  Using your \texttt{rolling\_mean()} function as inspiration, write a
  \texttt{rolling\_sd()} function that calculates the rolling standard
  deviation of a numeric vector.

  \begin{itemize}
  \item[$\square$]
    Extend your \texttt{rolling\_sd()} function to optionally return
    approximate point-wise confidence bands for your rolling standard
    deviations. These should be \(\pm2\) standard errors by default and
    may be computed using analytical or re-sampling methods.
  \item[$\square$]
    Create a visualisation using your extended \texttt{rolling\_sd()}
    function to assess whether the variability in the daily change in
    Dow Jones Index is changing over time.
    \href{data/dowjones.csv}{{[}data{]}}
  \end{itemize}
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\item[$\square$]
  Add your \texttt{rolling\_sd()} function to your R package, adding
  documentation and tests.
\item[$\square$]
  During an exploratory analysis, we often need to assess the validity
  of an assumed distribution based on a sample of data. Write your own
  versions of \texttt{qqnorm()} and \texttt{qqplot()}, which add
  point-wise tolerance intervals to assess whether deviation from the
  line \(y=x\) are larger than expected.
\item[$\square$]
  Add your own versions of \texttt{qqnorm()} and \texttt{qqplot()} to
  your R package, along with documentation and tests.
\end{itemize}

\section*{Live Session}\label{live-session-2}
\addcontentsline{toc}{section}{Live Session}

\markright{Live Session}

In the live session we will begin with a discussion of this week's
tasks. We will then break into small groups for two data visualisation
exercises.

(\textbf{Note:} For one of these exercises, it would be helpful to bring
a small selection of coloured pens or pencils, of you have access to
some. If not, please don't worry - inventive use of black, blue and
shading are perfectly acceptable alternatives!)

Please come to the live session prepared to discuss the following
points:

\begin{itemize}
\item
  Which NormConf video did you watch and what did you learn from it?
\item
  Other than \texttt{\{ggplot2\}}, what else have you used to create
  data visualisations? What are their relative strengths and weaknesses?
\item
  How did you implement your \texttt{rolling\_sd()} function and what
  conclusions did you draw when applying it to the Dow Jones data?
\end{itemize}

\part{Preparing for Production}

\part{Introduction}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-important-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-important-color-frame, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\chapter{Reproducibility}\label{production-reproducibility}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{The Data Scientific Method}\label{the-data-scientific-method}

In what we have covered so far we have been very much focused on the
first aspect of data science: the data. When we come to consider about
whether our work can be reproduced or our results can be replicated,
this shifts our focus to the second other, the science.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/401-production-reproducibility/scientific-method.png}

}

\caption{Cycle of scientific enquiry.}

\end{figure}%

As data scientists, we like to think that we are applying the scientific
method in our work.

We start with a question we want to answer or a problem we want to
solve. This is followed by a search of the existing literature: is this
is a well-known problem that lots of people have solved before? If it
is, fantastic, we can learn from their efforts. If not, then we proceed
to gather our own evidence and combine this with whatever existing
knowledge we could scrape together. Finally, we draw conclusions from
this synthesised information.

When doing so we acknowledge that the conclusions we reach are not the
truth, just our current best approximation of it. We have a usefully
simplified model of the messy reality that we can share with the world.
We will happily update our model as we become aware of new evidence,
whether that new information supports or contradicts our current way of
thinking.

That sounds excellent and is, in an ideal world, how both science and
data science would progress. However, just like our models this is a
simplified (and in this case idealised) description of what really
happens.

\section{Issue: Multiple, Dependent
Tests}\label{issue-multiple-dependent-tests}

\begin{itemize}
\item
  Projects are usually not a single hypothesis test
\item
  Sequence of dependent decisions
\item
  e.g.~Model development
\item
  Can fool ourselves by looking lots of times or ignoring sequential and
  dependent structure.
\end{itemize}

The aims of a data science project are rarely framed as a clear an
unambiguous hypothesis, for which we will design a study and perform a
single statistical analysis. Apart from in special cases, like A/B
testing, we have a much more general aim for our data science projects.

\includegraphics{images/401-production-reproducibility/forking-paths-abstract.png}\{Abstract
text of Gelman and Loken (2013).\}

We might want to construct a model for a given phenomenon and we'll try
many variants of that model along the way. By taking a more relaxed
approach to data analysis, data scientists can run the risk of finding
spurious relationships within our data that don't hold more generally.
If you look for a relationship between the quantity you are trying to
predict and enough samples of random noise will almost surely find a
significant relationship, even though there is no true link.

\section{\texorpdfstring{Issues: \(p\)-hacking and Publiction
Bias}{Issues: p-hacking and Publiction Bias}}\label{issues-p-hacking-and-publiction-bias}

\includegraphics{images/401-production-reproducibility/p-values-in-medical-papers.jpg}\{Bar
chart of p-values in medical publications, showing a large drop between
4-5 percent and 5-6 percent.\}

Okay, so our methods of investigation as data scientists might not be
completely sound, but that should be balanced by the results of other
studies that exist, right? Well, the second worrisome aspect of this
process is that we can't always trust the published literature to be a
fair representation of what people have tried in the past.

Studies that don't provide strong evidence against the null hypothesis
rarely make it into publications, reports or the news. This is largely
because of the way that scientific enquiry is rewarded in the the
academy, business and the media. Funding and attention tend to go to
studies with new findings, rather than those which aim to confirm or
strengthen the findings of existing studies.

This systemic problem incentives scientists to `massage' numbers to
obtain a \(p\)-value less than 0.05 so that a result can be reported as
statistically significant. This process is known as \(p\)-hacking and
can occur through deliberate malpractice but more often it's a result of
scientists generally not receiving adequate training in statistics. As
statistically trained data scientists we know that a declaration of
significance is no indication of a meaningful effect size and that the
conventional significance level of 5\% is entirely arbitrary. However,
we need to be aware that this is not the case across science and that
even we aren't immune to the societal and systematic influences that
favour the publication of novel results over confirmatory ones.

These influences also lead to a much more subtle problem than direct
\(p\)-hacking. Consider a model with an obvious but unnecessary
additional property to add: a example here might be adding an
unnecessary term to a regression. Because this extension is such low
hanging fruit, many scientists independently design experiments to test
it out. Most of these experiments provide insufficient evidence against
the null hypothesis and don't get developed into published papers or
technical reports, because they just support the status-quo. However,
after enough people have attempted this some scientist will get
``lucky'' and find a significant (and potentially relevant) benefit. Of
all the experiments that were done, this is the only one that makes it
onto the public record.

All of these studies being left in the proverbial desk drawer induces
publication bias in the scientific literature. When we come to assess
the state of existing knowledge, we are unable to properly assess the
importance of findings, because we lack the context of all those null
results that went unreported.

This same process means that the results of many more scientific studies
than we would expect cannot be recreated. This is what is known as the
scientific replication crisis.

\section{Reproducibility}\label{reproducibility-1}

\begin{quote}
\emph{Reproducibility:} given the original raw data and code, can you
get all of the results again?
\end{quote}

\begin{itemize}
\item
  Reproducible != Correct
\item
  ``Code available on request'' is the new ``Data available on request''
\item
  Reproducible data analysis requires effort, time and skill.
\end{itemize}

This idea of reproducibility requires us to be able to recover the exact
same numerical summaries as the original investigator. In particular
this means we should be able to reproduce the exact same point estimates
and measures of uncertainty that they did, which ensures we'll draw the
same conclusions as that original investigator.

When putting our work into production there are several reasons why we
might require it to be reproducible. The first is logistical: production
code needs to be robust and efficient - this often means your code will
be re-factored, rewritten or translated into another language. If your
results are not reproducible then there is no way to verify that this
has been done correctly. Secondly, if a problem is identified in your
work (say a customer raises a complaint that their loan application was
incorrectly rejected) you need to be able to accurately recreate that
instance to diagnose if there is a problem and exactly what caused it.

Note that just because findings are reproducible, that doesn't by any
means imply that they're correct. We could have a very well documented
but flawed analysis that is entirely reproducible but is also completely
unsuitable or just plain wrong.

In our data science projects, we have already taken several steps that
greatly improve the reproducibility of our work. Although we scripted,
tested and documented our work to improve the management of our project,
these decisions improve the scientific quality of our work too. This
puts us in a strong position relative to the scientific literature as a
whole.

At a point now where it is almost standard to publish data along with
papers, but for a long time this was not the case and data if data were
available at all, this was only by request. We are now in a similar
situation when it comes to code. It's still far from standard for the
analysis code to be required and put up to detailed scrutiny as part of
the peer-review process.

With a little more context this isn't so unreasonable. Across many
scientific disciplines, code-based approaches to analysis is not
standard; statistical software with a graphical user interface is used
instead. The idea here is to allow scientists to analyse their own data
by providing tools trough a combination of menus and buttons. However,
these interfaces often leave no record of how the data were manipulated
and the software itself can be highly specialised or proprietary. This
combination means that even when full datasets are provided, it is often
impossible for others to reproduce the original analysis.

None of this is meant to scold or disparage scientists who use this type
of software to allow them to perform statistical analyses. You're well
aware of how much time and effort it takes to learn how to use and
implement statistical methods correctly. This is time that other
scientists invest in learning their subject, so that they can get to the
point of doing research in the first place. This is one of the wonders
of data science: the ability to work in multi-disciplinary teams where
individual members are specialised in different areas.

This is where we need to pause and check ourselves, because the same
fate can easily befall us as data scientists. Yes, it take time to learn
the skills and practices to ensure reproducibility, but it also takes
time to implement them and the time of an expert data practitioner
doesn't come cheap. If you wait until the end of a project before you
make it reproducible then you'll usually be too late - time or money
will have run out.

\section{Replicability}\label{replicability}

\begin{quote}
\emph{Replicable:} if the experiment were repeated by an independent
investigator, you would get slightly different data but would the
substative conclusions be the same?
\end{quote}

\begin{itemize}
\item
  In the specific sense, this is the core worry for a statistician!
\item
  Also used more generally: are results stable to perturbations in
  population / study design / modelling / analysis?
\item
  Only real test is to try it. Control risk with shadow and parallel
  deployment. Statisticians are well aware that if we were to repeat an
  experiment we would get slightly different data. This would lead to
  slightly different estimates and slightly different results.
\end{itemize}

Ultimately, this is the core problem that statisticians get paid to
worry about: will those changes be small enough that the substantive
conclusions are not impacted? Yes, point estimates will vary slightly
but do your conclusions about the existence, direction or magnitude of
an effect still hold? Alternatively, if you are estimating a
relationship between two variables, is the same functional form chosen
as the most suitable?

In a general scientific context, replication takes a more broad meaning
and asks whether the key properties of your results could be replicated
by another person. In the context of getting your work put into
production, we acre concerned about whether your the results of your
findings will also hold when applied to future instances that might
differ from those you have seen already.

If you come to data science from a statistical background then you are
well accustomed to these sorts of considerations. Whenever you perform a
hypothesis test or compare two models, you take steps to make sure the
comparison is not only valid for this particular sample, but that is
also true out-of-sample. This is the whole reason data scientists make
training and test sets in the first place, as an approximate test for
this sort of generalisation. We we do any of these things we are asking
of ourselves: will this good performance replicate if we had different
inputs?

Of course train-test split, bootstrap resampling or asymptotic arguments
can only ever approximate the ways in which our sample differs from the
production population, to which our models will be applied. The only way
to truly assess the out-of-sample performance of our models or
generalisability of our findings is to put that work into production.

This opens us up to risk: what if our findings don't generalise and our
new model is actually much worse than the current one? It's not possible
to both deploy our new model and also avoid this risk entirely. However,
we can take some steps to mitigate our level of exposure.

\subsection{Shadow deployment}\label{shadow-deployment}

In the most risk-adverse setting we might implement a shadow deployment
of our new model. In this case, the current model is still used for all
decision making but our candidate model is also run in the background so
that we can see how it might behave in the wild. This is good in that we
can identify any points of catastrophic failure for the new model, but
is also expensive to run and can give us only limited information.

Suppose, for example, our model is a recommender system on a retail
website. A shadow deployment will let us check that the new system
functions correctly and we can gather data on what products are
recommended to each customer and investigate how these differ from those
recommended by the current system. A shadow deployment cannot in this
case tell us what the customer would have done had they been shown those
products instead. This means that a shadow deployment doesn't allow is
to investigate whether the new system leads to equivalent or greater
revenue than the current system.

\subsection{Parallel deployment}\label{parallel-deployment}

Parallel deployment or A/B tests have both the current and the proposed
new models running at the same time. This allows us to truly test
whether our findings generalise to the production population while
controlling our level of risk exposure by setting the proportion of
times each model is used. The more instances we assign to the new model
the faster we will learn about its performance but this also increases
our risk exposure.

\section{Reproduction and Replication in Statistical Data
Science}\label{reproduction-and-replication-in-statistical-data-science}

\subsection{Monte Carlo Methods}\label{monte-carlo-methods}

In data science we rely a lot on the use of stochastic methods. These
are often used to increase the chance of our findings being replicated
by another person or in production. However, they also make it more
difficult to ensure that our exact results can be reproduced, whether by
another person or our future selves.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{401-production-reproducibility_files/figure-pdf/monte-carlo-approximation-of-pi-1.pdf}

}

\caption{Monte Carlo approximation of \(\pi\)}

\end{figure}%

Monte Carlo methods are any modelling, estimation or approximation
technique that leverages randomness in some way.

We have seen examples of this to improve the probability of successful
replication. The most obvious example of this is the random allocation
of data in a Train/Test split for model selection.

Another example focused on improving replication is the use of bootstrap
resampling to approximate the sampling distribution of a test statistic.
This might be a parametric bootstrap, where alternative datasets are
generated by sampling values from the fitted model of our observed data.
Alternatively, a non-parametric bootstrap would generate these
alternative datasets by sampling from the original data with
replacement.

Monte Carlo methods can also be used to express uncertainties more
generally, or to approximate difficult integrals. These are both common
applications of Monte Carlo methods in Bayesian modelling, where (unless
our models are particularly simple) the posterior or
posterior-predictive distribution has to be approximated by a collection
of values sampled from that distribution.

Each time we run any of these analyses we'll get slightly different
outcomes. For our work to be replicable we need to quantify this level
of variation. For example, if we had a different sample from the
posterior distribution, how much would the estimated posterior mean
change by? Sometimes, as in this case, we can appeal to the law of large
numbers to help us out. If we take more samples, the variation between
realisations will shrink. We can then collect enough samples to our
estimated mean is stable across realisations, up to our desired number
of significant figures.

To make our results reproducible we will have to ensure that we can
reproduce the remaining, unstable digits for any particular realisation.
We can do this by setting the seed of our random number generator, an
idea we will return to shortly.

\subsection{Optimisation}\label{optimisation}

Optimisation is the second aspect of data science that can be very
difficult to ensure is reproducible and replicable.

Is the optimum you find stable over:

\begin{itemize}
\tightlist
\item
  runs of the procedure?
\item
  starting points?
\item
  step size / learning rate?
\item
  realisations of the data?
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics{images/401-production-reproducibility/contour-optimisation.png}

}

\caption{A poorly drawn contour plot. Local modes make this optimiation
unstable to the choice of starting point.}

\end{figure}%

If optimisation routines are used in parameter estimation, we have to
ensure that the results they find for a particular data set are
reproducible for a given configuration. This might be a given data set,
initial set of starting parameters and step size, the learning rate
(which controls how that step size changes) and the maximum number of
iterations to perform.

We additionally need to be concerned about replication here. If we had
chosen a different starting point would the optimisation still converge,
and would it converge to the same mode? Our original method may have
found a local optimum but how can we confirm that this is a global
optimum?

If the optimisation fails to converge, can your reproduce that case to
diagnose the problem? This can be particularly tricky when the
optimisation routine itself uses Monte Carlo methods, such as stochastic
gradient descent or simulated annealing.

\subsection{(Pseudo-)Random Numbers}\label{pseudo-random-numbers}

Sometimes we have stochastic elements to our work that we can't use
brute force to eliminate. Perhaps this is beyond our computational
abilities or else the number of realisations is an important, fixed
aspect of our study.

Fortunately, in computing it's not common to have truly random numbers.
Instead, what we usually have is a complex but deterministic function
that generates a sequence of numbers that are statistically
indistinguishable from a series of independent random variates.

The next term in this sequence of pseudo-random numbers is generated
generated based on value the current one. This means that, by setting
the starting point, we can always get the same sequence of pseudo-random
variates each time we run the code. In R we set this starting point
using \texttt{set.seed()}

This is especially useful for simulations that involve random variables,
as it allows us to recreate the same results exactly. This not only
makes it possible for others to recreate our results but it can also
make it much easier to test and debug our own code.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# different values}
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] {-}1.2053334  0.3014667 {-}1.5391452  0.6353707}
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{} [1]  0.7029518 {-}1.9058829  0.9389214 {-}0.2244921}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the same value}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] {-}1.2070657  0.2774292  1.0844412 {-}2.3456977}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] {-}1.2070657  0.2774292  1.0844412 {-}2.3456977}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Beware}\label{beware}

When running code sequentially and interactively, setting the seed is
about all you will need to solve reproducibility problems. However, I'd
advise you to take great care when combining this with some of the
methods we'll see for speeding up your code. In some cases, the
strategies to optimize your code performance can interfere with the
generation of random numbers and lead to unintended results.

When writing code that's executed in parallel across multiple cores or
processors, you have to carefully consider is whether or not to give
each the same seed value. The correct decision here is context specific
and depends on the interpretation of the random variates you will be
generating. If you are making a comparison between iterations it might
be important the the random aspects are kept as similar as possible,
while if you are paralleling only for speed gains this might not be
important at all.

Finally, it's important to be wary of the quality of pseudo-random
number generation and the interfacing R with other programming
languages. R was developed as a \emph{statistical} programming language
and most other languages are not as statistically focused. Different
languages may use different algorithms for generating pseudo-random
numbers, and the quality of the generated numbers can vary. It's
important to make sure that seeds are appropriately passed between
languages to ensure that the correct sequence of random numbers is
generated.

\section{Wrapping Up}\label{wrapping-up-5}

To get our work put into production it should be both reproducible and
replicable.

\begin{itemize}
\item
  \emph{Reproducible:} can recreate the same results from the same code
  and data
\item
  \emph{Replicable:} core results remain valid when using different data
\end{itemize}

While randomness is a key part of most data science workflows but can
lead to reproducibility nightmares. We can manage these by appealing to
the stability of averages in large samples and by explicitly setting the
sequence of pseudo-random numbers that we generate using
\texttt{set.seed()}.

Finally, we should take special care when we have to combine efficiency
with replicable workflows.

With these ideas you can now use good data to do good science.

\chapter{Explainability}\label{production-explainability}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{What are we explaining and to
whom?}\label{what-are-we-explaining-and-to-whom}

There are many reasons you might need to explain the behaviour of your
model before it can be put into production. As an example, we can
consider a credit scoring system that determines whether or not
customers should be given a line of credit.

\begin{itemize}
\tightlist
\item
  Regulatory or legal requirements to describe how your model works
  (e.g.~ban on ``black-box'' modelling).
\item
  Understanding how your model works to improve it.
\item
  Explaining to individual load decisions to customers.
\end{itemize}

In each of these cases, what exactly do we mean by an explanation? It's
likely not the same thing in each example.

\begin{itemize}
\item
  Data scientists we might be interested to know exactly what types of
  mapping between covariates and responses can be represented by the
  neural network architecture underlying the credit scoring system.
\item
  Stakeholders within the company or regulators are likely indifferent
  to this and are more concerned about understanding the general
  behaviour of the model across large numbers of loan applications.
\item
  Finally, individual customers might have some investment in the
  overall behaviour of the scoring model but would also like to know
  what actions they can take to increase their chance of securing a
  loan.
\end{itemize}

Between each of these examples, the level of technical detail differs
but more importantly the fundamental nature of the explanations are
different.

\section{Explaining a Decision Tree}\label{explaining-a-decision-tree}

With some models giving an explanation is relatively straightforward.
Decision trees are perhaps the easiest model to explain because they
mimic human decision making and can be represented like flow-charts that
make sequential, linear partitions of the predictor space.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{images/402-production-explainability/ambulance-triage.png}

}

\caption{\label{fig-abulance-triage}An example of a decision tree,
optimised to correctly identify category 1 ambulance calls in as few
questions as possible.}

\end{figure}%

These models use the same sort of logic that is used for medical triage
when you call an ambulance, to determine the urgency of the call. The
binary decisions used in this type of triage are optimised to identify
critical calls as soon as possible, but this is just one form of loss
function we could use. We might instead pick these partitions to get the
most accurate overall classification of calls to urgency categories.
This might not be an appropriate loss function for ambulance calls but
might be when deciding which loan applicants to grant credit to.

The issue is that these decision trees are limited in the relationships
they can represent (linear relationships approximated by step function)
and are sensitive to small changes in the training data. To overcome
these deficiencies we can use a bootstrap aggregation or a random forest
model to make predictions based on a collection of these trees. This
leads to models that are more stable and flexible but also removes any
chance of a simple and human-friendly explanation.

\section{Explaining Regression
Models}\label{explaining-regression-models}

Another model that is relatively straightforward to interpret is a
linear regression. We can interpret this model using the estimated
regression coefficients, which describe how the predicted outcome
changes with a unit change in each covariate while the values of all
other covariates are held constant.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{images/402-production-explainability/interpreting-linear-regression.png}

}

\caption{\label{fig-interpreting-linear-regression}Linear models have
global, conditional explanations, provided by the estimated regression
coefficients.}

\end{figure}%

This is a global and a conditional explanation.

It is \textbf{global} because the effect of increasing a covariate by
one unit is the same no matter what the starting value of that
covariate. The explanation is the same in all parts of the covariate
space.

The explanation is \textbf{conditional} because it assumes that all
other values are held constant. This can lead to some odd behaviour in
our explanations, they are dependent on what other terms are included
(or left out of) our model.

This can be contrasted against non-linear regression, where covariate
effects are still interpreted conditional on the value of other
covariates but the size or direction of that effect might vary depending
on the value of the covariate.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{images/402-production-explainability/interpreting-nonlinear-regression.png}

}

\caption{\label{fig-interpreting-nonlinear-models}Non-linear models have
local, conditional explanations, provided by the estimated regression
coefficients.}

\end{figure}%

Here we have an example where a unit increase in the covariate is
associated with a large change in the model response at low values of
the covariate, but a much smaller change at large values of the
covariate.

\section{Example: Cherrywood
regression}\label{example-cherrywood-regression}

As an example of this we can look at the height, girth and volume of
some cherry trees.

If we are wanting to use a lathe to produce pretty, cherry wood
ornaments we might be interested in understanding how the girth of the
trees varies with their height and total volume. Using a linear model,
we see that both have a positive linear association with girth.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{402-production-explainability_files/figure-pdf/fig-cherry-tree-lm-1.pdf}

}

\caption{\label{fig-cherry-tree-lm}Cherry tree girth can be well
modelled as a linear function of either tree height or harvestable
volume.}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(Girth }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Height, }\AttributeTok{data =}\NormalTok{ trees)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Girth \textasciitilde{} 1 + Height, data = trees)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{} (Intercept)       Height  }
\CommentTok{\#\textgreater{}     {-}6.1884       0.2557}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(Girth }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Volume, }\AttributeTok{data =}\NormalTok{ trees)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Girth \textasciitilde{} 1 + Volume, data = trees)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{} (Intercept)       Volume  }
\CommentTok{\#\textgreater{}      7.6779       0.1846}
\end{Highlighting}
\end{Shaded}

However, when we include both terms in our model, our interpretation
changes dramatically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(Girth }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Height }\SpecialCharTok{+}\NormalTok{ Volume, }\AttributeTok{data =}\NormalTok{ trees)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Girth \textasciitilde{} 1 + Height + Volume, data = trees)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{} (Intercept)       Height       Volume  }
\CommentTok{\#\textgreater{}    10.81637     {-}0.04548      0.19518}
\end{Highlighting}
\end{Shaded}

Height is no longer positively associated with girth. This is because
the size, direction and significance of our estimated effects is
conditional on what other terms are included in the model. For a
\emph{fixed volume} of wood, a taller tree necessarily has to have a
smaller girth.

Techniques such as
\href{https://christophm.github.io/interpretable-ml-book/shap.html}{SHAP}
try to quantify the importance of a predictor by averaging over all
combinations of predictors that might be included within the model. You
can read more about such techniques in
\href{https://christophm.github.io/interpretable-ml-book/}{Interpretable
Machine Learning} by Christoph Molnar.

\section{Simpson's Paradox}\label{simpsons-paradox}

This effect is related to \textbf{Simpson's Paradox}, where a trend
appears in several groups of data but disappears or reverses when the
groups are combined.

This regularly arises in fields like epidemiology, where population
level trends are assumed to apply at the level of individuals or small
groups, where this is known as the ecological fallacy.

Actually, Simpson's parodox is a terrible name, because it isn't
actually a paradox at all. It's not surprising that we have two
different answers to two different questions, the supposed contradiction
only arises when we fail to distinguish between those questions.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{402-production-explainability_files/figure-pdf/fig-simpsons-paradox-1.pdf}

}

\caption{\label{fig-simpsons-paradox}A simulated example in which a
covariate has a negative trend at the population level but a positive
trend within each sub-group.}

\end{figure}%

What I hope to have highlighted here is that for some of the simplest
models we might use as data scientists, explanations are very much
possible but must be made with care and attention to detail - correctly
interpreting these models in context can be far from straightforward.

\section{What hope do we have?}\label{what-hope-do-we-have}

At this stage, you might be asking yourself what hope we have of
explaining more complex models like random forests or neural networks,
given how difficult it is to explain even the simple models we might
take as our benchmark. You'd be right to worry about this and it is
important to remain humble in what we can and cannot know about these
complex systems that we are building.

All hope isn't lost though - we still have a few tricks up our sleeve!

\subsection{Permutation Testing}\label{permutation-testing}

Suppose we're asked by our product manager to determine which predictors
or covariates are usually the most import in our model when determining
credit scores and loan outcomes.

One way to do this would be to remove each covariate from the model and
investigate how that changes the predictions made by our model. However,
we've seen already that removing a predictor can substantively change
some models.

So instead, we could answer this question by using permutation methods.

If we take the observed values of a covariate, say income, and randomly
allocate these among all our training examples then this will destroy
any association between income and loan outcome. This allows us to
remove the information provided by income but without altering the
overall structure of our model. We can then refit our model to this
modified data set and investigate how rearranging the covariate alters
our predictions. If there is a large performance drop, then the
covariate is playing an important role within our model.

There are many variations on this sort of permutation test. They can be
simple but powerful tools for understanding the behaviour of all sorts
of model.

\subsection{Meta-modelling}\label{meta-modelling}

Meta-models are, as the name suggests, models of models and these can be
effective methods of providing localised explanations for complex
models.

The idea here is to look at a small region of the covariate space that
is covered by a complex and highly flexible model, such as a neural
network. We can't easily give a global explanation for this complex
model's behaviour - it is just too complicated.

However, we can interrogate the model's behaviour within a small region
and construct a simplified version of the model (a meta-model) that lets
us explain the model within that small region. Often this meta-model is
chosen as a linear model.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{images/402-production-explainability/local-linear-approximation.png}

}

\caption{\label{fig-function-approximation}A local linear approximation
in two dimensions}

\end{figure}%

This is partly for convenience and familiarity but also has a
theoretical backing: if our complex model is sufficiently smooth then we
can appeal to
\href{https://en.wikipedia.org/wiki/Taylor\%27s_theorem}{Taylor's
Theorem} to say that in a small enough region the mapping can be well
approximated by a linear function.

This sort of local model explanation is particularly useful where we
want to explain individual predictions or decisions made by our model:
for example why a single loan applicant was not granted a loan. By
inspecting the coefficients of this local surrogate model we can
identify which covariates were the most influential in the decision and
suggest how the applicant could increase their chance of success by
summarising those covariates that were both influential and are within
the ability of the applicant to change. This is exactly the approach
taken by the
\href{https://christophm.github.io/interpretable-ml-book/lime.html}{LIME}
methodology developed by Ribiero et al.~

\subsection{Aggregating Meta-models}\label{aggregating-meta-models}

Using local or conditional explanations of our model's behaviour can be
useful in some circumstances but they don't give a broader understanding
of what is going on overall. What if we want to know how a covariate
influences \emph{all} outcomes not a particular one? What if we care
about the covariate's expected effect over all loan applicants, or the
\emph{distribution} of effects over all applicants?

This is where local and conditional explanations are particularly nice.
By making these explanations at many points we can aggregate these
explanations to understand the global and marginal behaviour of our
models.

To aggregate our conditional effects into a marginal effect, or a local
effect into a global effect we must integrate these over the joint
distribution of all covariates. If this sounds a bit scary to you, then
you are right. Integration is hard enough at the best of times without
adding in the fact that we don't know the joint distribution of the
covariates we are using as predictors.

Don't worry though, we can take the easy way out and do both of these
things approximately. We can approximate the joint distribution of the
covariates by the empirical distribution that we observe in our sample,
and then our nasty integrals simplify to averages over the measurement
units in our data (the loan applicants in our case).

If we construct a local, conditional model for each loan applicant in
our data set, we can approximate the marginal effect of each covariate
by averaging the conditional effects we obtain for each loan applicant.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{images/402-production-explainability/local-neighbourhoods.png}

}

\caption{\label{fig-local-approximation}Local approximations around each
observation can be combined to understand global model behaviour.}

\end{figure}%

This gives us a global understanding of how each covariate influences
the response of our model. It does this over all possible values of the
other covariates and appropriately weights these according to their
frequency within the sample (and also within the population, if our
sample is representative).

\section{Wrapping Up}\label{wrapping-up-6}

We've seen that as our models become more flexible they also become more
difficult to explain, whether that is to ourselves, and subject expert
or a user of our model.

That's not to say that simple models are always easy to explain or
interpret, simple models can be deceptively tricky to communicate
accurately.

Finally, we looked at a couple of techniques for explaining more complex
models.

We can use permutation tests measure feature importance in our models:
shuffling the predictor values breaks any relationship to our response,
and we can measure how much this degrades our model performance. A big
dip implies that feature was doing a lot of explaining.

We can also look at the local behaviour of models by making surrogate or
meta models, that are interpretable, and aggregate these to understand
the model globally.

Effective explanations are essential if you want your model to be used
in production and to feed into real decisions decision making. This
requires some level of skill with rhetoric to tailor your explanation so
that it is clear to the person who requested it. But this isn't a soft
skill, it also requires a surprising amount of computational and
mathematical skill to extract such explanations from complex modern
models.

\chapter{Scalability}\label{production-scalability}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-important-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-important-color-frame, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section{Scalability and Production}\label{scalability-and-production}

When put into production code gets used more and on more data. We will
likely have to consider scalability of our methods in terms of

\begin{itemize}
\item
  Computation time
\item
  Memory requirements
\end{itemize}

When doing so we have to balance a trade-off between development costs
and usage costs.

\subsection{Example: Bayesian
Inference}\label{example-bayesian-inference}

\begin{itemize}
\item
  MCMC originally takes \textasciitilde24 hours
\item
  Identifying and amending bottlenecks in code reduced this to
  \textasciitilde24 minutes.
\end{itemize}

\emph{Is this actually better?} That will depend on a number of factors,
including:

\begin{itemize}
\tightlist
\item
  human hours invested
\item
  frequency of use
\item
  safe / stable / general / readable
\item
  trade for scalability
\end{itemize}

\subsection{Knowing when to worry}\label{knowing-when-to-worry}

Sub-optimal optimisation can be worse than doing nothing

\begin{quote}
\ldots{} programmers have spent far too much time worrying about
efficiency in \emph{the wrong places} and at \emph{the wrong times};
premature optimisation is the root of all evil (or at least most of it)
in programming. - Donald Knuth
\end{quote}

\begin{center}
\includegraphics[width=0.8\textwidth,height=\textheight]{images/403-production-scalability/pareto-frontier.png}
\end{center}

\subsection{Our Focus}\label{our-focus}

Writing code that scales well in terms of computaiton time or memory
used is a huge topic. In this section we restrict our aims to:

\begin{itemize}
\tightlist
\item
  Basic profiling to find bottlenecks.
\item
  Strategies for writing scalable (R) code.
\item
  Signpost advanced methods \& further reading.
\end{itemize}

\section{Basics of Code Profiling}\label{basics-of-code-profiling}

\subsection{R as a stopwatch}\label{r-as-a-stopwatch}

The simplest way to profile your code is to time how long it takes to
run. There are three common ways to do this.

Firstly, you could record the time before your code starts executing,
the time it completes and look at the difference of those.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_start }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\FunctionTok{Sys.sleep}\NormalTok{(}\FloatTok{0.5}\NormalTok{) }\CommentTok{\# YOUR CODE}
\NormalTok{t\_end }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}

\NormalTok{t\_end }\SpecialCharTok{{-}}\NormalTok{ t\_start}
\CommentTok{\#\textgreater{} Time difference of 0.5064578 secs}
\end{Highlighting}
\end{Shaded}

The system.time function provides a shorthand for this if your code runs
sequentially and extends the functionality to work for parallel code
too.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{system.time}\NormalTok{(}
  \FunctionTok{Sys.sleep}\NormalTok{(}\FloatTok{0.5}\NormalTok{)}
\NormalTok{)}
\CommentTok{\#\textgreater{}    user  system elapsed }
\CommentTok{\#\textgreater{}   0.001   0.000   0.504}
\end{Highlighting}
\end{Shaded}

The \texttt{\{tictoc\}} package has similar features, but also allows
you to add intermediate timers to more understand which parts of your
code are taking the most time to run.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tictoc)}

\FunctionTok{tic}\NormalTok{() }
\FunctionTok{Sys.sleep}\NormalTok{(}\FloatTok{0.5}\NormalTok{) }\CommentTok{\# YOUR CODE }
\FunctionTok{toc}\NormalTok{()}
\CommentTok{\#\textgreater{} 0.504 sec elapsed}
\end{Highlighting}
\end{Shaded}

With \texttt{\{tictoc\}} we can get fancy

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tic}\NormalTok{(}\StringTok{"total"}\NormalTok{)}
\FunctionTok{tic}\NormalTok{(}\StringTok{"first, easy part"}\NormalTok{)}
\FunctionTok{Sys.sleep}\NormalTok{(}\FloatTok{0.5}\NormalTok{)}
\FunctionTok{toc}\NormalTok{(}\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} first, easy part: 0.506 sec elapsed}
\FunctionTok{tic}\NormalTok{(}\StringTok{"second, hard part"}\NormalTok{)}
\FunctionTok{Sys.sleep}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\FunctionTok{toc}\NormalTok{(}\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} second, hard part: 3.008 sec elapsed}
\FunctionTok{toc}\NormalTok{()}
\CommentTok{\#\textgreater{} total: 3.517 sec elapsed}
\end{Highlighting}
\end{Shaded}

If your code is already very fast (but will be run \emph{very} many
times, so further efficiency gains are required) then the methods may
fail because they do not sample the state of the code at a high enough
frequency. In those cases you might want to explore the
\texttt{\{mircobenchmark\}} package.

\section{Profiling Your Code}\label{profiling-your-code}

To diagnose scaling issues you have to understand what your code is
doing.

\begin{itemize}
\item
  Stop the code at time \(\tau\) and examine the \emph{call-stack}.

  \begin{itemize}
  \tightlist
  \item
    The current function being evaluated, the function that called that,
    the function that called that, \ldots, top level function.
  \end{itemize}
\item
  Do this a lot and you can measure (estimate) the proportion of working
  memory (RAM) uses over time and the time spent evaluating each
  function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(profvis)}
\FunctionTok{library}\NormalTok{(bench)}
\end{Highlighting}
\end{Shaded}

\subsection{Profiling: Toy Example}\label{profiling-toy-example}

Suppose we have the following code in a file called
\texttt{prof-vis-example.R}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  profvis}\SpecialCharTok{::}\FunctionTok{pause}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{\}}

\NormalTok{g }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  profvis}\SpecialCharTok{::}\FunctionTok{pause}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  \FunctionTok{h}\NormalTok{()}
\NormalTok{\}}

\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  profvis}\SpecialCharTok{::}\FunctionTok{pause}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  \FunctionTok{g}\NormalTok{()}
\NormalTok{  profvis}\SpecialCharTok{::}\FunctionTok{pause}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  \FunctionTok{h}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Then the call stack for \texttt{f()} would look something like this.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/403-production-scalability/call-stack.png}

}

\caption{Callstack for f()}

\end{figure}%

We can examine the true call stack using the \texttt{profvis()} function
from the \texttt{\{profvis\}} package. By saving the code in a separate
file and sourcing it into our session, this function will also give us
line-by-line information about the time and memory demands of our code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"prof{-}vis{-}example.R"}\NormalTok{)}
\NormalTok{profvis}\SpecialCharTok{::}\FunctionTok{profvis}\NormalTok{(}\FunctionTok{f}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\textwidth,height=\textheight]{images/403-production-scalability/profiling-example-speed.png}
\end{center}

In both the upper histogram and the lower flame plot we can see that the
majority of time is being spent in \texttt{pause()} and \texttt{h()}.
What we have to be careful of here is that the upper plot shows the
total amount of time in each function call, so \texttt{h()} appears to
take longer than \texttt{g()}, but this is because it is called more
often in the code snippet we are profiling.

\section{Notes on Time Profiling}\label{notes-on-time-profiling}

We will get slightly different results each time you run the function

\begin{itemize}
\tightlist
\item
  Changes to internal state of computer
\item
  Usually not a big deal, mainly effects fastest parts of code
\item
  Be careful with stochastic simulations
\item
  Use \texttt{set.seed()} to make a fair comparison over many runs.
\end{itemize}

\subsection{Source code and compiled
functions}\label{source-code-and-compiled-functions}

If you write a function you can see the source of that function by
calling it's name

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pad\_with\_NAs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, n\_left, n\_right)\{}
  \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, n\_left), x, }\FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, n\_right))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pad\_with\_NAs}
\CommentTok{\#\textgreater{} function (x, n\_left, n\_right) }
\CommentTok{\#\textgreater{} \{}
\CommentTok{\#\textgreater{}     c(rep(NA, n\_left), x, rep(NA, n\_right))}
\CommentTok{\#\textgreater{} \}}
\end{Highlighting}
\end{Shaded}

This is equally true for functions within packages.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eds}\SpecialCharTok{::}\NormalTok{pad\_with\_NAs}
\CommentTok{\#\textgreater{} function (x, n\_left, n\_right) }
\CommentTok{\#\textgreater{} \{}
\CommentTok{\#\textgreater{}     stopifnot(n\_left \textgreater{}= 0)}
\CommentTok{\#\textgreater{}     stopifnot(n\_right \textgreater{}= 0)}
\CommentTok{\#\textgreater{}     stopifnot(class(x) \%in\% c("character", "complex", "integer", }
\CommentTok{\#\textgreater{}         "logical", "numeric", "factor"))}
\CommentTok{\#\textgreater{}     c(rep(NA, n\_left), x, rep(NA, n\_right))}
\CommentTok{\#\textgreater{} \}}
\CommentTok{\#\textgreater{} \textless{}bytecode: 0x7fba8a4a3c70\textgreater{}}
\CommentTok{\#\textgreater{} \textless{}environment: namespace:eds\textgreater{}}
\end{Highlighting}
\end{Shaded}

Some functions use \emph{compiled code} that is written in another
language. This is the case for dplyr's \texttt{arrange()}, which calls
some compiled C++ code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\NormalTok{arrange}
\CommentTok{\#\textgreater{} function (.data, ..., .by\_group = FALSE) }
\CommentTok{\#\textgreater{} \{}
\CommentTok{\#\textgreater{}     UseMethod("arrange")}
\CommentTok{\#\textgreater{} \}}
\CommentTok{\#\textgreater{} \textless{}bytecode: 0x7fba8b8689b0\textgreater{}}
\CommentTok{\#\textgreater{} \textless{}environment: namespace:dplyr\textgreater{}}
\end{Highlighting}
\end{Shaded}

It is also true for many functions from base R, for which there is (for
obvious reason) no R source code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean}
\CommentTok{\#\textgreater{} function (x, ...) }
\CommentTok{\#\textgreater{} UseMethod("mean")}
\CommentTok{\#\textgreater{} \textless{}bytecode: 0x7fba8274a9c8\textgreater{}}
\CommentTok{\#\textgreater{} \textless{}environment: namespace:base\textgreater{}}
\end{Highlighting}
\end{Shaded}

These compiled functions have no R source code, and the profiling
methods we have used here don't extend into compiled code. See
\href{https://github.com/r-prof/jointprof}{\{jointprof\}} if you really
need this profiling functionality.

\section{Memory Profiling}\label{memory-profiling}

\texttt{profvis()} can similarly measure the memory usage of your code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{integer}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FloatTok{1e4}\NormalTok{) \{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(x, i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}
\includegraphics[width=1\textwidth,height=\textheight]{images/403-production-scalability/profiling-example-memory.png}
\end{center}

\begin{itemize}
\tightlist
\item
  Copy-on-modify behaviour makes growing objects slow.\\
\item
  Pre-allocate storage where possible.
\item
  Strategies and structures, see
  \href{https://www.burns-stat.com/pages/Tutor/R_inferno.pdf}{R inferno}
  and
  \href{https://csgillespie.github.io/efficientR/performance.html}{Effecient
  R}.
\end{itemize}

\section{Tips to work at scale}\label{tips-to-work-at-scale}

\textbf{TL;DR:} pick your object types carefully, vectorise your code
and as a last resort implement your code in a faster language.

\subsection{Vectorise}\label{vectorise}

Two bits of code do the same task, but the second is much faster,
because it involves fewer function calls.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{11}\SpecialCharTok{:}\DecValTok{20} 
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\FunctionTok{length}\NormalTok{(x))}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(x)) \{}
\NormalTok{  z[i] }\OtherTok{\textless{}{-}}\NormalTok{ x[i] }\SpecialCharTok{*}\NormalTok{ y[i]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{11}\SpecialCharTok{:}\DecValTok{20} 
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{*}\NormalTok{ y}
\end{Highlighting}
\end{Shaded}

Where possible write and use functions to take advantage of vectorised
inputs. E.g.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Be careful of recycling!

\subsection{Linear Algebra}\label{linear-algebra}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{data =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}

\NormalTok{X }\SpecialCharTok{\%*\%}\NormalTok{ y}
\CommentTok{\#\textgreater{}      [,1]}
\CommentTok{\#\textgreater{} [1,]  2.0}
\CommentTok{\#\textgreater{} [2,]  0.5}
\end{Highlighting}
\end{Shaded}

More on vectorising:
\href{http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/}{Noam
Ross Blog Post}

\section{For loops in disguise}\label{for-loops-in-disguise}

\subsection{The apply family}\label{the-apply-family}

Functional programming equivalent of a for loop. {[}\texttt{apply()},
\texttt{mapply()}, \texttt{lapply()}, \ldots{]}

Apply a function to each element of a list-like object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{data =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{4}\NormalTok{)}
\NormalTok{A}
\CommentTok{\#\textgreater{}      [,1] [,2] [,3] [,4]}
\CommentTok{\#\textgreater{} [1,]    1    4    7   10}
\CommentTok{\#\textgreater{} [2,]    2    5    8   11}
\CommentTok{\#\textgreater{} [3,]    3    6    9   12}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MARGIN = 1 =\textgreater{} rows,  MARGIN = 2 =\textgreater{} columns}
\FunctionTok{apply}\NormalTok{(}\AttributeTok{X =}\NormalTok{ A, }\AttributeTok{MARGIN =} \DecValTok{1}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ sum)}
\CommentTok{\#\textgreater{} [1] 22 26 30}
\end{Highlighting}
\end{Shaded}

This generalises functions from \texttt{\{matrixStats\}}, where for some
special operations we can do all to the necessary calculation in C++.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rowSums}\NormalTok{(A)}
\CommentTok{\#\textgreater{} [1] 22 26 30}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{\{purrr\}}}{\{purrr\}}}\label{purrr}

Iterate over a single object with \texttt{map()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ mu, }\AttributeTok{.f =}\NormalTok{ rnorm, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1]  {-}9.687805  {-}8.371496  {-}8.729832 {-}11.120552 {-}10.334015}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1]  0.1534114 {-}1.3350648  0.7062538 {-}0.5633030 {-}1.1556981}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] 10.591297  9.342731 11.110070  8.143277 10.036150}
\end{Highlighting}
\end{Shaded}

Iterate over multiple objects \texttt{map2()} and \texttt{pmap()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{map2}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ mu, }\AttributeTok{.y =}\NormalTok{ sigma, }\AttributeTok{.f =}\NormalTok{ rnorm, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1] {-}10 {-}10 {-}10 {-}10 {-}10}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1]  0.03626844 {-}0.21129591 {-}0.15601918  0.05018852 {-}0.10746944}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] 10 10 10 10 10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{pmap}\NormalTok{(}
  \AttributeTok{.f =}\NormalTok{ rnorm, }
  \AttributeTok{n =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{.l =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{mean =}\NormalTok{ mu, }
    \AttributeTok{sd =}\NormalTok{ sigma))}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1] {-}10 {-}10 {-}10 {-}10 {-}10}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1] {-}0.140996058  0.097719568  0.107271314  0.009781948  0.023183190}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] 10 10 10 10 10}
\end{Highlighting}
\end{Shaded}

For more details and variants see Advanced R
\href{https://adv-r.hadley.nz/functionals.html}{chapters 9-11} on
functional programming.

\section{Easy parallelisation with
furrr}\label{easy-parallelisation-with-furrr}

\begin{itemize}
\item
  \texttt{\{parallel\}} and \texttt{\{futures\}} allow parallel coding
  over multiple cores.
\item
  Powerful, but steep learning curve.
\item
  \texttt{\{furrr\}} makes this very easy, just add \texttt{future\_} to
  purrr verbs.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{furrr}\SpecialCharTok{::}\FunctionTok{future\_map}\NormalTok{(}
  \AttributeTok{.x =}\NormalTok{ mu, }
  \AttributeTok{.f =}\NormalTok{ rnorm,}
  \AttributeTok{.options =}\NormalTok{ furrr}\SpecialCharTok{::}\FunctionTok{furrr\_options}\NormalTok{(}\AttributeTok{seed =} \ConstantTok{TRUE}\NormalTok{),}
  \AttributeTok{n =} \DecValTok{5}\NormalTok{) }
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1] {-}11.751294 {-}10.458282 {-}10.728360 {-}10.589310  {-}7.562764}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1] {-}0.02956282 {-}0.25632089  0.46127565 {-}1.51559833  0.38168009}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] 12.023730 11.941894  9.102389  8.674188 10.962068}
\end{Highlighting}
\end{Shaded}

This is, of course excessive for this small example!

One thing to be aware of is that we need to be very careful handling
random number generation in relation to parallelisation. There are many
options for how you might want to set this up, see
\href{https://www.r-bloggers.com/2020/09/future-1-19-1-making-sure-proper-random-numbers-are-produced-in-parallel-processing/}{R-bloggers}
for more details.

\section{Sometimes R doesn't cut it}\label{sometimes-r-doesnt-cut-it}

\includegraphics{images/403-production-scalability/Rccp.png}

RCPP: An API for running C++ code in R. Useful when you need:

\begin{itemize}
\tightlist
\item
  loops to be run in order
\item
  lots of function calls (e.g.~deep recursion)
\item
  optimised data structures
\end{itemize}

Rewriting R code in C++ and other low-level programming languages is
beyond our scope, but good to know exists. Starting point: Advanced R
\href{https://adv-r.hadley.nz/rcpp.html}{Chapter 25}.

\section{Wrapping up}\label{wrapping-up-7}

\subsection*{Summary}\label{summary-2}
\addcontentsline{toc}{subsection}{Summary}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick you battles wisely
\item
  Target your energy with profiling
\item
  Scale loops with vectors
\item
  Scale loops in parallel processing
\item
  Scale in another language
\end{enumerate}

\subsection*{Help!}\label{help}
\addcontentsline{toc}{subsection}{Help!}

\begin{itemize}
\tightlist
\item
  Articles and blog links
\item
  The R inferno
  \href{https://www.burns-stat.com/pages/Tutor/R_inferno.pdf}{(Circles
  2-4)}
\item
  Advanced R \href{https://adv-r.hadley.nz/techniques.html}{(Chapters
  23-25)},
\item
  Efficient R
  \href{https://csgillespie.github.io/efficientR/performance.html\#prerequisites-6}{(Chapter
  7)}.
\end{itemize}

\chapter*{Checklist}\label{production-checklist}
\addcontentsline{toc}{chapter}{Checklist}

\markboth{Checklist}{Checklist}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-important-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-important-color-frame, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

\section*{Videos / Chapters}\label{videos-chapters-3}
\addcontentsline{toc}{section}{Videos / Chapters}

\markright{Videos / Chapters}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f48d43b4-b370-4cfb-a438-af9e00bf79b5}{Reproducibility}
  (26 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/04-01-reproducibility/04-01-reproducibility.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f2c64757-faea-470f-9dfc-af9e00ba4929}{Explainability}
  (16 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/04-02-explainability/04-02-explainability.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=5305fbb1-8dc9-4232-82d0-afa00187f942}{Scalability}
  (30 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/04-03-scalability/04-03-scalability.pdf}{{[}slides{]}}
\end{itemize}

\section*{Reading}\label{reading-3}
\addcontentsline{toc}{section}{Reading}

\markright{Reading}

Use the \hyperref[production-reading]{Preparing for Production} section
of the reading list to support and guide your exploration of this week's
topics. Note that these texts are divided into core reading, reference
materials and materials of interest.

\section*{Activities}\label{activities-1}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

This week has fewer activities, since you will be working on the first
assessment.

\emph{Core}

\begin{itemize}
\item
  Read the LIME paper, which we will discuss during the live session.
\item
  Work through the
  \href{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}{understanding
  LIME R tutorial}
\item
  Use code profiling tools to assess the performance of your
  \texttt{rolling\_mean()} and \texttt{rolling\_sd()} functions.
  Identify any efficiencies that can be made.
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\tightlist
\item
  Write two functions to simulate a
  \href{https://en.wikipedia.org/wiki/Poisson_point_process\#Homogeneous_case_2}{homogeneous
  Poisson process} with intensity \(\lambda >0\) on the interval
  \((t_1, t_2) \subset \mathbb{R}\). The first should use the
  exponential distribution of inter-event times to simulate events in
  sequence. The second should use the Poisson distribution of the total
  event count to first simulate the number of events and then randomly
  allocate locations over the interval. Evaluate and compare the
  reproducibility and scalability of each implementation.
\end{itemize}

\section*{Live Session}\label{live-session-3}
\addcontentsline{toc}{section}{Live Session}

\markright{Live Session}

In the live session we will begin with a discussion of this week's
tasks. We will then break into small groups for a reading group style
discussion of the LIME paper that was set as reading for this week.

\part{Data Science Ethics}

\part{Introduction}

When carrying out a data science project, ethical concerns can arise at
many stages.

This week we focus on the two most common, privacy and fairness. We
explore case studies where these ethical principles have been violated
in the past and investigate how technical measures and professional
guidelines can help to prevent such events from occurring again in the
future.

\chapter{Privacy}\label{ethics-privacy}

\section{Privacy and Data Science}\label{privacy-and-data-science}

Data privacy is all about keeping people's personal information
confidential and secure. It's the idea that people have control over
their personal data, and that companies and organizations are
responsible for protecting it. Personal data can include things like
names, addresses, phone numbers, email addresses, medical information,
and even online activity.

\begin{quote}
What is personal data?

\begin{itemize}
\tightlist
\item
  Name, national insurance number, passport number
\item
  Contact details: address, phone number, email address
\item
  Medical history
\item
  Online activity, GPS data, finger print or face-ID,
\end{itemize}

Should not be collected, analysed or distributed without consent.
\end{quote}

Under the concept of data privacy, individuals have the right to know
what information is being collected about them, how it's being used, who
it's being shared with, and to have the ability to control that
information. Companies and organizations, on the other hand, have a
responsibility to keep this information secure and to use it only for
the purpose it was intended.

It's important to remember that with the growing use of machine learning
and data science methods, personal information is being collected,
stored, and shared more than ever before. This makes data privacy a
critical issue to our work as data scientists. By ensuring that personal
information is handled responsibly and with respect for people's
privacy, we can build trust and confidence in the digital world. In this
section of the course I hope to introduce you to some key ideas around
data privacy and use some case studies to demonstrate that privacy is
not an easy thing to ensure.

\section{Privacy as a Human Right}\label{privacy-as-a-human-right}

\subsection{Article 12 of the Universal Declaration of Human
Rights}\label{article-12-of-the-universal-declaration-of-human-rights}

\begin{quote}
No one shall be subjected to arbitrary interference with his privacy,
family, home or correspondence, nor to attacks upon his honour and
reputation. Everyone has the right to the protection of the law against
such interference or attacks. -
\href{https://www.un.org/en/about-us/universal-declaration-of-human-rights}{UN
General Assembly}, 1948.
\end{quote}

The idea of a right to privacy is not a new one. Article 12 of the
Universal Declaration of Human Rights, written in 1948, states that
everyone has the right to privacy in their personal and family life,
home and correspondence (this includes communication via the post,
telephone, and email).

This means that everyone has the right to keep their personal
information, private and protected from being disclosed to others
without their consent. This right to privacy is essential for protecting
an individual's autonomy, dignity, and freedom.

The Universal Declaration of Human Rights it is often considered as a
benchmark for human rights and many countries have incorporated its
principles into their own laws and regulations. This means that in many
countries the right to privacy is legally protected and people have the
right to take action if their privacy is violated.

This means we have to take particular care in our work as data
scientists when handling any information personal information, whether
that is at the individual level or in aggregate.

\section{Data Privacy and the European
Union}\label{data-privacy-and-the-european-union}

\subsection{General Date Protection Regulation
(2018)}\label{general-date-protection-regulation-2018}

\begin{quote}
`Consent' of the data subject means any freely given, specific, informed
and unambiguous indication of the data subject's wishes by which he or
she, by a statement or by a clear affirmative action, signifies
agreement to the processing of personal data relating to him or her; -
GDPR \href{https://gdpr-info.eu/art-4-gdpr/}{Article 4}
\end{quote}

A more recent set of regulations relating to the use of personal data is
the General Data Protection Regulation (GDPR). This is a comprehensive
data privacy regulation that went into effect on May 2018 within the
European Union. The purpose of the GDPR is to give individuals more
control over their personal information and how it's used, and to unify
data protection laws across the EU.

The GDPR is an extensive legal document that lays down strict rules for
how companies and organizations must handle the personal data of EU
citizens, \emph{regardless of where that data is stored or processed}.

Some key provisions of the GDPR include the requirement to obtain
explicit, informed and active consent from individuals before the
collection and processing of their personal data. This is precisely why
banner notifications about cookies on websites became ubiquitous,

GDPR also establishes the right for individuals to request access to, or
deletion of, their personal data. Furthermore, it states that in the
event of a data breach (where unauthorised access or use of personal
data occurs) the data holder must inform the affected individuals and
relevant authorities within 72 hours.

\section{Privacy: Key Terms}\label{privacy-key-terms}

Measuring privacy within a dataset is a complex task that involves
assessing the degree to which personal information is protected from
unauthorized access, use, or disclosure. There are many ways of
measuring and increasing the degree of privacy within a data set. To
understand these, and the literature on data privacy, it helps to be
familiar with a few key terms.

\begin{quote}
\emph{Pseudonmisation}: processing data so that it does not relate to an
identifiable person.
\end{quote}

A data entry is pseudonymised when it has been processed in a way that
it does not relate to an identifiable person. The key word here is
identifiable. Replacing your name with your CID would be considered a
form of pseudonymisation, because based on that information alone you
cannot be identified.

\begin{quote}
\emph{Re-identification}: relating a pseudonymised data entry to an
identifiable person.
\end{quote}

Re-identification is the act of relating a pseudonymised data entry to
an identifiable person. Re-identification makes something about that
person known that wasn't known beforehand, perhaps by using processing
techniques or cross-referencing against external information. In the
previous grading scenario, re-identification occurs after marking so
that your grades can be returned to you.

\begin{quote}
\emph{Anonymisation}: A pseudonmisation method that precludes
re-identification.
\end{quote}

Anonymisation is a word that in casual usage is often conflated with our
previous definition of pseudonymisation. In the technical sense,
anonymisation is any form of pseudonymisation that precludes the
possibility of re-identification.

We don't want to anonymise test scores, or we would not be able to map
them back to individual students. However, if grades were to be
published online then we would want to ensure that this is only done in
an anonymised format.

\section{Measuring Privacy}\label{measuring-privacy}

\subsection{\texorpdfstring{Pseudo-identifiers and
\(k\)-anonymity}{Pseudo-identifiers and k-anonymity}}\label{pseudo-identifiers-and-k-anonymity}

\begin{quote}
\emph{Pseudo-identifiers}: Attributes that can also be observed in
public data. For example, someone's name, job title, zip code, or email.
\end{quote}

\begin{quote}
For the set of quasi-identifiers \(A_1, \ldots ,A_p\), a table is
\emph{\(k\)-anonymous} if each possible value assignment to these
variables \((a_1, . . . , a_n)\) is observed for either 0 or at least
\(k\) individuals.
\end{quote}

\(k\)-anonymity is a technique that ensures each combination of
attributes within a table, is shared by at least k records within that
dataset. This ensures that each individual's data can't be distinguished
from at least \(k-1\) other individuals.

This provides some rudimentary level of privacy, where \(k\) corresponds
to the size of smallest equivalence class of pseudo-identifiers within
the data. Therefore larger values of \(k\) correspond to greater levels
of privacy.

To see this more concretely, let's take a look at an example.

\subsection{\texorpdfstring{\(k\)-anonymity
example}{k-anonymity example}}\label{k-anonymity-example}

In this example we have a dataset recording visits to a sexual health
clinic. We wish to establish the \(k\)-anonymity of this data set, where
the diagnosed condition should be kept private. To do help with this,
the drug use status has been removed for all patients and only partial
information is available about their postcode and age.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
& Post Code & Age & Drug Use & \textbf{Condition} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & OX1**** & \textless20 & * & \textbf{Herpes} \\
2 & OX1**** & \textless20 & * & \textbf{Herpes} \\
3 & OX2**** & \textgreater=30 & * & \textbf{Chlamydia} \\
4 & OX2**** & \textgreater=30 & * & \textbf{Herpes} \\
5 & OX1**** & \textless20 & * & \textbf{Gonorrhoea} \\
6 & OX2**** & \textgreater=30 & * & \textbf{Gonorrhoea} \\
7 & OX1**** & \textless20 & * & \textbf{Gonorrhoea} \\
8 & LA1**** & 2* & * & \textbf{Chlamydia} \\
9 & LA1**** & 2* & * & \textbf{Chlamydia} \\
10 & OX2**** & \textgreater=30 & * & \textbf{Gonorrhoea} \\
11 & LA1**** & 2* & * & \textbf{Chlamydia} \\
12 & LA1**** & 2* & * & \textbf{Chlamydia} \\
\end{longtable}

By grouping observations by each distinct combination of
pseudo-identifiers we can establish the equivalence classes within the
data.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0588}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1618}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0882}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3088}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Post Code
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Drug Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Condition}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Equivalence Class}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & OX1**** & \textless20 & * & \textbf{Herpes} & \emph{1} \\
2 & OX1**** & \textless20 & * & \textbf{Herpes} & \emph{1} \\
3 & OX2**** & \textgreater=30 & * & \textbf{Chlamydia} & \emph{2} \\
4 & OX2**** & \textgreater=30 & * & \textbf{Herpes} & \emph{2} \\
5 & OX1**** & \textless20 & * & \textbf{Gonorrhoea} & \emph{1} \\
6 & OX2**** & \textgreater=30 & * & \textbf{Gonorrhoea} & \emph{2} \\
7 & OX1**** & \textless20 & * & \textbf{Gonorrhoea} & \emph{1} \\
8 & LA1**** & 2* & * & \textbf{Chlamydia} & \emph{3} \\
9 & LA1**** & 2* & * & \textbf{Chlamydia} & \emph{3} \\
10 & OX2**** & \textgreater=30 & * & \textbf{Gonorrhoea} & \emph{2} \\
11 & LA1**** & 2* & * & \textbf{Chlamydia} & \emph{3} \\
12 & LA1**** & 2* & * & \textbf{Chlamydia} & \emph{3} \\
\end{longtable}

Here we have three distinct equivalence classes, each with four
observations. Therefore the smallest equivalence class is also of size
four and this data set is 4-anonymous.

While we can easily identify the equivalence classes in this small
dataset, doing so in large datasets is a non-trivial task.

\section{Improving Privacy}\label{improving-privacy}

There are three main ways that you can improve the level of privacy
within your data, and we have seen examples of two of these already.

\textbf{Redaction} may be applied to individual or to an attribute,
leading to a whole row or column being censored. This is quite an
extreme approach: it can lead to a large amount of information being
removed from the data set. However, sometimes redacting a full row is
necessary; for example when that row contains identifying information
like a person's name or national insurance number. An additional concern
when redacting \emph{rows} from your data is that it will artificially
alter the distribution of your sample, making it unrepresentative of the
population values.

\textbf{Aggregation} or coarsening is a second approach where the level
of anonymity can be increased by binning continuous variables into
discrete ranges or by combining categories within a variable that
already takes discrete values. The idea here is to reduce the number of
equivalence classes within the quasi-identifiers so that the level of
k-anonymity is increased.

A similar approach is to corrupt or obfuscate the observed data by
\textbf{adding noise to the observations}, or permuting some portion of
them. The aim is to retain overall patterns but ensure individual
recorded values no longer correspond to an individual in the raw data
set. The difficulty here is in setting the type and amount of noise to
be added to the data to grant sufficient privacy without removing all
information from the dataset.

This trade-off between information loss and privacy is a common theme
throughout all of these methods.

\section{\texorpdfstring{Breaking
\(k\)-anonymity}{Breaking k-anonymity}}\label{breaking-k-anonymity}

\(k\)-anonymity ensures that there are at least \(k-1\) other people
with your particular combination of pseudo-identifiers. What it does not
do is ensure that there is any variation within a particular group. The
dataset on sexual health we just saw was 4-anonymous, but if we know a
person how attended the clinic was from a Lancashire (LA) postcode (and
in their 20s) then we know for certain that they have Chlamydia. An
alternative privacy measure called \(l\)-diversity tries to address this
issue.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
& Post Code & Age & Drug Use & Condition \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
8 & LA1**** & 2* & * & \emph{Chlamydia} \\
9 & LA1**** & 2* & * & \emph{Chlamydia} \\
11 & LA1**** & 2* & * & \emph{Chlamydia} \\
12 & LA1**** & 2* & * & \emph{Chlamydia} \\
\end{longtable}

A second problem with \(k\)-anonymity is that this type of privacy
measure is focused entirely on the data available within this dataset.
It does not take into account data that might be available elsewhere or
might become publicly available in the future. An \textbf{external
data-linkage attack} can cross-reference this table against other
information to reduce the size of equivalence classes and reveal
personal information.

\section{Cautionary tales}\label{cautionary-tales}

\subsection{Massachussets Medical
Data}\label{massachussets-medical-data}

Medical research is often slow because it is very difficult to share
medical records while maintaining patients' privacy. In the 1990s a
government agency in Massachusetts wanted to improve this by releasing a
dataset summarising the hospital visits made by all state employees.
They were understandably quite careful about this, making this
information available only to academic researchers and redacted all
information like names, addresses and security numbers. They did include
the the patient's date of birth, zip code, and sex - this information
was deemed sufficiently general while allowing difference in healthcare
provision to be investigated.

\begin{figure}[H]

{\centering \includegraphics{images/501-ethics-privacy/latanya-sweeney.jpg}

}

\caption{Latanya Sweeney Speaking in New York, 2017. Image CC-4.0 from
\href{https://en.wikipedia.org/wiki/User:ParkerHiggins}{Parker
Higgins}.}

\end{figure}%

Latanya Sweeney is now a pre-eminent researcher in the field of data
privacy. In the 1990s she was studying for a PhD and MIT and wanted to
demonstrate the potential risks of de-anonymising this sort of data. To
demonstrate her point she chose to focus on the public records of
Massachusetts' governor, William Weld. For a small fee, Sweeney was able
to obtain the voter registration records for the area in which the
governor lived. By cross-referencing the two datasets Sweeney was able
to uniquely identify the governors medical history and send them to him
in the post. This was particularly embarrassing for Weld. since he had
previously given a statement reassuring the public that this data
release would not compromise the privacy of public servants.

This interaction between Sweeney and the Governor of Massachusetts was
significant because it highlighted the potential privacy risks
associated with the release of publicly available information. It
demonstrated that even when data is stripped of names and other
identifying information, it can still be possible to re-identify
individuals and potentially access sensitive information. The problem
here only grows with the dimension of the dataset - the more
characteristics that are measures the greater the chance of one person
having a unique combination of those.

\subsection{Neflix Competition}\label{neflix-competition}

\begin{figure}[H]

{\centering \includegraphics{images/501-ethics-privacy/netflix-logo-old.png}

}

\caption{Netflix logo 2001-2014. Public domain image.}

\end{figure}%

In 2006 Netflix announced a public competition to improve the
performance of their recommender system with a pros of 1 million USD to
the first person or group to improve its performance by at least 10\%.
For this competition over 100 million ratings from 480,000 users were
made public. Each entry contained a pseudonymised user ID, a film id,
the rating out of 5 stars and the date that rating was given.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
\textbf{User ID} & \textbf{Film ID} & \textbf{Rating} & \textbf{Date} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
000001 & 548782 & 5 & 2001-01-01 \\
000001 & 549325 & 1 & 2001-01-01 \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
\end{longtable}

Although the Netflix team had pseudonymised the data (and taken other
steps like adding noise to the observations), two researchers at the
University of Texas were able to successfully re-identify a 96\% of
individuals within the data. They did this by cross reference the
competition dataset against reviews on the openly available internet
movie database (IMDb), working on the supposition that users would rate
films on both services at approximately the same time - the 96\% figure
uses a margin of 3 days.

The researchers went further, showing that if we were to alter the data
to achieve even a modest 2-anonymity then almost all of the useful
information would be removed from competition data set.

This example should show how difficult it can be to ensure individual
privacy in the face of unknown external data sources. It might seem like
a trivial example compared to medical records but the media that you
consume, and in particular how you rate that media, cam reveal you
religious beliefs, your political stance or your sexual orientation.
These are protected characteristics that you might not want to broadcast
freely.

It might not be important if you, or even the average Netflix user, if
that information becomes public. What is important is whether any user
would find this privacy breach objectionable and potentially come to
harm because of it.

\section{Wrapping Up}\label{wrapping-up-8}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Privacy is a fundamental concern.
\item
  Privacy is hard to measure and hard to ensure.
\item
  Also a model issue, since models are trained on data.
\item
  No universal answers, but an exciting area of ongoing research.
\end{itemize}
\end{quote}

Although we have only scratched the surface of privacy in data science,
we will have to wrap this video up here.

We have seen that privacy should be a fundamental concern when working
with any from of human-related data. This is chiefly because we aren't
in a position to determine what types of privacy breach might
significantly and negatively impact the lives of the people we hold data
about.

We have seen through one example metric that privacy can be both
difficult to measure and even more difficult to preserve. This is not
only an issue when releasing data into the world, but also when
publishing models trained on this data. Approaches analogous to those
used by Latanya Sweeney's can be used be used by bad-actors to identify
with high precision whether a given individual was included within the
data that was used to train a published model.

There are no universal answers to the question of privacy in data
science, this is what makes it an exciting area of ongoing research. It
is also for this reason that a lot of Sweeney's work was published only
after overcoming great resistance: it exposed systematic vulnerabilities
for which there are currently few reliable solutions.

\chapter{Fairness}\label{ethics-fairness}

\section{Fairness and the Data
Revolution}\label{fairness-and-the-data-revolution}

\includegraphics{images/502-ethics-fairness/90s-windows-wallpaper.jpg}

Before the 1990s, large datasets were typically only collected to
understand huge, complex systems. These systems might be the weather,
public infrastructure (e.g.~hospitals, roads or train networks), the
stock market or even populations of people.

Collecting high quality data on these systems was immensely expensive
but paid dividends by allowing us to describe the expected behaviour of
these systems at an aggregate level. Using this sort of information, we
can't make journeys or healthcare better at an individual level but we
\emph{can} make changes to try and make these experiences better on
average.

Things changed with the widespread adoption of the internet in the
mid-1990s and the subsequent surge in data collection, sharing and
processing. Suddenly, we as individuals shifted from being just one part
of these huge processes to being a complex process worth of modelling
all on our own.

\begin{figure}

\centering{

\includegraphics{502-ethics-fairness_files/figure-pdf/fig-global-data-barplot-1.pdf}

}

\caption{\label{fig-global-data-barplot}Volume of data created,
captured, copied, and consumed worldwide from 2010 to 2022.}

\end{figure}%

It was at this point that focus shifted toward making individual,
personalised predictions for specific people, based on the vast amounts
of data that we generate as we go about our daily lives.

This shift from aggregate to individual behaviour creates the
opportunity not only for these predictions to systematically harm groups
of people, as they always could, but also to acutely harm individuals.

\section{You are Your Data}\label{you-are-your-data}

The blunt truth is that, as far as a data science model is concerned,
you are nothing more than a point in a high-dimensional predictor space.

\begin{verbatim}
Warning in geom_text(aes(label = "You are just a point in predictor space", : All aesthetics have length 1, but the data has 126 rows.
i Please consider using `annotate()` or provide this layer with data containing
  a single row.
\end{verbatim}

\begin{center}
\includegraphics{502-ethics-fairness_files/figure-pdf/you-are-your-data-plot-1.pdf}
\end{center}

The model might use your location in that space to group you with other
points that are in some sense ``nearby''. Alternatively, the model might
estimate some information about you that it currently doesn't have,
based on what it knows about those surrounding points. These other
points also represent unique humans with rich and fascinating lives -
but the model doesn't care about that, it is just there to group some
points or predict some values.

\begin{center}
\includegraphics{502-ethics-fairness_files/figure-pdf/classification-plot-1.pdf}
\end{center}

The idea of fairness comes into data science when we begin to ask
ourselves which predictors we should provide the model with when
carrying out these tasks. We aren't asking this from a model selection
stand-point. We are asking what are morally permissible predictors, not
what leads to a significant improvement in model fit.

\section{Forbidden Predictors}\label{forbidden-predictors}

The argument about what features of a human being can be used to make
decisions about them started well before the 1990s. The most contentious
of these arguments centre around the inclusion of characteristics that
are either immutable or not easily changed. Some of these
characteristics including race, gender, age or religion receive legal
protections. These protected attributes are often forbidden to be used
in important decisions, such as whether a bank loan is accepted.

This natural lead us to ask what classifies as an important decision?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\href{https://www.gov.uk/discrimination-your-rights}{Protected
Characteristics} under the
\href{https://www.legislation.gov.uk/ukpga/2010/15/contents}{Equality
Act (2010)}

\begin{itemize}
\tightlist
\item
  age
\item
  gender reassignment
\item
  marriage / civil partnership
\item
  pregnancy / parental leave
\item
  disability
\end{itemize}

\begin{itemize}
\tightlist
\item
  race including colour, nationality, ethnic or national origin
\item
  religion or belief
\item
  sex
\item
  sexual orientation
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

We also need to be careful if these protected attributes actually have
strong predictive power and would improve our predictions (potentially
to the benefit of the groups that are being protected by these
regulations). Just because a protected attribute isn't used directly
within a model that doesn't mean the model will not discriminate
according to that attribute.

If we have multiple predictors within a model, then withholding a
protected attribute does not make the model ignorant of that attribute.
If you have access to someone's browsing history, where they live and
some of their recent purchases you can probably make a fairly accurate
profile of that person, including many of these supposedly protected
attributes. In the same way, a model can use or combine attributes that
are not legally protected to create a new variable that acts as an
accurate proxy for the protected characteristic.

And why wouldn't our model do this? When using a standard loss function
we have literally asked it to get the best possible predictive
performance. If a protected attribute has predictive power then the
model is likely to approximate it using the predictors that are
available to it.

Before we see how to handle this concern, let's step back and consider
how we can quantify and measure fairness in a model.

\section{Measuring Fairness}\label{measuring-fairness}

Converting the concept of fairness into a mathematical statement is a
very difficult task. This is partly because moving from natural language
to precise formalism is hard, but it's also because the term fairness
means different things to different people in different contexts.
Because of this, there are many complementary definitions of fairness
that all try to capture some intuitive notion of a fair model. However,
these measures all capture different facets of this complicated concept.
Despite this, these measures vary to such an extent they can't all be
satisfied simultaneously.

I'll introduce four such measures shortly, focusing in on the case of
binary outcomes where a ``positive'' response of 1 corresponds to an
event that would be considered favourably when taken in context. For
example this might be a loan that will be successfully repaid or that a
person released on bail will not re-offend.

\begin{itemize}
\tightlist
\item
  Binary outcome \(Y \in \{0,1\}\).
\end{itemize}

We'll consider the simple case where a binary prediction is made in each
instance, and where we want our predictions to be fair across the \(k\)
distinct levels of some protected attribute \(A\).

\begin{itemize}
\tightlist
\item
  Binary Prediction \(\hat Y \in \{0,1\}\).
\item
  Protected attribute \(A\) takes values in
  \(\mathcal{A} = \{a_1, \ldots, a_k\}\).
\end{itemize}

\subsection{Demographic Parity}\label{demographic-parity}

The first, and potentially most obvious fairness definition is that of
demographic parity. Here a model is deemed fair if, across all subgroups
of the protected attribute, the probability of predicting a successful
outcome is equal.

\begin{quote}
The probability of predicting a `positive' outcome is the same for all
groups.
\end{quote}

\[\mathbb{P}(\hat Y = 1 | A = a_i) = \mathbb{P}( \hat Y = 1 | A = a_j), \  \text{ for all }\  i,j \in \mathcal{A}.\]

An obvious shortcoming demographic parity is that it does not allow us
to account for the fact that a positive outcome might not be equally
likely in each of these subgroups. In this way demographic parity is
analogous to treating people equally, rather than equitably.

\subsection{Equal Opportunity}\label{equal-opportunity}

Equality of opportunity addresses this shortcoming by conditioning on a
truly positive outcome. Equality of opportunity states that of those who
are ``worthy'' of a loan (in some sense), all subgroups of the protected
characteristic should be treated equally.

\begin{quote}
Among those who have a true `positive' outcome, the probability of
predicting a `positive' outcome is the same for all groups.
\end{quote}

\[\mathbb{P}(\hat Y = 1 | A = a_i, Y =1) = \mathbb{P}( \hat Y = 1 | A = a_j, Y=1), \  \text{ for all }\  i,j \in \mathcal{A}.\]

\subsection{Equal Odds}\label{equal-odds}

Of course, you have encountered two-way tables, type-I and type-II
errors. Equally important as granting loans to people who will repay
them is to deny loans to those who cannot afford them.

A model satisfying the equal odds condition can identify true positives
and false negatives equally well across all sub-groups of the protected
characteristic.

\begin{quote}
Among those who have a true `positive' outcome, the probability of
predicting a `positive' outcome is the same for all groups.

\emph{AND}

Among those who have a true `negative' outcome, the probability of
predicting a `negative' outcome is the same for all groups.
\end{quote}

\[\mathbb{P}(\hat Y = y | A = a_i, Y =y) = \mathbb{P}( \hat Y = y | A = a_j, Y=y), \ \text{ for all } \ y \in \{0,1\} \ \text{ and } \  i,j \in \mathcal{A}.\]

\subsection{Predictive Parity}\label{predictive-parity}

All of the measures we have considered so far consider the probability
of a prediction given the true credit-worthiness of an applicant.
Predictive Parity reverses the order of conditioning (as compared to
equal opportunity).

It ensures that among those predicted to have a successful outcome, the
probability of a truly successful outcome should be the same for all
subgroups of the protected characteristic. This ensures that, in our
financial example, the bank is spreading its risk exposure equally
across all subgroups; each subgroup should have an approximately equal
proportion of approved loans being successfully repaid.

\begin{quote}
The probability of a true `positive' outcome for people who were
predicted a `positive' outcome is equal across groups.
\end{quote}

\[\mathbb{P}(Y = 1 | \hat Y = 1, A = a_i) = \mathbb{P}(Y_1 = 1 | \hat Y = 1, A = a_j) \ \text{ for all } \  i,j \in \mathcal{A}.\]

We can play devil's advocate here and say that this might not be
appropriate if there is a genuine difference in the probability of
successful repayment between groups.

\section{Metric Madness}\label{metric-madness}

Even with this very simple binary classification problem that there are
many ways we can interpret the term fairness. Which, if any, of these
will be appropriate is going to be highly context dependent.

An issue with many of these metrics, including some of those introduced,
is that they require knowledge of the true outcome. This means that
these metrics can only be evaluated retrospectively: if we knew this
information to begin with then we wouldn't need a model to decide who
get a loan. On top of this, it means that we only ever get information
about the loans that are granted - we don't have access to the counter
factual outcome of whether a loan that was not granted would have been
repaid.

An additional problem is that evaluating these fairness metrics requires
us to know which protected sub-group each individual belongs to. This is
clearly a problem: to evaluate the fairness of our loan applications we
need to know sensitive information about the applicants, who would -
very reasonably - be unwilling to provide that information because it
legally cannot be used to inform the decision making process. For this
reason, an independent third-party is often required to assess fairness
by collating data from the applicants and the bank.

A third complication here is that these definitions deal in strict
equalities. In any given sample, these are almost surely not going to be
satisfied even if the metric is truly satisfied. A formal statistical
test should be used to assess whether these differences are consistent
with a truly fair model, however the more common approach is for
regulators to set some acceptable tolerance on the discrepancy in metric
values between sub-groups.

Finally, it is worth noting that all of these problems arise for a
simple binary classifier but most models are far more complicated than
this. Even working with these conditional probability statements
requires careful attention, but things get much trickier when the
response or sensitive attribute are continuous valued or when other,
non-sensitive predictors are also included in the model.

\section{Modelling Fairly}\label{modelling-fairly}

\subsection{Fairness Aware Loss
Functions}\label{fairness-aware-loss-functions}

Now we have some methods to detect and quantify the fairness of our
models, how do we incorporate that into the model fitting process?

\includegraphics{images/502-ethics-fairness/fairness-error-pareto-front.png}

We now have multiple objectives: to predict our outcome as accurately as
possible while also treating people as fairly as possible (by which ever
fairness metric or combination of metrics we care to consider).
Unfortunately, these things are generally in competition with each
other. There is no one best model but rather a family of best models,
from which we have to pick a single model to implement.

Can resolve this issue by linearisation, create our loss function as a
linear weighted sum of the two component objectives. This simplifies the
problem mathematically, but actually just shift the problem rather than
resolving it. Up to scaling constant, each combination of weights
corresponds to a unique point on the Pareto frontier, so we have just
translated our problem from picking a point on the frontier to picking a
pair of weights.

To do actually resolve this issue we need to define our relative
preference between fairness and predictive power. When I say ``our
preference'', what I actually mean that of the company or organisation
for whom we are doing an analysis - not our own personal view. Eliciting
this preference and communicating the idea of an optimal frontier can be
tricky. One solution is to present a small set of possible models, which
represent a range of preferences between the two competing objectives,
and ask the stakeholder to choose between these.

\subsection{Other Approaches}\label{other-approaches}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Re-weighting or resampling to better represent minority groups.
\item
  Forgetting factor to reduce impact of bias in historical data.
\item
  Meta-modelling to intervene in harmful feedback loops.
\end{itemize}
\end{quote}

Whenever we treating all predictions equally and our loss function
optimises purely for predictive performance, good predictions for
minority groups will never be prioritised. One strategy to correct for
this is to either re-weighting or re-sample each observation so that
minority groups are given greater importance within the loss function.

A lot of the problems of fairness that we see are because our models are
replicating what happens or used to happen in reality. In some cases,
this is being better addressed now and a model can be made more fair by
down-weighting training data as it ages. This allows our model to more
quickly adapt to changes in the system it is trying to represent.

In other cases the use of historically biased data to train models that
are put into production has lead to a feedback loop that makes more
recent data even more unjust. One example of this, we can consider
racial disparity in the interest rates offered on mortgages. Suppose
that one racial group of applicants was in the past slightly more likely
to default on loans, perhaps due to historical pay inequity. This means
that models would likely suggest higher interest loans to this group, in
an attempt to offset the bank's exposure to the risk of non-repayment.

This not only reduces the number of loans that will be granted to that
racial group but it \emph{also} makes the loans that are granted more
difficult to repay and more likely to be defaulted on. This in turn
leads to another increase in the offered interest rate, driving down the
number of loans approved and pushing up the chance of non-repayment even
further.

The decisions made using this model are impacting its future training
data and creating a harmful and self-reinforcing feedback loop.
Historical down weighting will do nothing to address this sort of issue,
which requires active intervention.

A meta-modelling approach is possible type of intervention. Here
post-hoc methods used to estimate the biases within a fitted model and
these estimates are used to explicitly correct for historical biases,
\emph{before} the model is used to make predictions or decisions.

\section{Wrapping Up}\label{wrapping-up-9}

That's a good point for us to wrap up this introduction to fairness in
data science.

We have seen that optimising for predictive accuracy alone can lead to
unjust models. We also raised concerns about protected characteristics
being included in models, whether that is directly as a predictor or via
a collection of other predictors that well approximate them.

We have seen that there are a multitude of measures to assess the
fairness of our models. We can combine these with standard metrics for
goodness-of-fit to create custom loss functions which represent our
preference between fairness and predictive performance.

As with privacy, there are no universal answers when it comes to
measuring and implementing fair data science methodology. This is still
a relatively new and rapidly evolving field of data science.

\chapter{Codes of Conduct}\label{ethics-conduct}

\section{Data Science: Miracle Cure and Sexiest
Job}\label{data-science-miracle-cure-and-sexiest-job}

\begin{figure}[H]

{\centering \includegraphics{images/503-ethics-conduct/harvard-business-review-sexiest-job.png}

}

\caption{Title of Harvard Business Review article: Is data scientist
still the sexiest job of the 21st Century?}

\end{figure}%

It has been more than 10 years since it was proclaimed that data science
was the sexiest job of the century. Current turbulence in the technology
sector may have you questioning the veracity of that claim, but it still
rings true if we take a less myopic view of where data science is used.

Data science and machine learning are being used more extensively than
ever across fields including medicine, healthcare and sociology. Data
science is also applied to understand our environment, study ecological
systems and inform our strategies for their preservation. In addition to
this, data science is used widely across the private sector, where it
informs business strategies and financial services, alongside the public
sector where it influences governance and policy development.

\section{What could go wrong?}\label{what-could-go-wrong}

While data science methods can be wonderful, they are not infallible. As
the number of use cases increases, we should also expect that the number
of misuses or high-profile failures to increase too.

These two news articles highlight just how badly this can all go.

\begin{figure}[H]

{\centering \includegraphics{images/503-ethics-conduct/bbc-facial-recognition.png}

}

\caption{BBC news article: facial recognition fails on race, government
study says.}

\end{figure}%

The first article shows how a combination of non-representative training
data and lack of consideration on the part of developers led to a facial
recognition system that was much less accurate for people with darker
skin. This is absolutely unacceptable, particularly given the widespread
use of similar systems in important security applications such as
biometric ID for online banking or automatic passport gates.

A second example shows the risk of catastrophic failures in self-driving
cars. We know that misclassification and missed edge cases are
inevitable and that in the case of self-driving cars these errors can
cause serious and even fatal accidents. We might then ask ourselves if
these errors are acceptable if they lead to fewer accidents or
fatalities than would be caused by human drivers.

\begin{figure}[H]

{\centering \includegraphics{images/503-ethics-conduct/bbc-self-driving_1.png}

}

\caption{BBC news article: Uber's self-driving operator charged over
fatal crash.}

\end{figure}%

Another important point to establish is where liability falls in such
cases: should it be with the operator of the self-driving vehicle, the
vendor who solid it or the data scientist who wrote the script
controlling the car's actions? If this behaviour was determined by
training data for test-drives, should that driver share in the
liability.

These are all important questions that we haven't necessarily thought to
ask before deploying data science solutions into the world.

\section{\texorpdfstring{That's not \emph{my} type of data science
\ldots{}}{That's not my type of data science \ldots{}}}\label{thats-not-my-type-of-data-science}

You might be thinking at this doesn't effect you because you don't work
in image analysis or on self-driving cars, but related issues come up in
more standard applications of data science.

Consider a retailer implementing targeted promotions. They might combine
previous shopping habits and recent product searches to target their
offers.

\begin{figure}

\centering{

\includegraphics[width=4.83in,height=\textheight]{images/503-ethics-conduct/pregnancy-prediction.png}

}

\caption{\label{fig-pregnancy-prediction}Predicting pregnancy from
search and purchase history.}

\end{figure}%

Around 7 months ago, one customer stopped regularly buying
contraceptives and alcohol and started buying supplements for vitamin D
and folic acid. Based on similar behaviour by lots of other customers, a
recommender model might now expect a good way to increase sales would be
to send that customer offers for nappies, baby food and talcum powder.

This could be seriously upsetting if that customer also happened to be
experiencing fertility issues or had been pregnant but had an abortion
or miscarriage. It is important that human-overrides are included in
such systems like this so that the customer could contact the retailer
and prevent this sort of advertising from continuing to happen for
months or years to come.

\section{Technological Adoption Relies on Public
Trust}\label{technological-adoption-relies-on-public-trust}

\begin{figure}[H]

{\centering \includegraphics[width=4.92in,height=\textheight]{images/503-ethics-conduct/collapsed-bridge.png}

}

\caption{Thirteen people were killed and 145 injured during the
Interstate 35W bridge collapse in 2007.}

\end{figure}%

Data science is a new discipline, which means that some of the issues we
have mentioned are new and happening for the first time. However, many
of these issues are not unique to data science and that means we can
learn from other disciplines that have already invested a lot of time
and energy into those problems.

Across many other professions it's standard to follow a professional
code of practice or to have a code of ethics that applies to everyone
working in that space. Usually, the professions that have this in place
are also those who are legally liable in some way for the outcomes and
consequences of their work.

The most obvious example of this might be doctors and other medical
professionals, but this holds equally true in many jobs from law to
engineering, where practitioner's work can impact the freedom or safety
of other people. When thinking critically about the practical and
ethical implications of our work as data scientists, we shouldn't start
from scratch but should learn as much as possible from these other
fields.

One profession in particular that we can learn from is medicine. Around
the world, doctors agree to uphold the privacy of their patients by
maintaining medical confidentiality. The also agree to only act in what
they believe to be the best interests of the patient and to do them no
harm. These are both admirable principles that can be tracked directly
onto our work in data science.

Google's original internal code encapsulated a more extreme version of
this non-maleficence principle, starting with the phrase `Don't be evil'
(thought this was later rephrased to `You can make money without being
evil').

\section{A Hippocratic Oath for Data
Scientists}\label{a-hippocratic-oath-for-data-scientists}

\begin{figure}[H]

{\centering \includegraphics[width=2.3in,height=\textheight]{images/503-ethics-conduct/wired-hippocratic.png}

}

\caption{In Weapons of Math Destruction (2016), Cathy O'Neil was among
the first authors to make a call for a Hippocratic oath for data
scientists}

\end{figure}%

We've seen examples of both the broad and the acute harms that can
befall individuals because of data science. With these in mind, it seems
reasonable to expect that data scientists should have to be aware of the
negative impacts of their work and be required to mitigate these
wherever possible.

This is the argument made by Cathy O'Neil in her book Weapons of Math
Destruction: data scientists, and those working with mathematical models
more generally, need an equivalent of the Hippocratic oath that's used
in medicine. To understand why this is necessary, we have to understand
why it is \emph{not} the case already.

Doing the right thing, in data science as in life,is neither obvious nor
easy.

In the most naive sense, negative consequences can come about as a
result of a lack of understanding or because of unanticipated
consequences. This lack of understanding might relate to the model
itself (which might not be explainable) or about the setting in which
the model is being applied (which might require domain specific
expertise that the data scientist lacks).

Alternatively, this ignorance can be about the groups of people who are
most at risk of harm from data science models. Data science methods tend
to model expected behaviour and echo biases from within the training
data. This means that minority groups or groups that have been
historically disadvantaged are most at risk of further harm. This same
minority status or historical disadvantage means that these same groups
have low representation within data science teams. This increases the
chance that data science teams are ignorant of or ignore the breadth and
depth of the damage they might be causing.

A second way that data science can lead to harm is when incentives are
not properly aligned. As an example we consider a credit scoring
application where a data scientist is trying to include fairness
measures in addition to optimising predictive accuracy for loan
repayments. If business incentives are purely based around immediate
increases in profit, then this will likely be very difficult to get put
into production. There is a misalignment between the priorities of the
business and the ethics of the data scientist.

As we have seen,some errors are inevitable and sometimes the best we can
do is to balance an inherent trade-off between conflicting priorities or
different sources of harm. In these cases it is vitally important that
we highlight this trade-off explicitly and offer decision makers a range
of solutions that they may choose according to their own priorities.

Doing the right thing is neither obvious or easy:

\begin{itemize}
\item
  Lack of understanding,
\item
  Unanticipated consequences
\item
  Incentive structures,
\item
  Inherent trade-offs.
\end{itemize}

\section{Codes of Conduct}\label{codes-of-conduct}

Membership of a professional body can offer some of the ethical benefits
of a Hippocratic oath, because these memberships often require agreement
to abide by a code of conduct.

This provides individual data scientists with accountability for their
actions in a broader sense than through their direct accountability to
their employer. This can be helpful as a motivator to at a high quality
and in line with ethical guidelines.

It can also be useful as a form of external support, to convince
organisations who employ data scientists that ethical considerations are
an important aspect of our work. There is also the opportunity for
peer-to-peer support, so that best practices can be established and
improved in a unified way across industries, rather than in isolation
within individual organisations. This might be through informal
networking and knowledge sharing or through formal training organised by
that professional body.

Data science is still a developing field and there currently is no
professional body specifically dedicated to data science. Instead, data
scientists might join one or more related professional bodies, such as
the \href{https://rss.org.uk/}{Royal Statistical Society}, the
\href{https://www.theorsociety.com/}{Operational Research Society}, or
the \href{https://www.siam.org/}{Society for Industrial and Applied
Mathematics}.

\section{Wrapping Up}\label{wrapping-up-10}

Data Science is still maturing, both as a field of study and as a career
path. In combination with the widespread adoption of data science in
recent years, this means that as a discipline we're now navigating many
new modes of failure and ethical issues. However, not all of these
issues are entirely novel. There is an awful lot that we can learn,
borrow or adapt from more established fields such as medicine,
engineering or physics.

It is important that we remain alert to the dangers of our work: both
the liability it opens us up to and the harm we might be causing to
others, potentially without being aware of either of these things. By
joining professional bodies and upholding their codes of conduct we can
push back against bad practices and reward good ones.

\chapter*{Checklist}\label{ethics-checklist}
\addcontentsline{toc}{chapter}{Checklist}

\markboth{Checklist}{Checklist}

\section*{Videos / Chapters}\label{videos-chapters-4}
\addcontentsline{toc}{section}{Videos / Chapters}

\markright{Videos / Chapters}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=5d85c64e-ce0a-4a18-bd3b-afa5010ceca8}{Privacy}
  (19 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/05-01-privacy/05-01-privacy.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=96301a9c-c129-44fd-bd3f-afa700fdeeaa}{Fairness}
  (20 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/05-02-fairness/05-02-fairness.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=5c457e71-1f44-4272-a46b-afa701747ba6}{Codes
  of Conduct} (14 min)
  \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/05-03-codes-of-conduct/05-03-codes-of-conduct.pdf}{{[}slides{]}}
\end{itemize}

\section*{Reading}\label{reading-4}
\addcontentsline{toc}{section}{Reading}

\markright{Reading}

Use the \hyperref[ethics-reading]{Data Science Ethics} section of the
reading list to support and guide your exploration of this week's
topics. Note that these texts are divided into core reading, reference
materials and materials of interest.

\section*{Activities}\label{activities-2}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

This week has fewer activities, so that you may look over the second
assessment before the end of the course.

\emph{Core:}

\begin{itemize}
\item
  Read
  \href{https://proceedings.mlr.press/v81/buolamwini18a.html}{Gender
  Shades: Intersectional Accuracy Disparities in Commercial Gender
  Classification} by Joy Buolamwini and Timnit Gebru (2018). Proceedings
  of the 1st Conference on Fairness, Accountability and Transparency.
\item
  Find an example case study or method relating to ethical data science
  that has not been covered in the lectures. Share what you find by
  writing a short summary of the case study or method on the discussion
  forum.
\item
  Skim over the Professional Guidelines listed in the reference
  materials for this week, in preparation for the live session.
\end{itemize}

\emph{Bonus}

\begin{itemize}
\item
  Complete the
  \href{https://zakvarty.github.io/effective-data-science-resources/activities/fairness-worksheet/fairness-worksheet-questions.html}{Fairness
  worksheet}.
\item
  Complete the
  \href{https://zakvarty.github.io/effective-data-science-resources/activities/privacy-worksheet/privacy-worksheet-questions.html}{Privacy
  worksheet}.
\end{itemize}

\section*{Live Session}\label{live-session-4}
\addcontentsline{toc}{section}{Live Session}

\markright{Live Session}

In the live session this week we will begin with a few minutes of Q \& A
about the assessments. We will then break into groups to discuss and
compare several sets of professional guidelines on ethical data science.

Finally, if time allows we will round up the session with an activity on
randomised response survey designs or data anonymisation.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Reading List}\label{reading-list}

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, bottomtitle=1mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, breakable, colback=white, bottomrule=.15mm, opacityback=0, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, titlerule=0mm, toptitle=1mm, opacitybacktitle=0.6, arc=.35mm, rightrule=.15mm, coltitle=black, left=2mm]

Effective Data Science is still a work-in-progress. This chapter is
largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so
at \url{https://github.com/zakvarty/data_science_notes}.

\end{tcolorbox}

This reading list is organised by topic, according to each week of the
course. These are split into several categories.

\begin{itemize}
\item
  \textbf{Core Materials:} These form a core part of the course
  activities.
\item
  \textbf{Reference Materials:} These will be used extensively in the
  course, but should be seen as helpful guides, rather than required
  reading from cover to cover.
\item
  \textbf{Materials of Interest:} These will not form a core part of the
  course, but will give you a deeper understanding or interesting
  perspective on the weekly topic. There might be some fun other stuff
  in here too.
\end{itemize}

\section{Effective Data Science Workflows}\label{workflows-reading}

\subsection*{Core Materials}\label{core-materials}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\tightlist
\item
  The \href{https://style.tidyverse.org/}{Tidyverse R Style Guide} by
  Hadley Wickham.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510&ref=https://githubhelp.com}{Wilson,
  et al (2017)}. Good Enough Practices in Scientific Computing. PLOS
  Computational Biology.
\end{itemize}

\subsection*{Reference Materials}\label{reference-materials}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  \href{https://r4ds.had.co.nz/index.html}{R For Data Science Chapters
  2, 6 and 8} by Hadley Wickham and Garrett Grolemund. Chapters covering
  R workflow basics, a scripting and project based workflow.
\item
  \href{https://here.r-lib.org/articles/here.html}{Documentation} for
  the \{here\} package
\item
  \href{https://r-pkgs.org/}{R Packages Book} (Second Edition) by Hadley
  Wickham and Jenny Bryan.
\end{itemize}

\subsection*{Materials of Interest}\label{materials-of-interest}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\tightlist
\item
  \href{https://stat545.com/index.html}{STAT545, Part 1} by Jennifer
  Bryan and The STAT 545 TAs
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://rstats.wtf/}{What they forgot to teach you about R,
  Chapters 2-4} by Jennifer Bryan and Jim Hester.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://www.amstat.org/docs/default-source/amstat-documents/pol-reproducibleresearchrecommendations.pdf}{Broman
  et al (2017)}. Recommendations to Funding Agencies for Supporting
  Reproducible Research. American Statistical Association.
\end{itemize}

\begin{itemize}
\item
  \href{https://adv-r.hadley.nz/}{Advanced R} by Hadley Wickham Section
  introductions on \href{https://adv-r.hadley.nz/fp.html}{functional}
  and \href{https://adv-r.hadley.nz/oo.html}{object oriented} approaches
  to programming.
\item
  \href{https://www.atlassian.com/agile/project-management}{Atlassian
  Article} on Agile Project Management
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/}{The
  Pragmatic Programmer, 20th Anniversary Edition Edition} by David
  Thomas and Andrew Hunt. The section on
  \href{https://media.pragprog.com/titles/tpp20/dry.pdf}{DRY coding} and
  a few others are freely available.
\end{itemize}

\begin{itemize}
\item
  \href{https://csgillespie.github.io/efficientR/}{Efficient R
  programming} by Colin Gillespie and Robin Lovelace. Chapter 5
  considers
  \href{https://csgillespie.github.io/efficientR/input-output.html}{Efficient
  Input/Output} is relevant to this week. Chapter 4 on
  \href{https://csgillespie.github.io/efficientR/workflow.html}{Efficient
  Workflows} links nicely with last week's topics.
\item
  \href{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\#References}{Towards
  A Principled Bayesian Workflow} by Michael Betancourt.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://happygitwithr.com/}{Happy Git and GitHub for the useR}
  by Jennifer Bryan
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://monashbioinformaticsplatform.github.io/2017-11-16-open-science-training/topics/automation.html}{Make
  Tutorial} by the Monash Informatics Platform.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://robjhyndman.com/hyndsight/makefiles/}{Makefiles for R
  and LaTeX projects} blog post by Rob Hyndman
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://makefiletutorial.com/\#getting-started}{Makefile
  tutorial} by Chase Lambert
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Aquiring and Sharing Data}\label{data-reading}

\subsection*{Core Materials}\label{core-materials-1}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\item
  \href{https://r4ds.had.co.nz/tidy-data.html}{R for Data Science
  Chapters 9 - 12} by Hadley Wickham. These chapters introduce tibbles
  as a data structure, how to import data into R and how to wrangle that
  data into tidy format.
\item
  \href{https://csgillespie.github.io/efficientR/}{Efficient R
  programming} by Colin Gillespie and Robin Lovelace. Chapter 5
  considers
  \href{https://csgillespie.github.io/efficientR/input-output.html}{Efficient
  Input/Output} is relevant to this week.
\item
  \href{https://vita.had.co.nz/papers/tidy-data.html}{Wickham (2014)}.
  Tidy Data. Journal of Statistical Software. The paper that brought
  tidy data to the mainstream.
\end{itemize}

\subsection*{Reference Materials}\label{reference-materials-1}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  The \{readr\} \href{https://readr.tidyverse.org/}{documentation}
\item
  The \{data.table\}
  \href{https://cran.r-project.org/web/packages/data.table/data.table.pdf}{documentation}
  and
  \href{https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html}{vignette}
\item
  The \{rvest\} \href{https://rvest.tidyverse.org/}{documentation}
\item
  The \{tidyr\} \href{https://tidyr.tidyverse.org/}{documentation}
\item
  MDN Web Docs on
  \href{https://developer.mozilla.org/en-US/docs/Web/HTML}{HTML} and
  \href{https://developer.mozilla.org/en-US/docs/Web/CSS}{CSS}
\end{itemize}

\subsection*{Materials of Interest}\label{materials-of-interest-1}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\tightlist
\item
  \href{https://zapier.com/learn/apis/chapter-1-introduction-to-apis/}{Introduction
  to APIs} by Brian Cooksey
\item
  \href{https://r4ds.hadley.nz/}{R for Data Science (Second Edition)}
  Chapters within the \href{https://r4ds.hadley.nz/import.html}{Import}
  section.
\end{itemize}

This covers importing data from spreadsheets, databases, using Apache
Arrow and importing hierarchical data as well as web scraping.

\section{Data Exploration and Visualisation}\label{edav-reading}

\subsection*{Core Materials}\label{core-materials-2}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\tightlist
\item
  \href{https://bookdown.org/rdpeng/exdata/}{Exploratory Data Analysis
  with R} by Roger Peng.
\end{itemize}

Chapters 3 and 4 are core reading, respectively introducing
\href{https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html}{data
frame manipulation with \{dplyr\}} and an example
\href{https://bookdown.org/rdpeng/exdata/exploratory-data-analysis-checklist.html}{workflow
for exploratory data analysis}. Other chapters may be useful as
references.

\begin{itemize}
\tightlist
\item
  \href{https://stefvanbuuren.name/fimd/}{Flexible Imputation of Missing
  Data} by Stef van Buuren.
  \href{https://stefvanbuuren.name/fimd/ch-introduction.html}{Sections
  1.1-1.4} give a thorough introduction to missing data problems.
\end{itemize}

\subsection*{Referene Materials}\label{referene-materials}
\addcontentsline{toc}{subsection}{Referene Materials}

\begin{itemize}
\item
  \href{}{A ggplot2 Tutorial for Beautiful Plotting in R}
  https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/)
  by Cdric Scherer.
\item
  The \{dplyr\} \href{https://dplyr.tidyverse.org/}{documentation}
\item
  \href{https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf}{RStudio
  Data Transformation Cheat Sheet}
\item
  \href{https://r4ds.had.co.nz/index.html}{R for Data Science (First
  Edition)} Chapters on
  \href{https://r4ds.had.co.nz/transform.html}{Data Transformations},
  \href{https://r4ds.had.co.nz/exploratory-data-analysis.html}{Exploratory
  Data Analysis} and
  \href{https://r4ds.had.co.nz/relational-data.html}{Relational Data}.
\item
  Equivalent sections in R for Data Science
  \href{https://r4ds.hadley.nz/}{Second Edition}
\end{itemize}

\subsection*{Materials of Interest}\label{materials-of-interest-2}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1198_jcgs_2009_07098&context=PC&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Primo\%20Central&tab=Everything&query=any,contains,layered\%20grammar\%20of\%20graphics&offset=0}{Wickham,
  H. (2010)}. A Layered Grammar of Graphics. Journal of Computational
  and Graphical Statistics.
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000664639501591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local\%20Search\%20Engine&tab=Everything&query=any,contains,better\%20data\%20visualisations&offset=0}{Better
  Data Visualisations} by Jonathan Schwabish
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000211295101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local\%20Search\%20Engine&tab=Everything&query=any,contains,Data\%20Visualization\%20\%E2\%80\%93\%20A\%20Practical\%20Introduction&offset=0}{Data
  Visualization: A Practical Introduction} by Kieran Healy
\end{itemize}

\section{Preparing for Production}\label{production-reading}

\subsection*{Core Materials}\label{core-materials-3}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local\%20Search\%20Engine&tab=Everything&query=any,contains,kearns\%20and\%20roth&mode=Basic}{The
  Ethical Algorithm} M Kearns and A Roth (Chapter 4)
\item
  \href{https://arxiv.org/abs/1602.04938}{Ribeiro et al (2016)}. ``Why
  Should I Trust You?'': Explaining the Predictions of Any Classifier.
\end{itemize}

\subsection*{Reference Materials}\label{reference-materials-2}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  The \href{https://docker-curriculum.com/}{Docker Curriculum} by
  Prakhar Srivastav.
\item
  LIME
  \href{https://cran.r-project.org/web/packages/lime/index.html}{package
  documentation} on CRAN.
\item
  \href{https://christophm.github.io/interpretable-ml-book/}{Interpretable
  Machine Learning: A Guide for Making Black Box Models Explainable} by
  Christoph Molnar.
\item
  Documentation for
  \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply}{apply()},
  \href{https://purrr.tidyverse.org/reference/map.html}{map()} and
  \href{https://furrr.futureverse.org/}{pmap()}
\item
  \href{https://adv-r.hadley.nz/index.html}{Advanced R (Second Edition)}
  by Hadley Wickham.
  \href{https://adv-r.hadley.nz/perf-measure.html}{Chapter 23} on
  measuring performance and
  \href{https://adv-r.hadley.nz/perf-improve.html}{Chapter 24} on
  improving performance.
\end{itemize}

\subsection*{Materials of Interest}\label{materials-of-interest-3}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108&context=PC&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Primo\%20Central&tab=Everything&query=any,contains,ASA\%20p-value&offset=0}{The
  ASA Statement on \(p\)-values: Context, Process and Purpose}
\item
  \href{http://stat.columbia.edu/~gelman/research/unpublished/forking.pdf}{The
  Garden of Forking Paths: Why multiple comparisons can be a problem,
  even when there is no ``Fishing expedition'' or ``p-hacking'' and the
  research hypothesis was posited ahead of time}. A Gelman and E loken
  (2013)
\item
  \href{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}{Understanding
  LIME tutorial} by T Pedersen and M Benesty.
\item
  \href{https://adv-r.hadley.nz/index.html}{Advanced R (Second Edition)}
  by Hadley Wickham. \href{https://adv-r.hadley.nz/rcpp.html}{Chapter
  25} on writing R code in C++.
\end{itemize}

\section{Data Science Ethics}\label{data-science-ethics-1}

\subsection*{Core Materials}\label{core-materials-4}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local\%20Search\%20Engine&tab=Everything&query=any,contains,kearns\%20and\%20roth&mode=Basic}{The
  Ethical Algorithm} M Kearns and A Roth. Chapters 1 and 2 on
  Algorithmic Privacy and Algortihmic Fairness.
\item
  \href{https://proceedings.mlr.press/v81/buolamwini18a.html}{Gender
  Shades: Intersectional Accuracy Disparities in Commercial Gender
  Classification} by Joy Buolamwini and Timnit Gebru (2018). Proceedings
  of the 1st Conference on Fairness, Accountability and Transparency.
\item
  \href{https://ieeexplore.ieee.org/document/4531148}{Robust
  De-anonymization of Large Sparse Datasets} by Arvind Narayanan and
  Vitaly Shmatikov (2008). IEEE Symposium on Security and Privacy.
\end{itemize}

\subsection*{Reference Materials}\label{reference-materials-3}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  \href{https://fairmlbook.org/}{Fairness and machine learning
  Limitations and Opportunities} by Solon Barocas, Moritz Hardt and
  Arvind Narayanan.
\item
  Professional Guidleines on Data Ethics from:

  \begin{itemize}
  \tightlist
  \item
    \href{http://www.ams.org/about-us/governance/policy-statements/sec-ethics}{The
    American Mathematical Society}
  \item
    \href{https://op.europa.eu/s/sUPP}{The European Union}
  \item
    \href{https://www.gov.uk/guidance/understanding-artificial-intelligence-ethics-and-safety}{UK
    Government}
  \item
    \href{https://rss.org.uk/RSS/media/News-and-publications/Publications/Reports\%20and\%20guides/A-Guide-for-Ethical-Data-Science-Final-Oct-2019.pdf}{Royal
    Statistical Society}
  \item
    \href{https://www.government.nl/documents/reports/2021/07/31/impact-assessment-fundamental-rights-and-algorithms}{Dutch
    Government}
  \end{itemize}
\end{itemize}

\subsection*{Materials of Interest}\label{materials-of-interest-4}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/2001.09784}{Algorithmic Fairness} (2020).
  Pre-print of review paper by Dana Pessach and Erez Shmueli.
\end{itemize}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\end{CSLReferences}



\end{document}
