[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective Data Science",
    "section": "",
    "text": "About this Course\nModel building and evaluation are are necessary but not sufficient skills for the effective practice of data science. In this module you will develop the technical and personal skills that are required to work successfully as a data scientist within an organisation.\nDuring this module you will critically explore how to:\nThis interdisciplinary course will draw from fields including statistics, computing, management science and data ethics. Each topic will be investigated through a selection of lecture videos, conference presentations and academic papers, hands-on lab exercises, along with readings on industry best-practices from recognised professional bodies."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Effective Data Science",
    "section": "Schedule",
    "text": "Schedule\nThese notes are intended for students on the course MATH70076: Data Science in the academic year 2023/24.\nAs the course is scheduled to take place over five weeks, the suggested schedule is:\n\n1st week: effective data science workflows;\n2nd week: acquiring and sharing data;\n3rd week: exploratory data analysis and visualisation;\n4th week: preparing for production;\n5th week: ethics and context of data science.\n\nA pdf version of these notes may be downloaded here. Please be aware that these are very rough and will be updated less frequently than the course webpage."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Effective Data Science",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this module students should be able to:\n\nIndependently scope and manage a data science project;\nSource data from the internet through web scraping and APIs;\nClean, explore and visualise data, justifying and documenting the decisions made;\nEvaluate the need for (and implement) approaches that are explainable, reproducible and scalable;\nAppraise the ethical implications of a data science projects, particularly the risks of compromising privacy or fairness and the potential to cause harm."
  },
  {
    "objectID": "index.html#allocation-of-study-hours",
    "href": "index.html#allocation-of-study-hours",
    "title": "Effective Data Science",
    "section": "Allocation of Study Hours",
    "text": "Allocation of Study Hours\nLectures: 10 Hours (2 hours per week)\nGroup Teaching: 5 Hours (1 hour per week)\nLab / Practical: 10 hours (2 hours per week)\nIndependent Study: 100 hours (15 hours per week + 30 hours coursework)\nDrop-In Sessions: Each week there will be a 1-hour optional drop-in session to address any questions about the course or material. This is where you can get support from the course lecturer or GTA on the topics covered each week, individually or in small groups.\nOffice Hours: Additionally, there will be an office hour each week. This is a weekly opportunity for 1-1 discussion with the course lecturer to address any individual questions, concerns or problems that you might have. These meetings can be in person or on Teams and can be academic (relating to course content or progress) or pastoral (relating to student well-being) in nature. To book a 1-1 meeting please use the link on the course blackboard page."
  },
  {
    "objectID": "index.html#assessment-structure",
    "href": "index.html#assessment-structure",
    "title": "Effective Data Science",
    "section": "Assessment Structure",
    "text": "Assessment Structure\nThe course will be assessed entirely by coursework, reflecting the practical and pragmatic nature of the course material.\nCoursework 1 (30%): To be completed during the fourth week of the course.\nCoursework 2 (70%): To be released in the last week of the course and submitted following the examination period in Summer term."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Effective Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes were created by Dr Zak Varty. They were inspired by a previous lecture series by Dr Purvasha Chakravarti at Imperial College London and draw from many resource that were made available by the R community."
  },
  {
    "objectID": "100-workflows-introduction.html",
    "href": "100-workflows-introduction.html",
    "title": "Effective Workflows",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nAs a data scientist you will never work alone.\nWithin a single project a data scientist is likely that you will interact with a range of other people, including but not limited to: one or more project managers, stakeholders and subject matter experts. These experts might come from a single specialism or form a multidisciplinary team, depending on the type of work that you are doing.\nTo get your project put into use and working at scale you will likely have to collaborate with data engineers. You will also work closely with other data scientists, to review one another’s work or to collaborate on larger projects.\nFamiliarity with the skills, processes and practices that make for collaboration is instrumental to being a successful as a data scientist. The aim for this part of the course is to provide you with a structure on how you organise and perform your work, so that you can be a good collaborator to current colleges and your future self.\nThis is going to require a bit more effort upfront, but the benefits will compound over time. You will get more done by wasting less time staring quizzically at messy folders of indecipherable code. You will also gain a reputation of someone who is good to work with. This promotes better professional relationships and greater levels of trust, which can in turn lead to working on more exciting and impactful projects."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#what-are-we-trying-to-do",
    "href": "101-workflows-organising-your-work.html#what-are-we-trying-to-do",
    "title": "1  Organising your work",
    "section": "1.1 What are we trying to do?",
    "text": "1.1 What are we trying to do?\nFirst, let’s consider why we want to provide our data science projects with some sense of structure and organization.\nAs a data scientist you’ll never work alone. Within a single project you’ll interact with a whole range of other people. This might be a project manager, one or more business stakeholders or a variety of subject matter experts. These experts might be trained as sociologists, chemists, or civil servants depending on the exact type of data science work that you’re doing.\nTo then get your project put into use and working at scale you’ll have to collaborate with data engineers. You’ll also likely work closely with other data scientists. For smaller projects this might be to act as reviewers for one another’s work. For larger projects working collaboratively will allow you to tackle larger challenges. These are the sorts of project that wouldn’t be feasible alone, because of the inherent limitations on the time and skill of any one individual person.\nEven if you work in a small organization, where you’re the only data scientist, then adopting a way of working that’s focused on collaborating will pay dividends over time. This is because when you inevitably return to the project that you’re working on in several weeks or months or years into the future you’ll have forgotten almost everything of what you did the first time around. You’ll also have forgotten why you made the decisions that you did and what other potential options there were that you didn’t take.\nThis is exactly like working with a current colleague who has shoddy or poor working practices. Nobody wants to be that colleague to somebody else, let alone to their future self. Even when working alone, treating your future self as a current collaborator (and one that you want to get along well with) makes you a kind colleague and a pleasure to work with.\nThe aim of this week is to provide you with a guiding structure on how you organize and perform your work. None of this is going to be particularly difficult or onerous. However it will require a bit more effort up front and daily discipline. Like with flossing, the daily effort required is not large but the benefits will compound over time.\nYou’ll get more done by wasting less time staring quizzically at a mess of folders and indecipherable code. You’ll also get a reputation as someone who’s well organized and good to work with. This promotes better professional relationships and greater levels of trust within your team. These can then, in turn, tead to you working on more exciting and more impactful projects in the future."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#an-r-focused-approach",
    "href": "101-workflows-organising-your-work.html#an-r-focused-approach",
    "title": "1  Organising your work",
    "section": "1.2 An R Focused Approach",
    "text": "1.2 An R Focused Approach\nThe structures and workflows that are recommend here and throughout the rest of this module are focused strongly on a workflow that predominantly uses R, markdown and LaTeX.\nSimilar techniques, code and software can achieve the same results that I show you here when coding in Python or C, or when writing up projects in Quarto or some other markup language. Similarly, different organizations have their own variations on these best practices that we’ll go through together. Often organisations will have extensive guidance on these topics.\nThe important thing is that once you understand what good habits are and have build them in one programming language or business, then transferring these skills to a new setting is largely a matter of learning some new vocabulary or slightly different syntax.\nWith that said, let’s get going!"
  },
  {
    "objectID": "101-workflows-organising-your-work.html#one-project-one-directory",
    "href": "101-workflows-organising-your-work.html#one-project-one-directory",
    "title": "1  Organising your work",
    "section": "1.3 One Project = One Directory",
    "text": "1.3 One Project = One Directory\nIf there’s one thing you should take away from this chapter, it’s this one Golden Rule:\n\nEvery individual project you work on as a data scientist should be in a single, self-contained directory or folder.\n\nThis is worth repeating. Every single project that you work on should be self-contained and live in a single directory. An analogy here might be having a separate ring-binder folder for each of your modules on a degree program.\n\n\n\n\n\n\n\n\n\nThis one golden rule is deceptively simple.\nThe first issue here is that it requires a predetermined scope of what is and what isn’t going to be covered by this particular project. This seems straightforward but at the outset of the project you often do not know exactly where your project will go, or how it will link to other pieces of work within your organization.\nThe second issue is the second law of Thermodynamics, which applies equally well to project management as it does to the heatdeath of the universe. It takes continual external effort to prevent the contents of this one folder from becoming chaotic and disordered over time.\nThat being said, having a single directory has several benefits which more than justify this additional work."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#properties-of-a-well-orgainsed-project",
    "href": "101-workflows-organising-your-work.html#properties-of-a-well-orgainsed-project",
    "title": "1  Organising your work",
    "section": "1.4 Properties of a Well-Orgainsed Project",
    "text": "1.4 Properties of a Well-Orgainsed Project\nWhat are the properties that we would like this single, well-organized project to have? Ideally, we’d like to organize our projects so that I have the following properties:\n\nPortable\nVersion Control Friendly\nReproducible\nIDE friendly.\n\nDon’t worry if you haven’t heard of some of these terms already. We’re going to look at each of them in a little bit of detail.\n\n1.4.1 Portability\nA project is said to be portable if it can be easily moved without breaking.\n\n\n\n\n\n\n\n\n\nThis might be a small move, like relocating the directory to a different location on your own computer. It might also mean a moderate move, say to another machine if yours dies just before a big deadline. Alternatively, it might be a large shift - to be uses by another person who is using a different operating system.\nFrom this thought experiment you can see that there’s a full spectrum of how portable a project may or may not need to be.\n\n\n1.4.2 Version Control Friendly\nA project under Version Control has all changes tracked either manually or automatically. This means that snapshots of the project are taken regularly as it gradually develops and evolves over time. Having these snapshots as many, incremental changes are made to the project allow it to be rolled back to a specific previous state if something goes wrong.\nA version controlled pattern of working helps to avoid the horrendous state that we have all found ourselves in - renaming final_version.doc to final_final_version.doc and so on.\nBy organising your workflow around incremental changes helps you to acknowledge that no work is ever finally complete. There will always be small changes that need to be done in the future.\n\n\n1.4.3 Reproducibility\n\nA study is reproducible if you can take the original data and the computer code used to analyze the data and recreate all of the numerical findings from the study. \nBroman et al (2017). “Recommendations to Funding Agencies for Supporting Reproducible Research”\n\nIn their paper, Broman et al define reproducibility as a project where you can take the original data and code used to perform the analysis and using these we create all of the numerical findings of the study.\nThis definition leads naturally to several follow-up questions.\nWho exactly is you in this definition? Does it specifically mean yourself in the future or should someone else with access to all that data and code be able to recreate your findings too? Also, should this reproducibility be limited to just the numerical results? Or should they also be able to create the associated figures, reports and press releases?\nAnother important question is when this project needs to be reproduced. Will it be in a few weeks time or in 10 years time? Do you need to protect your project from changes in dependencies, like new versions of packages or modules? How about different versions of R or Python? Taking this time scale out even further, what about different operating systems and hardware?\nIt’s unlikely you’d consider someone handing you a floppy disk of code that only runs on Windows XP to be acceptably reproducible. Sure, you could probably find a way to get it to work, but that would be an awful lot of effort on your end.\nThat’s perhaps a bit of an extreme example, but it emphasizes the importance of clearly defining the level of reproducibility that you’re aiming for within every project you work on. This example also highlights the amount of work that can be required to reproduce an analysis, especially after quite some time. It is important to explicitly think about how we dividing that effort between ourselves as the original developer and the person trying to reproduce the analysis in the future.\n\n\n1.4.4 IDE Friendly\nOur final desirable property is that we’d like our projects to play nicely with integrated development environments.\nWhen you’re coding document and writing your data science projects it’d be possible for you to work entirely in either a plain text editor or typing code directly at the command line. While these approaches to a data science workflow have the benefit of simplicity, they also expect a great deal from you as a data scientist.\nThese workflows expect that you should type everything perfectly accurately every time, that you recall the names and argument orders of every function you use, and that you are constantly aware of the current state of all objects within your working environment.\nIntegrated Development Environments (IDEs) are applications that help to reduce this burden, helping make you a more effective programmer and data scientist. IDEs offer tools like code completion and highlighting to make your code easier to read and to write. They offer tools for debugging, to fix where things are going wrong, and they also offer environment panes so that you don’t have to hold everything in your head all at once. Many IDEs also often have templating facilities. These let you save and reuse snippets of code so that you can avoid typing out repetitive, boilerplate code and introducing errors in the process.\nEven if you haven’t heard of IDEs before, you’ve likely already used one. Some common examples might be RStudio for R-users, PyCharm for python users, or Visual Studio as a more language agnostic coding environment.\nWhichever of these we use, we’d like our project to play nicely with them. This lets us reap their benefits while keeping our project portable, version controlled, and reproducible for someone working with a different set-up."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#project-structure",
    "href": "101-workflows-organising-your-work.html#project-structure",
    "title": "1  Organising your work",
    "section": "1.5 Project Structure",
    "text": "1.5 Project Structure\nI’ve given a pretty exhaustive argument for why having a single directory for each project is a good idea. Let’s now take a look inside that directory and define a common starting layout for the content of all of your projects.\nHaving this sort of project directory template will mean that you’ll always know where to find what you’re looking for and other members of your team will too. Again, before we start I’ll reiterate that we’re taking an opinionated approach here and providing a sensible starting point for organizing many projects.\nEvery project is going to be slightly different and some might require slight alterations to what I suggest here. Indeed, even if you start as I suggest then you might have to adapt your project structure as it develops and grows. I think it’s helpful to consider yourself as a tailor when making these changes. I’m providing you with a one size fits all design, that’s great for lots of projects but perfect for none of them. It’s your job to alter and refine this design for each individual case.\nOne final caveat before we get started: companies and businesses will many times have a house style how to write and organize your code or projects. If that’s the case, then follow the style guide that your business or company uses. The most important thing here is to be consistent at both an individual level and across the entire data science team. It’s this consistency that reaps the benefits.\nOkay, so imagine now that you’ve been assigned a shiny new project and have created a single directory in which to house that project. Here we’ve, quite imaginatively, called that directory exciting-new-project. What do we populate this folder with?\n\n\n\n\n\n\n\n\n\nIn the rest of this video, I’ll define the house-style for organizing the root directory of your data science projects in this module.\n\n\n\n\n\n\n\n\n\nWithin the project directory there will be some subdirectories, which you can tell a folders in this file structure because they have a forward slash following their names. There will also be some files directly in the root directory. One of these is called readme.md and the another called either makefile or make.r. We’re going to explore each of these files and directories in turn.\n\n1.5.1 README.md\n\n\n\n\n\n\n\n\n\nLet’s begin with the readme file. This gives a brief introduction to your project and gives information on what the project aims to do. The readme file should describe how to get started using the project and how to contribute to its development.\nThe readme is written either in a plain text format so readme.txt or in markdown format readme.md. The benefit of using markdown is that it allows some light formatting such as sections headers and lists using plain text characters. Here you can see me doing that by using hashes to mark out first and second level headers and using bullet points for a unnumbered list. Whichever format you use, the readme file for your project is always stored in the root directory and is typically named in all uppercase letters.\nThe readme file should be the first thing that someone who’s new to your project reads. By placing the readme in the root directory and capitalising the file name you are increase the visibility of this file and increase the chances of this actually happening.\nAn additional benefit to keeping the readme in the root directory of your project is that code hosting services like GitHub, GitLab or BitBucket will display the contents of that readme file next to the contents of your project. Those services will also nicely format any markdown that you use for you in your readme file.\nWhen writing the readme, it can be useful to imaginge that you are writing this for a new, junior team member. The readme file should let them get started with the project and make some simple contributions after reading only that file. It might also link out to more detailed project documentation that will help the new team member toward a more advanced understanding or complex contribution.\n\n\n1.5.2 Inside the README\nlet’s take a quick aside to see in more detail what should be covered within a readme file.\nA readme we should include the name of the project, which should be self-explanatory (so nothing like my generic choice of exciting-new-project). The readme should also give the project status, which is just a couple of sentences to say whether your project is still under development, the version oft the current release or, on the other end of the project life-cycle, if the project is being deprecated or closed.\nFollowing this, we should also include a description of your project. This will state the purpose of your work and to provide, or link to, any additional context or references that visitors aren’t assumed to be familiar with.\nIf your project involves code or depends on other packages then you should give some instruction on how to install those dependencies and run your code. This might just be text but it could also include things like screenshots, code snippets, gifs or a video of the whole process.\nIt’s also a good practice to include some simple examples of how to use the code within your project an the expected results, so that new users can confirm that everything is working on their local instance. Keep the examples as simple and minimal as you can so that new users\nFor longer or more complicated examples that aren’t necessary in this short introductory document you can add links to those in the readme and explain them in detail elsewhere.\nThere should ideally be a short description of how people can report issues with the project and also how people can get started in resolving those issues or extend the project in some way.\nThat leads me on to one point that I’ve forgotten to list here. There there should be a section listing the authors of the work and the license in which under which it’s distributed. This is to give credit to all the people who’ve contributed to your project and the license file then says how other people may use your work. The license declares how other may use your project and whether they have to give direct attribution to your work in any modifications that they use.\n\n\n1.5.3 data\n\n\n\n\n\n\n\n\n\nMoving back to our project structure, next we have the data directory.\nThe data directory will have two subdirectories one called raw and one called derived. All data that is not generate as part of your project is stored in the raw subdirectory. To ensure that a project is reproducible, data in the Raw folder should never be edited or modified.\n\n\n\n\n\n\n\n\n\nIn this example we’ve got two different data types: an Excel spreadsheet the XLS file and a JSON file. These files are exacty as we received them from our project stakeholder.\nThe text file metadata.txt is a plain text file explaining the contents and interpretation of each of the raw data sets. This metadata should include descriptions of all the measured variables, the units that are recorded in, the date the file was created or acquired, and the source from which it was obtained.\nThe raw data likely isn’t going to be in a form that’s amenable to analyzing straight away. To get the data into a more pleasant form to work, it will require some data manipulation and cleaning. Any manipulation or cleaning that is applied should be well documented and the resulting cleaned files saved within the derived data directory.\n\n\n\n\n\n\n\n\n\nIn our exciting new project, we can see the clean versions of the previous data sets which are ready for modelling. There’s also a third file in this folder. This is data that we’ve acquired for ourselves through web scraping, using a script within the project.\n\n\n1.5.4 src\n\n\n\n\n\n\n\n\n\nThe src or source directory contains all the source code for your project. This will typically be the functions that you’ve written to make the analysis or modelling code more accessible.\nHere we’ve saved each function in its own R script and, in this project, we’ve used subdirectories to organise these by their use case. We’ve got two functions used in data cleaning: the first replaces NA values with a given value, the second replaces these by the mean of all non-missing values.\nWe also have three helper functions: the first two calculate rolling mean and the geometric mean of a given vector, the third is a function that scrapes the web data we saw in the derived data subdirectory.\n\n\n1.5.5 tests\n\n\n\n\n\n\n\n\n\nMoving on then to the tests directory. The structure of this directory mirrors that of the source directory. Each function file has its own counterpart file of tests.\nThese test files provide example sets of inputs and the expected outputs for each function. The test files are used to check edge cases of a function or to assure yourself that you haven’t broken anything while fixing some small bug or adding new capabilities to that function.\n\n\n1.5.6 analyses\n\n\n\n\n\n\n\n\n\nThe analyses directory contains what you probably think of as the bulk of your data science work. It’s going to have one subdirectory for each major analysis that’s performed within your project and within each of these there might be a series of steps that we collect into separate scripts.\nThe activity performed at each step is made clear by the name of each script, as is the order in which we’re going to perform these steps. Here we can see the scripts used for the 2021 annual report. First is a script used to take the raw monthly receipts and produce the cleaned version of the same data set that we saw earlier. This is followed by a trend analysis of this cleaned data set.\nSimilarly for the spending review we have a data cleaning step, followed by some forecast modelling and finally the production of some diagnostic plots to compare these forecasts.\n\n\n1.5.7 outputs\n\n\n\n\n\n\n\n\n\nThe outputs directory has again one subdirectory for each meta-analysis within the project. These are then further organized by the output type whether that be some data, a figure, or a table.\nDepending on the nature of your project, you might want to use a modified subdirectory structure here. For example, if you’re doing several numerical experiments then you might want to arrange your outputs by experiment, rather than by output type.\n\n\n1.5.8 reports\n\n\n\n\n\n\n\n\n\nThe reports directory is then where everything comes together. This is where the written documents that form the final deliverables of your project are created. If these final documents are written in LaTeX or markdown, both the source and the compiled documents can be found within this directory.\nWhen including content in this report, for example figures, I’d recommend against making copies of those figure files within the reports directory. If you do that, then you’ll have to manually update the files every time you modify them. Instead you can use relative file paths to include these figures. Relative file paths specify how to get to the image, starting from your TeX document and moving up and down through the levels of your project directory.\nIf you’re not using markdown or LaTeX to write your reports, but instead use an online platform like overleaf as a latex editor or Google docs to write collaboratively then links to them in the reports directory using additional readme files. Make sure you set the read and write permissions for those links appropriately, too.\nWhen using these online writing systems, you’ll have to manually upload and update your plots whenever you modify any of your earlier analysis. That’s one of the drawbacks of these online tools that has to be traded off against their ease of use.\nIn our exciting new project, here we can see that the annual report is written in a markdown format, which is compiled to both HTML and PDF. The spendiing review is written in LaTeX and we only have the source for it, we don’t have the compiled pdf version of the document.\n\n\n1.5.9 make file\n\n\n\n\n\n\n\n\n\nThe final element of our template project structure is a make file. We aren’t going to cover how to read or write make files in this course. Instead, I’ll give you a brief description of what they are and what it is supposed to do.\nAt a high level, the make file is just a text file. What makes it special is what it contains. Similar to a shell or a bash script, make file contains code that could be run at the command line. This code will create or update each element of your project.\nThe make file defines shorthand commands for the full lines of code that create each element of your project. The make file also records the order in which these operations have to happen, and which of these steps are dependent on one another. This means that if one step part of your project is updated then any changes will be propagated through your entire project. This is done in quite a clever way so the only part of your projects that are re-run are those that need to be updated.\nWe’re omitting make files from this course not because they’re fiendishly difficult to write or read, but rather because they require a reasonable foundation in working at the command line to be understood. What I suggest you do instead throughout this course is to create your own R or markdown file called make. This file will define the intended running order and dependencies of your project and if it is an R file, it might also automate some parts of your analysis."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#wrapping-up",
    "href": "101-workflows-organising-your-work.html#wrapping-up",
    "title": "1  Organising your work",
    "section": "1.6 Wrapping up",
    "text": "1.6 Wrapping up\nWrapping up then, that’s everything for this chapter.\nI’ve introduced a project structure that will serve you well as a baseline for the vast majority of projects in data science.\n\n\n\n\n\n\n\n\n\nIn your own work, remember that the key here is standardisation. Working consistently across projects, a company or a group is more important than sticking rigidly to the particular structure that I have defined here.\nThere are two notable exceptions where you probably don’t want to use this project structure. That’s when you’re building an app or you’re building a package. These require specific organisation of the files within your project directory. We’ll explore the project structure used for package development during the live session this week."
  },
  {
    "objectID": "102-workflows-naming-files.html#introduction",
    "href": "102-workflows-naming-files.html#introduction",
    "title": "2  Naming Files",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\nPhil Karlton, Netscape Developer\n\nWhen working on a data science project we can in principle name directories, files, functions and other objects whatever we like. In reality though, using an ad-hoc system of naming is likely to cause confusion, headaches and mistakes. We obviously want to avoid all of those things, in the spirit of being kind to our current colleges and also to our future selves.\nComing up with good names is an art form. Like most art, naming things is an activity that you get better at with practice. Another similarity is that the best naming systems don’t come from giving data scientists free reign over their naming system. Like all art, the best approaches to naming things give you strong guidelines and boundaries within which to express your creativity and skill.\nIn this lecture we’ll explore what these boundaries and what we want them to achieve for us. The content of this lecture is based largely around a talk of the same name given by Jennifer Bryan and the tidyverse style guide, which forms the basis of Google’s style guide for R programming."
  },
  {
    "objectID": "102-workflows-naming-files.html#naming-files",
    "href": "102-workflows-naming-files.html#naming-files",
    "title": "2  Naming Files",
    "section": "2.2 Naming Files",
    "text": "2.2 Naming Files\nWe’ll be begin by focusing in on what we call our files. That is, we’ll first focus on the part of the file name that comes before the dot. In the second part of this video, we’ll then cycle back around to discuss file extensions.\n\n2.2.1 What do we want from our file names?\nBefore we dive into naming files, we should first consider what we want from the file names that we choose. There are three key properties that that we would like to satisfy.\n\nMachine Readable\nHuman Readable\nOrder Friendly\n\nThee first desirable property is for file names to be easily readable by computers, the second is for the file names to be easily readable by humans and finally the file names should take advantage of the default ordering imposed on our files.\nThis set of current file names is sorely lacking across all of these properties:\nabstract.docx\nEffective Data Science's module guide 2022.docx \nfig 12.png\nRplot7.png\n1711.05189.pdf\nHR Protocols 2015 FINAL (Nov 2015).pdf\nWe want to provide naming conventions to move us toward the better file names listed below.\n2015-10-22_human-resources-protocols.pdf\n2022_effective-data-science-module-guide.docx\n2022_RSS-conference-abstract.docx \nfig12_earthquake-timeseries.png \nfig07_earthquake-location-map.png\nogata_1984_spacetime-clustering.pdf\nLet’s take a few minutes to examine what exactly we mean by each of these properties.\n\n\n2.2.2 Machine Readable\nWhat do we mean by machine readable file names?\n\nEasy to compute on by deliberate use of delimiters:\n\nunderscores_separate_metadata, hyphens-separate-words.\n\nPlay nicely with regular expressions and globbing:\n\navoid spaces, punctuation, accents, cases;\nrm Rplot*.png\n\n\nMachine readable names are useful when:\n\nmanaging files: ordering, finding, moving, deleting:\nextracting information directly from file names,\nworking programmatically with file names and regex.\n\nWhen we are operating on a large number of files it is useful to be able to work with them programmatically.\nOne example of where this might be useful is when downloading assessments for marking. This might require me to unzip a large number of zip files, copying the pdf report from each unzipped folder into a single directory and all of the R scripts from each unzipped folder into another directory. The marked scripts and code then need to be paired back up in folders named by student, and re-zipped ready to be returned.\nThis is monotonously dull and might work for ~50 students but not for ~5000. Working programmatically with files is the way to get this job done efficiently. This requires the file names to play nicely with the way that computers interpret file names, which they regard as a string of characters.\nIt is often helpful to have some metadata included in the file name, for example the student’s id number and the assessment title. We will use an underscore to separate elements of metadata within the file name and a hyphen to separate sub-elements of meta-data, for example words within the assessment title.\nRegular expressions and globbing are two ideas from string manipulation that you may not have met, but which will inform our naming conventions. Regular expressions allow you to search for strings (in our case file names) that match a particular pattern. Regular expressions can do really complicated searches but become gnarly when you have to worry about special characters like spaces, punctuation, accents and cases, so these should be avoided in file names.\nA special type of regular expression is called globbing where a star is used to replace any number of subsequent characters in a file name, so that here we can delete all png images that begin with Rplot using a single line of code. Globbing becomes particular powerful when you use a consistent structure to create your file names.\nAs in the assessment marking example, having machine readable file names is particularly useful when managing files, such as ordering, finding, moving or deleting them. Another example of this is when your analysis requires you to load a large number of individual data files.\nMachine readable file names are also useful for extracting meta-information from files without having to open them in memory. This is particularly useful when the files might be too large to load into memory, or you only want to load data from a certain year.\nThe final benefit we list here is the scalability, reduction in drudgery and lowered risk for human error when operating on a very large number of files.\n\n\n2.2.3 Order Friendly\nThe next property we will focus on also links to how computers operate. We’d like our file names to exploit the default orderings used by computers. This means starting file names with character strings or metadata that allow us order our files in some meaningful way.\n\n2.2.3.1 Running Order\nOne example of this is where there’s some logical order in which your code should be executed, as in the example analysis below.\ndiagnositc-plots.R\ndownload.R\nruntime-comparison.R\n...\nmodel-evaluation.R\nwrangle.R\nPrepreding numbers to these file names can make the intended ordering immediately obvious.\n00_download.R\n01_wrangle.R\n02_model.R\n...\n09_model-evaluation.R\n10_model-comparison-plots.R\nStarting single digit numbers with a leading 0 is a very good idea here to prevent script 1 being sorted in with the tens, script 2 in with the twenties and so on. If you might have over 100 files, for example when saving the output from many simulations, use two or more zeros to maintain this nice ordering.\n\n\n2.2.3.2 Date Order\nA second example of orderable file names is when the file has a date associated with it. This might be a version of a report or the date on which some data were recorded, cleaned or updated.\n2015-10-22_human-resources-protocols.pdf\n...\n2022-effective-data-science-module-guide.docx\nWhen using dates, in file names or elsewhere, you should conform to the ISO standard date format.\n\nISO 8601 sets an international standard format for dates: YYYY-MM-DD.\n\nThis format uses four numbers for the year, followed by two numbers for the month and two numbers of the day of the month. This structure mirrors a nested file structure moving from least to most specific. It also avoids confusion over the ordering of the date elements. Without using the ISO standard a date like 04-05-22 might be interpreted as the fourth of May 2022, the fifth of April 2022, or the 22nd of May 2004.\n\n\n\n2.2.4 Human Readable\nThe final property we would like our file names to have is human readability. This requires the names of our files to be meaningful, informative and easily read by real people.\nThe first two of these are handled by including appropriate metadata in the file name. The ease with which these are read by real people is determined by the length of the file name and by how that name is formatted.\nThere are lots of formatting options with fun names like camelCase, PascalCase, and snake_case.\n   easilyReadByRealPeople (camelCase)\n   EasilyReadByRealPeople (PascalCase)\n   easily_read_by_real_people (snake_case)\n   easily-read-by-real-people (skewer-case)\nhere’s weak evidence that suggests snake and skewer cases are most the readable. We’ll use a mixture of these, using snake case between metadata items and skewer case within them. This has a slight cost to legibility, in a trade-off against making computing on these file names easier.\nThe final aspect that you have control over is the length of the name. Having short, evocative and useful file names is not easy and is a skill in itself. For some hints and tips you might want to look into tips for writing URL slugs. These are last part of a web address that are intended to improve accessibility by being immediately and intuitively meaningful to any user.\n\n\n2.2.5 Naming Files - Summary\n\nFile names should be meaningful, informative and scripts end in .r\nStick to letters, numbers underscores (_) and hyphens (-).\nPay attention to capitalisation file.r \\(\\neq\\) File.r on all operating systems.\nShow order with left-padded numbers or ISO dates."
  },
  {
    "objectID": "102-workflows-naming-files.html#file-extensions-and-where-you-work",
    "href": "102-workflows-naming-files.html#file-extensions-and-where-you-work",
    "title": "2  Naming Files",
    "section": "2.3 File Extensions and Where You Work",
    "text": "2.3 File Extensions and Where You Work\nSo far we have focused entirely on what comes before the dot, that is the file name.Equally, if not more, important is what comes after the dot, the file extension.\nexample-script.r\nexample-script.py\n\nproject-writeup.doc\nproject-writeup.tex\nThe file extension describes how information is stored in that file and determines the software that can use, view or run that file.\nYou likely already use file extensions to distinguish between code scripts, written documents, images, and notebook files. We’ll now explore the benefits and drawbacks of various file types with respect to several important features.\n\n2.3.1 Open Source vs Proprietary File Types\nThe first feature we’ll consider is whether the file type is open source, and can be used by anyone without charge, or if specialist software must be paid for in order to interact with those files.\n\n\n\n\n\n\n\n\n\nIn the figure above, each column represents a different class of file, moving left to right we have example file types for tabular data, list-like data and text documents. File types closer to the top are open source while those lower down rely on proprietary software, which may or may not require payment.\nTo make sure that our work is accessible to as many people as possible we should favour the open source options like csv files over Google sheets or excel, JSON files over Matlab data files, and tex or markdown over a word or Google doc.\nThis usually has a benefit in terms of project longevity and scalability. The open source file types are often somewhat simpler in structure, making them more robust to changes over time less memory intensive.\nTo see this, let’s take a look inside some data files.\n\n\n2.3.2 Inside Data Files\n\n2.3.2.1 Inside a CSV file\nCSV or comma separated value files are used to store tabular data.\nIn tabular data, each row of the data represents one record and each column represents a data value.\nA csv encodes this by having each record on a separate line and using commas to separate values with that record. You can see this by opening a csv file in a text editor such as notepad.\nThe raw data stores line breaks using \\n and indicates new rows by \\r. These backslashed indicae that these are escape characters with special meanings, and should not be literally interpreted as the letters n and r.\n\n#&gt; [1] \"Name,Number\\r\\nA,1\\r\\nB,2\\r\\nC,3\"\n\nWhen viewed in a text editor, the example file would look something like this.\nName,Number \nA,1\nB,2\nC,3\n\n\n2.3.2.2 Inside a TSV file\nTSV or tab separated value files are also used to store tabular data.\nLike in a csv each record is given on a new line but in a tsv tabs rather than commas are used to separate values with each record. This can also be seen by opening a tsv file in a text editor such as notepad.\n\n#&gt; [1] \"Name\\tNumber\\r\\nA\\t1\\r\\nB\\t2\\r\\nC\\t3\"\n\nName    Number \nA   1\nB   2\nC   3\nOne thing to note is that tabs are a separate character and are not just multiple spaces. In plain text these can be impossible to tell apart, so most text editors have an option to display tabs differently from repeated spaces, though this is usually not enabled by default.\n\n\n2.3.2.3 Inside an Excel file\nWhen you open an excel file in a text editor, you will immediately see that this is not a human interpretable file format.\n504b 0304 1400 0600 0800 0000 2100 62ee\n9d68 5e01 0000 9004 0000 1300 0802 5b43\n6f6e 7465 6e74 5f54 7970 6573 5d2e 786d\n6c20 a204 0228 a000 0200 0000 0000 0000\n0000 0000 0000 0000 0000 0000 0000 0000\n.... .... .... .... .... .... .... ....\n0000 0000 0000 0000 ac92 4d4f c330 0c86\nef48 fc87 c8f7 d5dd 9010 424b 7741 48bb\n2154 7e80 49dc 0fb5 8da3 241b ddbf 271c\n1054 1a83 0347 7fbd 7efc cadb dd3c 8dea\n.... .... .... .... .... .... .... ....\nEach entry here is a four digit hexadecimal number and there are a lot more of them than we have entries in our small table.\nThis is because excel files can carry a lot of additional information that a csv or tsv are not able to, for example cell formatting or having multiple tables (called sheets by excel) stored within a single file.\nThis means that excel files take up much more memory because they are carrying a lot more information than is strictly contained within the data itself.\n\n\n2.3.2.4 Indise a JSON file\nJSON, or Java Script Object Notation, files are an open source format for list-like data. Each record is represented by a collection of key:value pairs. In our example table each entry has two fields, one corresponding to the Name key and one corresponding to the Number key.\n[{\n    \"Name\": \"A\",\n    \"Number\": \"1\"\n}, {\n    \"Name\": \"B\",\n    \"Number\": \"2\"\n}, {\n    \"Name\": \"C\",\n    \"Number\": \"3\"\n}]\nThis list-like structure allows non-tabular data to be stored by using a property called nesting: the value taken by a key can be a single value, a vector of values or another list-like object.\nThis ability to create nested data structures has lead to this data format being used widely in a range of applications that require data transfer.\n\n\n2.3.2.5 Inside an XML file\nXML files are another open source format for list-like data, where each record is represented by a collection of key:value pairs.\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;root&gt;\n  &lt;row&gt;\n    &lt;Name&gt;A&lt;/Name&gt;\n    &lt;Number&gt;1&lt;/Number&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;Name&gt;B&lt;/Name&gt;\n    &lt;Number&gt;2&lt;/Number&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;Name&gt;C&lt;/Name&gt;\n    &lt;Number&gt;3&lt;/Number&gt;\n  &lt;/row&gt;\n&lt;/root&gt;\nThe difference from a JSON file is mainly in how those records are formatted within the file. In a JSON file this is designed to look like objects in the Java Script programming language and in XML the formatting is done to look like html, the markup language used to write websites.\n\n\n\n2.3.3 A Note on Notebooks\n\nThere are two and a half notebook formats that you are likely to use to: .rmd (alternatively .qmd) and .ipynb.\nR markdown documents .rmd are plain text files, so are very human friendly.\nJuPyteR notebooks have multi-language support but are not so human friendly (JSON in disguise).\nQuarto documents offer the best of both worlds and more extensive language support. Not yet as established as a format.\n\nIn addition to the files you read and write, the files that you code in will largely determine your workflow.\nThere are three main options for the way that you code: first is typing it directly at the command line, second is using a text editor or IDE to write scripts and third is writing a notebook that mixes code. text and output together in a single document.\nWe’ll compare these methods of working on the next slide, but first let’s do a quick review of what notebooks are available to you and why you might want to use them.\nAs a data scientist, there are two and a half notebook formats that you’re likely to have met before. The first two are Rmarkdown files for those working predominantly in R and interactive python or jupyter notebooks for those working predominantly in python. The final half format are quarto markdown documents, which are relatively new and extend the functionality of Rmarkdown files.\nThe main benefit of R markdown documents is that they’re plain text files, so they’re very human friendly. JuPyteR notebooks have the benefit of supporting code written in Julia, Python or R, but are not so human friendly - under the hood these documents are JSON files that should not be edited directly because a misplaced bracket will break them.\nQuarto documents offer the best of both worlds, with plain text formatting and even more extensive language support than jupyter notebooks. Quarto is a recent extension of Rmarkdown, which is rapidly becoming popular in the data science community.\nEach format has its benefits and drawbacks depending on the context in which they are used and all have some shared benefits and limitations by nature of them all being notebook documents.\n\n\n2.3.4 File Extensions and Where You Code\n\n\n\nProperty\nNotebook\nScript\nCommand Line\n\n\n\n\nreproducible\n~\n✓\nX\n\n\nreadable\n~\n✓\n~\n\n\nself-documenting\n✓\nX\nX\n\n\nin production\nX\n✓\n~\n\n\nordering / automation\n~\n✓\n~\n\n\n\nThe main benefit of notebook documents is that they are self-documenting, in that they can mix the documentation, code and report all into a single document. Notebooks also they provide a level of interactivity when coding that is not possible when working directly at the command line or using a text editor to write scripts. This second factor is easily overcome by using an integrated development environment when scripting.\nWriting code in .r files is not self-documenting but this separation of code, documentation and outputs has many other benefits. Firstly, the resulting scripts provide a reproducible and automatable workflow, unlike one-off lines of code being run at the command line. Secondly, using an IDE to write these provides you with syntax highlighting and code linting features to help you write readable and accurate code. Finally, the separation of code from documentation and output allows your work to be more easily or even directly put into production.\nIn this course we will advocate for a scripting-first approach to data science, though notebooks and command line work definitely have their place.\nNotebooks are great as teaching and rapid development tools but have strong limitations with being put into production. Conversely, coding directly at the command line can leave no trace of your workflow and lead to an analysis that cannot be replicated in the future.\n\n\n2.3.5 Summary\nFinally, let’s wrap things up by summarising what we have learned about naming files.\nBefore the dot we want to pick file names that machine readable, human friendly and play nicely with the default orderings provided to us.\n\nName files so that they are:\n\nMachine Readable,\nHuman Readable,\nOrder Friendly.\n\n\nAfter the dot, we want to pick file types that are widely accessible, easily read by humans and allow for our entire analysis to be reproduced.\n\nUse document types that are:\n\nWidely accessible,\nEasy to read and reproduce,\nAppropriate for the task at hand.\n\n\nAbove all we want to name our files and pick our file types to best match with the team we are working in and the task that we is at hand."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#introduction",
    "href": "103-workflows-organising-your-code.html#introduction",
    "title": "3  Code",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nWe have already described how we might organise an effective data science project at the directory and file level. In this chapter we will delve one step deeper and consider how we can structure our work within those files. In particular, we’ll focus on code files here.\nWe’ll start by comparing the two main approaches to structuring our code, namely functional programming and object oriented programming.\nWe’ll then see how we should order code within our scripts and conventions on how to name the functions and objects that we work with in our code.\nRounding up this chapter, I’ll summarise the main points from the R style guide that we will be following in this course and highlight some useful packages for writing effective code."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#functional-programming",
    "href": "103-workflows-organising-your-code.html#functional-programming",
    "title": "3  Code",
    "section": "3.2 Functional Programming",
    "text": "3.2 Functional Programming\nA functional programming style has two major properties:\n\nObject immutability,\nComplex programs written using function composition.\n\nFirstly, the original data or objects aren’t modified or altered by the code. We have met this idea before when making new, cleaner versions of our raw data but leave that original messy data intact. Object immutability is the exact same idea but in a code context rather than data context.\nSecondly, In functional programming, complex problems are solved by decomposing them into a series of smaller problems. A separate, self-contained function is then written to solve each sub-problem. Each individual function is, in itself, simple and easy to understand. This makes these small functions easy to test and easy to reuse in many places. Code complexity is then built up by composing these functions in various ways.\nIt can be difficult to get into this way of thinking, but people with mathematical training often find it quite natural. This is because mathematicians have many years of experience in working with function compositions in the abstract, mathematical sense.\n\\[y = g(x) = f_3 \\circ f_2 \\circ f_1(x).\\]\n\n3.2.1 The Pipe Operator\nOne issue with functional programming is that lots of nested functions means that there are also lots of nested brackets. These start to get tricky to keep track of when you have upwards of 3 functions being composed. This reading difficulty is only exacerbated if your functions have additional arguments on top of the original inputs.\n\n#&gt; [1] 1\n\nThe pipe operator %&gt;% from the {magrittr} package helps with this issue. It works exactly like function composition: it takes the whatever is on the left (whether that is an object or the output of a function) and passes it to the following function call as the first argument of that function.\n\n#&gt; [1] 1\n\n\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n\nThe pipe operator is often referred to as “syntactic sugar”. This is because it doesn’t add anything to your code in itself but rather it makes your code so much more palatable.\nIn R versions 4.1 and greater, there’s a built-in version of this pipe operator, which is written using the vertical bar symbol followed by a greater than sign. To type the vertical bar, you can usually find it found above backslash on the keyboard.\n(Just to cause confusion, the vertical bar symbol is also called the pipe symbol in general programming contexts. )\n\n#&gt; [1] 1\n\n\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n\nThe base R pipe usually behaves in the same way as the pipe from magrittr, but there are a few cases where they differ. For reasons of back-compatibility and consistency we’ll stick to the magrittr pipe in this course.\n\n\n3.2.2 When not to pipe\n\n\n\nPipes are designed to put focus on the the actions you are performing rather than the object that you are preforming those operations on. This means that there are two cases where you should almost certainly not use a pipe.\nThe first of these is when you need to manipulate more than one object at a time. Using secondary objects as reference points (but leaving them unchanged) is of course perfectly fine, but pipes should be used when applying a sequence of steps to create a new, modified version of one primary object.\nSecondly, just because you can chain together many actions into a single pipeline, that doesn’t mean that you necessarily should. Very long sequences of piped operations are easier to read than nested functions, however they still burden the reader with the same cognitive load on their short term memory. Be kind and create meaningful, intermediate objects with informative names. This’ll help the reader to more easily understand the logic within your code."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#object-oriented-programming",
    "href": "103-workflows-organising-your-code.html#object-oriented-programming",
    "title": "3  Code",
    "section": "3.3 Object Oriented Programming",
    "text": "3.3 Object Oriented Programming\nThe main alternative to functional programming is object oriented programming.\n\nSolve problems by using lots of simple objects\nR has 3 OOP systems: S3, S4 and R6.\nObjects belong to a class, have methods and fields.\nExample: agent based simulation of beehive.\n\n\n3.3.1 OOP Philosophy\nIn functional programming, we solve complicated problems by using lots of simple functions. In object oriented programming we solve complicated problems using lots of simple objects. Which of these programming approaches is best will depend on the particular type of problem that you are trying to solve.\nFunctional programming is excellent for most types of data science work. Object oriented comes into its own when your problem has many small components interacting with one another. This makes it great for things like designing agent-based simulations, which I’ll come back to in a moment.\nIn R there are three different systems for doing object oriented programming (called S3, S4, and R6), so things can get a bit complicated. We won’t go into detail about them here, but I’ll give you an overview of the main ideas.\nThis approach to programming might be useful for you in the future, for example if you want to extend base R functions to work with new types of input, and to have user-friendly displays. In that case (Advanced R)[https://adv-r.hadley.nz/] by Hadley Wickham is an excellent reference text.\nIn OOP, each object belongs to a class and has a set of methods associated with it. The class defines what an object is and methods describe what that object can do. On top of that, each object has class-specific attributes or data fields. These fields are shared by all objects in a class but the values that they take give information about that specific object.\n\n\n3.3.2 OOP Example\n\n\n\nThis is is all sounding very abstract. Let’s consider writing some object oriented code to simulate a beehive. Each object will be a bee, and each bee is an instance of one of three bee classes: it might be a queen, a worker or a drone for example. Different bee classes have different methods associated with them, which describe what the bee can do, for example all bees would have 6 methods that let them move up, down, left, right, forward and backward within the hive. An additional “reproduce” method might only be defined for queen bees and a pollinate method might only be defined for workers. Each instance of a bee has its own fields, which give data about that specific bee. All bees have x, y and z coordinate fields giving their location within the hive. The queen class might have an additional field for their number of offspring and the workers might have an additional field for how much pollen they are carrying.\nAs the simulation progresses, methods are applied to each object altering their fields and potentially creating or destroying objects. This is very different from the preservation mindset of functional programming, but hopefully you can see that it is a very natural approach to many types of problem."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#structuring-r-script-headers",
    "href": "103-workflows-organising-your-code.html#structuring-r-script-headers",
    "title": "3  Code",
    "section": "3.4 Structuring R Script Headers",
    "text": "3.4 Structuring R Script Headers\n\nTL;DR - Start script with a comment of 1-2 sentences explaining what it &gt; does. - setwd() and rm(ls()) are the devil’s work. - “Session” &gt; “Restart R” or Keyboard shortcut: crtl/cmd + shift &gt; + 0 - Polite to gather all library() and source() calls. - Rude to mess with other people’s set up using &gt; install.packages(). - Portable scripts use paths relative to the root directory of the project.\n\nFirst things first, let’s discuss what should be at the top of your R scripts.\nIt is almost always a good idea to start your file with a few commented out sentences describing the purpose of the script and, if you work in a large team, perhaps who contact with any questions about this script. (There is more on comments coming up soon, don’t worry!)\nIt is also good practise to move all library() and source() calls to the top of your script. These indicate the packages and helper function that are dependencies of your script; it’s useful to know what you need to have installed before trying to run any code.\nThat segues nicely to the next point, which is never to hard code package installations. It is extremely bad practise and very rude to do so because then your script might alter another person’s R installation. If you don’t know already, this is precisely the difference between an install.packages() and library() call: install.packages() will download the code for that package to the users computer, while library() takes that downloaded code and makes it available in the current R session. To avoid messing with anyone’s R installation, you should always type install.package() commands directly in the console and then place the corresponding library() calls within your scripts.\nNext, it is likely that you, or someone close to you, will commit the felony of starting every script by setting the working directory and clearing R’s global environment. This is very bad practice, it’s indicative of a workflow that’s not project based and it’s problematic for at least two reasons. Firstly, the path you set will likely not work on anyone else’s computer. Secondly, clearing the environment like this may look like it gets you back to fresh, new R session but all of your previously loaded packages will still be loaded and lurking in the background.\nInstead, to achieve your original aim of starting a new R session, go to the menu and select the “Session” drop down then select “Restart R”. Alternatively, you can use keyboard shortcuts to do the same. This is “crtl + shift + 0” on Windows and “cmd + shift + 0” on a mac. The fact that a keyboard shortcut exists for this should quite strongly hint that, in a reproducible and project oriented workflow, you should be restarting R quite often in an average working day. This is the scripting equivalent of “clear all output and rerun all” in a notebook.\nFinally, let’s circle back to the point I made earlier about setting the working directory. The reason that this will not work is because you are likely giving file paths that are specific to your computer, your operating system and your file organisation system. The chances of someone else having all of these the same are practically zero."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#portable-file-paths-with-here",
    "href": "103-workflows-organising-your-code.html#portable-file-paths-with-here",
    "title": "3  Code",
    "section": "3.5 Portable File paths with {here}",
    "text": "3.5 Portable File paths with {here}\n\n# Bad - breaks if project moved\nsource(\"zaks-mbp/Desktop/exciting-new-project/src/helper_functions/rolling_mean.R\")\n\n# Better - breaks if Windows\nsource(\"../../src/helper_functions/rolling_mean.R\")\n\n# Best - but use here:here() to check root directory correctly identified\nsource(here::here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n# For more info on the here package:\nvignette(\"here\")\n\nTo fix the problem of person- and computer-specific file paths you can have two options.\nThe first is to use relative file paths. In this you assume that each R script is being run in its current location and my moving up and down through the levels of your project directory you point to the file that you need.\nThis is good in that it solves the problem of paths breaking because you move the project to a different location on your own laptop. However, it does not fully solve the portability problem because you might move your file to a different location within the same project. It also does not solve the problem that windows uses MacOS and linux use forward slashes in file paths with widows uses backslashes.\nTo resolve these final two issues I recommend using the here() function from the {here} package. This package looks for a .Rproj or .git file to identify the root directory of your project and creates file paths relative to the root of your project, that are suitable for the operating system the code is being run on.\nIt really is quite marvellous. For more information on how to use the here package, explore its chapter in R - What They Forgot, R for Data Science or this project oriented workflow blog post."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#code-body",
    "href": "103-workflows-organising-your-code.html#code-body",
    "title": "3  Code",
    "section": "3.6 Code Body",
    "text": "3.6 Code Body\nMoving on now, we will go from the head to the body of the code. Having well named and organised code will facilitate both reading and understanding. Comments and sectioning do the rest of this work.\nThis section is designed as an introduction to the tidyverse style guide and not as a replacement to it. ### Comments\n\n# This is an example script showing good use of comments and sectioning \n\nlibrary(here)\nsource(here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n#===============================================================================  &lt;- 80 characters max for readability\n# Major Section on Comments ----\n#===============================================================================\n\n#-------------------------------------------------------------------------------\n##  Minor Section on inline comments ---- \n#-------------------------------------------------------------------------------\nx &lt;- 1:10 # this is an inline comment\n\n#-------------------------------------------------------------------------------\n##  Minor Section on full line comments ---- \n#-------------------------------------------------------------------------------\nrolling_mean(x)\n# This is an full line comment\n\nComments may be either short in-line comments at the end of a line or full lines dedicated to comments. To create either type of comment in R, simply type hash followed by one space. The rest of that line will not be evaluated and will function as a comment. If multi-line comments are needed simply start multiple lines with a hash and a space.\nThe purpose of these comments is to explain the why of what you are doing, not the what. If you are explaining what you are doing in most of your comments then you perhaps need to consider writing more informative function names, something we will return to in the general advice section.\nComments can also be used to add structure to your code, buy using commented lines of hyphens and equal signs to chunk your files into minor and major sections.\nMarkdown-like section titles can be added to these section and subsection headers. Many IDEs, such as RStudio, will interpret these as a table of contents for you, so that you can more easily navigate your code.\n\n3.6.1 Objects are Nouns\n\nObject names should use only lowercase letters, numbers, and _.\nUse underscores (_) to separate words within a name. (snake_case)\nUse nouns, preferring singular over plural names.\n\n\n# Good\nday_one\nday_1\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1\n\nWhen creating and naming objects a strong guideline in that objects should be named using short but meaningful nouns. Names should not include any special characters and should use underscores to separate words within the object name.\nThis is similar to our file naming guide, but note that hyphens can’t be used in object names because this conflicts with the subtraction operator.\nWhen naming objects, as far as possible use singular nouns. The main reason for this is that the plurisation rules in English are complex and will eventually trip up either you or a user of your code.\n\n\n3.6.2 Functions are Verbs\n\nFunction names should use only lowercase letters, numbers, and _.\nUse underscores (_) to separate words within a name. (snake_case)\nSuggest imperative mood, as in a recipe.\nBreak long functions over multiple lines. 4 vs 2 spaces.\n\n\n# Good\nadd_row()\npermute()\n\n# Bad\nrow_adder()\npermutation()\n\nlong_function_name &lt;- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\nThe guidelines for naming functions are broadly similar, with the advice that functions should be verbs rather than nouns.\nFunctions should be named in the imperative mood, like in a recipe. This is again for consistency; having function names in a range of moods and tenses leads to coding nightmares.\nAs with object names you should aim to give your functions and their arguments short, evocative names. For functions with many arguments or a long name, you might not be able to fit the function definition on a single line. In this case you can should place each argument on its own double indented line and the function body on a single indented line.\n\n\n3.6.3 Casing Consistantly\nAs we have mentioned already, we have many options for separating words within names:\n\nCamelCase\npascalCase\nsnakecase\nunderscore_separated ❤️\nhyphen-separated\npoint.separated 💀\n\nFor people used to working in Python it is tempting to use point separation in function names, in the spirit of methods from object oriented programming. Indeed, some base R functions even use this convention.\nHowever, the reason that we advise against it is because it is already used for methods in some of R’s inbuilt OOP functionality. We will use underscore separation in our work.\n\n\n3.6.4 Style Guide Summary\n\nUse comments to structure your code\nObjects = Nouns\nFunctions = Verbs\nUse snake case and consistant grammar"
  },
  {
    "objectID": "103-workflows-organising-your-code.html#further-tips-for-friendly-coding",
    "href": "103-workflows-organising-your-code.html#further-tips-for-friendly-coding",
    "title": "3  Code",
    "section": "3.7 Further Tips for Friendly Coding",
    "text": "3.7 Further Tips for Friendly Coding\nIn addition to naming conventions the style guide gives lots of other guidance on writing code in a way that is kind to future readers of that code.\nI’m not going to go repeat all of that guidance here, but the motivation for all of these can be boiled down into the following points.\n\nWrite your code to be easily understood by humans.\nUse informative names, typing is cheap.\n\n\n# Bad\nfor(i in dmt){\n  print(i)\n}\n\n# Good\nfor(temperature in daily_max_temperature){\n  print(temperature)\n}\n\n\nDivide your work into logical stages, human memory is expensive.\n\nWhen writing your code, keep that future reader in mind. This means using names that are informative and reasonably short, it also means adding white space, comments and formatting to aid comprehension. Adding this sort of structure to your code also helps to reduce the cognitive burden that you are placing on the human reading your code.\nInformative names are more important than short names. This is particularly true when using flow controls, which are things like for loops and while loops. Which of these for loops would you like to encounter when approaching a deadline or urgently fixing a bug? Almost surely the second one, where context is immediately clear.\nA computer doesn’t care if you call a variable by only a single letter, by a random key smash (like aksnbioawb) or by an informative name. A computer also doesn’t care if you include no white space your code - the script will still run. However, doing these things are friendly practices that can help yourself when debugging and your co-workers when collaborating."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#reduce-reuse-recycle",
    "href": "103-workflows-organising-your-code.html#reduce-reuse-recycle",
    "title": "3  Code",
    "section": "3.8 Reduce, Reuse, Recycle",
    "text": "3.8 Reduce, Reuse, Recycle\nIn this final section, we’ll look at how you can make your workflow more efficient by reducing the amount of code you write, as well as reusing and recycling code that you’ve already written.\n\n3.8.1 DRY Coding\nThis idea of making your workflow more efficient by reducing, reusing and recycling your code is summarised by the DRY acronym: don’t repeat yourself.\nThis can be boiled down to three main points:\n\n\nif you do something twice in a single script, then write a function to do that thing,\nif you want to use your function elsewhere within your project, then save it in a separate script\nIf you want to use your function across projects, then add it to a package.\n\n\nOf course, like with scoping projects in the first place, this requires some level of clairvoyance: you have to be able to look into the future and see whether you’ll use a function in another script or project. This is difficult, bordering on impossible. So in practice, this is done retrospectively - you find a second script or project that needs a function then pull it out its own separate file or include it in a package.\nAs a rule of thumb, if you are having to consider whether or not to make the function more widely available then you should do it. It takes much less effort to do this work now, while it’s fresh in your mind, than to have to refamiliarise yourself with the code in several years time.\nLet’s now look at how to implement those sub-bullet points: “when you write a function, document it” and “when you write a function, test it”.\n\n\n3.8.2 Rememer how to use your own code\nWhen you come to use a function written by somebody else, you likely have to refer to their documentation to teach or to remind yourself of things like what the expected inputs are and how exactly the method is implemented.\nWhen writing your own functions you should create documentation that fills the same need. Even if the function is just for personal use, over time you’ll forget exactly how it works.\n\nWhen you write a function, document it.\n\nBut what should that documentation contain?\n\nInputs\nOutputs\nExample use cases\nAuthor (if not obvious or working in a team)\n\nYour documentation should describe the inputs and outputs of your function, some simple example uses. If you are working in a large team, the documentation should also indicate who wrote the function and who’s responsible for maintaining it over time.\n\n\n3.8.3 {roxygen2} for documentation\nIn the same way that we used the {here} package to simplify our file path problems, we’ll use the {roxygen2} package to simplify our testing workflow.\nThe {roxygen2} package gives us an easily insert-able temple for documenting our functions. This means we don’t have to waste our time and energy typing out and remembering boilerplate code. It also puts our documentation in a format that allows us to get hints and auto-completion for our own functions, just like the functions we use from packages that are written by other people.\nTo use Roxygen, you only need to install it once - it doesn’t need to be loaded with a library call at the top of your script. After you’ve done this, and with your cursor inside a function definition, you can then insert skeleton code to document that function in one of two ways: you can either use the Rstudio menu or the keyboard short cut for your operating system.\n\ninstall.packages(\"roxygen2\")\nWith cursor inside function: Code &gt; Insert Roxygen Skeleton\nKeyboard shortcut: cmd + option + shift + r or crtl + option + shift + r\nFill out relevant fields\n\n\n\n3.8.4 An {roxygen2} example\nBelow, we’ve got an example of an Roxygen skeleton to document a function that calculates the geometric mean of a vector. Here, the hash followed by an apostrophe is a special type of comment. It indicates that this is function documentation rather than just a regular comment.\n\n#' Title\n#'\n#' @param x \n#' @param remove_NA \n#'\n#' @return\n#' @export\n#'\n#' @examples\ngeometric_mean &lt;- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n\nWe’ll fill in all of the fields in this skeleton apart from export, which we’ll remove. If we put this function in a R package, then the export field makes it available to users of that package. Since this is just a standalone function we won’t need the export field, though keeping it wouldn’t actually cause us any problems either.\n\n#' Calculate the geometric mean of a numeric vector\n#'\n#' @param x numeric vector\n#' @param remove_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. \n#'\n#' @return the geometric mean of the values in `x`, a numeric scalar value. \n#'\n#' @examples\n#' geometric_mean(x = 1:10)\n#' geometric_mean(x = c(1:10, NA), remove_NA = TRUE)\n#' \ngeometric_mean &lt;- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n\nOnce we have filled in the skeleton documentation it might look something like this. We have described what the function does, what the expected inputs are and what the user can expect as an output. We’ve also given an few simple examples of how the function can be used.\nFor more on Roxygen, see the package documentation or the chapter of R packages on function documentation.\n\n\n3.8.5 Checking Your Code\n\nIf you write a function, test it.\n\nTesting code has two main purposes:\n\nTo warn or prevent user misuse (e.g. strange inputs),\nTo catch edge cases.\n\nOn top of explaining how our functions should work, we really ought to check that they do work. This is the job of unit testing.\nWhenever you write a function you should test that it works as you intended it to. Additionally, you should test that your function is robust to being misused by the user. Depending on the context, this might be accidental or malicious misuse. Finally, you should check that the function behaves properly for strange, but still valid, inputs. These are known as edge cases.\nTesting can be a bit of a brutal process, you’ve just created a beautiful function and now you’re job is to do your best to break it!\n\n\n3.8.6 An Informal Testing Workflow\n\nWrite a function\nExperiment with the function in the console, try to break it\nFix the break and repeat.\n\nProblems: Time consuming and not reproducible.\nAn informal approach to testing your code might be to first write a function and then play around with it in the console to check that it behaves well when you give it obvious inputs, edge cases and deliberately wrong inputs. Each time you manage to break the function, you edit it to fix the problem and then start the process all over again.\nThis is testing the code, but only informally. There’s no record of how you have tried to break your code already. The problem with this approach is that when you return to this code to add a new feature, you’ll probably have forgotten at least one of the informal tests you ran the first time around. This goes against our efforts towards reproducibility and automation. It also makes it very easy to break code that used to work just fine.\n\n\n3.8.7 A Formal Testing Workflow\nWe can formalise this testing workflow by writing our tests in their own R script and saving them for future reference. Remember from the first lecture that these should be saved in the tests/ directory, the structure of which should mirror that of the src/ directory for your project. All of the tests for one function should live in a single file, which is named after that function.\nOne way of writing these tests is to use lots of if statements. The {testthat} can do some of that syntactic heavy lifting for us. It has lots of helpful functions to test that the output of your function is what you expect.\n\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1, NA), remove_NA = FALSE),\n  expected = NA)\n\n# Error: geometric_mean(x = c(1, NA), remove_NA = FALSE) not equal to NA.\n# Types not compatible: double is not logical\n\nIn this example, we have an error because our function returns a logical NA rather than a double NA. Yes, R really does have different types of NA for different types of missing data, it usually just handles these nicely in the background for you.\nThis subtle difference is probably not something that you would have spotted on your own, until it caused you trouble much further down the line. This rigorous approach is one of the benefits of of using the {testthat} functions.\nTo fix this test we change out expected output to NA_real_.\nWe’ll revisit the {testthat} package in the live session this week, when we will learn how to use it to test functions within our own packages."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#summary",
    "href": "103-workflows-organising-your-code.html#summary",
    "title": "3  Code",
    "section": "3.9 Summary",
    "text": "3.9 Summary\n\nFunctional and Object Oriented Programming\nStructuring your scripts\nStyling your code\nReduce, reuse, recycle\nDocumenting and testing\n\nLet’s wrap up by summarising what we have learned in this chapter.\nWe started out with a discussion on the differences between functional and object oriented programming. While R is capable of both, data science work tends to have more of a functional flavour to it.\nWe’ve then described how to structure your scripts and style your code to make it as human-friendly and easy to debug as possible.\nFinally, we discussed how to write DRY code that is well documented and tested."
  },
  {
    "objectID": "110-workflows-checklist.html#videos-chapters",
    "href": "110-workflows-checklist.html#videos-chapters",
    "title": "Workflows Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nOrganising your work (30 min) [slides]\nNaming Files (20 min) [slides]\nOrganising your code (27 min) [slides]"
  },
  {
    "objectID": "110-workflows-checklist.html#reading",
    "href": "110-workflows-checklist.html#reading",
    "title": "Workflows Checklist",
    "section": "Reading",
    "text": "Reading\nUse the workflows section of the reading list to support and guide your exploration of this week’s materials. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "110-workflows-checklist.html#tasks",
    "href": "110-workflows-checklist.html#tasks",
    "title": "Workflows Checklist",
    "section": "Tasks",
    "text": "Tasks\nCore:\n\nFind 3 data science projects on Github and explore how they organise their work. Write a post on the EdStem forum that links to all three, and in a couple of paragraphs describe the content and structure of one project.\nCreate your own project directory (or directories) for this course and its assignments.\nWrite two of your own R functions. The first should calculate the geometric mean of a numeric vector. The second should calculate the rolling arithmetic mean of a numeric vector.\n\nBonus:\n\nRe-factor an old project to match the project organisation and coding guides for this course. This might be a small research project, class notes or a collection of homework assignments. Use an R-based project if possible. If you only have python projects, then either translate these to R or apply the PEP8 style guide. Take care to select a suitably sized project so that this is a meaningful exercise but does not take more than a few hours.\nIf you are able to do so, host your re-factored project publicly and share it with the rest of the class on the EdStem Discussion forum."
  },
  {
    "objectID": "110-workflows-checklist.html#live-session",
    "href": "110-workflows-checklist.html#live-session",
    "title": "Workflows Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then create a minimal R package to organise and test the functions you have written.\nPlease come to the live session prepared to discuss the following points:\n\nDid you make the assignment projects as subdirectories or as their stand alone projects? Why?\nWhat were some terms that you had not met before during the readings? How did you find their meanings?\nWhat did you have to consider when writing your rolling mean function?"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]