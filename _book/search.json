[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective Data Science",
    "section": "",
    "text": "Model building and evaluation are are necessary but not sufficient skills for the effective practice of data science. In this module you will develop the technical and personal skills that are required to work successfully as a data scientist within an organisation.\n\nDuring this module you will critically explore how to:\n\neffectively scope and manage a data science project;\nwork openly and reproducibly;\nefficiently acquire, manipulate, and present data;\ninterpret and explain your work for a variety of stakeholders;\nensure that your work can be put into production;\nassess the ethical implications of your work as a data scientist.\n\nThis interdisciplinary course will draw from fields including statistics, computing, management science and data ethics. Each topic will be investigated through a selection of lecture videos, conference presentations and academic papers, hands-on lab exercises, along with readings on industry best-practices from recognised professional bodies.\n\nThese notes are intended for students on the course MATH70076: Data Science in the academic year 2023/24.\nAs the course is scheduled to take place over five weeks, the suggested schedule is:\n\n1st week: effective data science workflows;\n2nd week: acquiring and sharing data;\n3rd week: exploratory data analysis and visualisation;\n4th week: preparing for production;\n5th week: ethics and context of data science.\n\nA pdf version of these notes may be downloaded here. Please be aware that these are very rough and will be updated less frequently than the course webpage.\n\nOn successful completion of this module students should be able to:\n\nIndependently scope and manage a data science project;\nSource data from the internet through web scraping and APIs;\nClean, explore and visualise data, justifying and documenting the decisions made;\nEvaluate the need for (and implement) approaches that are explainable, reproducible and scalable;\nAppraise the ethical implications of a data science projects, particularly the risks of compromising privacy or fairness and the potential to cause harm.\n\nLectures: 10 Hours (2 hours per week)\nGroup Teaching: 5 Hours (1 hour per week)\nLab / Practical: 10 hours (2 hours per week)\nIndependent Study: 100 hours (15 hours per week + 30 hours coursework)\nDrop-In Sessions: Each week there will be a 1-hour optional drop-in session to address any questions about the course or material. This is where you can get support from the course lecturer or GTA on the topics covered each week, individually or in small groups.\nOffice Hours: Additionally, there will be an office hour each week. This is a weekly opportunity for 1-1 discussion with the course lecturer to address any individual questions, concerns or problems that you might have. These meetings can be in person or on Teams and can be academic (relating to course content or progress) or pastoral (relating to student well-being) in nature. To book a 1-1 meeting please use the link on the course blackboard page.\n\nThe course will be assessed entirely by coursework, reflecting the practical and pragmatic nature of the course material.\nCoursework 1 (30%): To be completed during the fourth week of the course.\nCoursework 2 (70%): To be released in the last week of the course and submitted following the examination period in Summer term.\n\nThese notes were created by Dr Zak Varty. They were inspired by a previous lecture series by Dr Purvasha Chakravarti at Imperial College London and draw from many resource that were made available by the R community."
  },
  {
    "objectID": "100-workflows-introduction.html",
    "href": "100-workflows-introduction.html",
    "title": "Effective Workflows",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nAs a data scientist you will never work alone.\nWithin a single project a data scientist is likely that you will interact with a range of other people, including but not limited to: one or more project managers, stakeholders and subject matter experts. These experts might come from a single specialism or form a multidisciplinary team, depending on the type of work that you are doing.\nTo get your project put into use and working at scale you will likely have to collaborate with data engineers. You will also work closely with other data scientists, to review one another’s work or to collaborate on larger projects.\nFamiliarity with the skills, processes and practices that make for collaboration is instrumental to being a successful as a data scientist. The aim for this part of the course is to provide you with a structure on how you organise and perform your work, so that you can be a good collaborator to current colleges and your future self.\nThis is going to require a bit more effort upfront, but the benefits will compound over time. You will get more done by wasting less time staring quizzically at messy folders of indecipherable code. You will also gain a reputation of someone who is good to work with. This promotes better professional relationships and greater levels of trust, which can in turn lead to working on more exciting and impactful projects."
  },
  {
    "objectID": "101-workflows-organising-your-work.html",
    "href": "101-workflows-organising-your-work.html",
    "title": "1  Organising your work",
    "section": "",
    "text": "source(\"_common.R\")\nstatus(\"polishing\")\nWelcome to this course on effective data science. This week we’ll be considering effective data science workflows. These workflows are ways of progressing a project that will help you to produce high quality work and help to make you a good collaborator.\nIn this Chapter, we’ll kick things off by looking at how you can structure data science projects and organize your work. Familiarity with these skills, processes and practices for collaborative working are going to be instrumental as you become a successful data scientist."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#what-are-we-trying-to-do",
    "href": "101-workflows-organising-your-work.html#what-are-we-trying-to-do",
    "title": "1  Organising your work",
    "section": "\n1.1 What are we trying to do?",
    "text": "1.1 What are we trying to do?\nFirst, let’s consider why we want to provide our data science projects with some sense of structure and organization.\nAs a data scientist you’ll never work alone. Within a single project you’ll interact with a whole range of other people. This might be a project manager, one or more business stakeholders or a variety of subject matter experts. These experts might be trained as sociologists, chemists, or civil servants depending on the exact type of data science work that you’re doing.\nTo then get your project put into use and working at scale you’ll have to collaborate with data engineers. You’ll also likely work closely with other data scientists. For smaller projects this might be to act as reviewers for one another’s work. For larger projects working collaboratively will allow you to tackle larger challenges. These are the sorts of project that wouldn’t be feasible alone, because of the inherent limitations on the time and skill of any one individual person.\nEven if you work in a small organization, where you’re the only data scientist, then adopting a way of working that’s focused on collaborating will pay dividends over time. This is because when you inevitably return to the project that you’re working on in several weeks or months or years into the future you’ll have forgotten almost everything of what you did the first time around. You’ll also have forgotten why you made the decisions that you did and what other potential options there were that you didn’t take.\nThis is exactly like working with a current colleague who has shoddy or poor working practices. Nobody wants to be that colleague to somebody else, let alone to their future self. Even when working alone, treating your future self as a current collaborator (and one that you want to get along well with) makes you a kind colleague and a pleasure to work with.\nThe aim of this week is to provide you with a guiding structure on how you organize and perform your work. None of this is going to be particularly difficult or onerous. However it will require a bit more effort up front and daily discipline. Like with flossing, the daily effort required is not large but the benefits will compound over time.\nYou’ll get more done by wasting less time staring quizzically at a mess of folders and indecipherable code. You’ll also get a reputation as someone who’s well organized and good to work with. This promotes better professional relationships and greater levels of trust within your team. These can then, in turn, tead to you working on more exciting and more impactful projects in the future."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#an-r-focused-approach",
    "href": "101-workflows-organising-your-work.html#an-r-focused-approach",
    "title": "1  Organising your work",
    "section": "\n1.2 An R Focused Approach",
    "text": "1.2 An R Focused Approach\nThe structures and workflows that are recommend here and throughout the rest of this module are focused strongly on a workflow that predominantly uses R, markdown and LaTeX.\nSimilar techniques, code and software can achieve the same results that I show you here when coding in Python or C, or when writing up projects in Quarto or some other markup language. Similarly, different organizations have their own variations on these best practices that we’ll go through together. Often organisations will have extensive guidance on these topics.\nThe important thing is that once you understand what good habits are and have build them in one programming language or business, then transferring these skills to a new setting is largely a matter of learning some new vocabulary or slightly different syntax.\nWith that said, let’s get going!"
  },
  {
    "objectID": "101-workflows-organising-your-work.html#one-project-one-directory",
    "href": "101-workflows-organising-your-work.html#one-project-one-directory",
    "title": "1  Organising your work",
    "section": "\n1.3 One Project = One Directory",
    "text": "1.3 One Project = One Directory\nIf there’s one thing you should take away from this chapter, it’s this one Golden Rule:\n\nEvery individual project you work on as a data scientist should be in a single, self-contained directory or folder.\n\nThis is worth repeating. Every single project that you work on should be self-contained and live in a single directory. An analogy here might be having a separate ring-binder folder for each of your modules on a degree program.\n\n\n\n\n\n\n\n\nThis one golden rule is deceptively simple.\nThe first issue here is that it requires a predetermined scope of what is and what isn’t going to be covered by this particular project. This seems straightforward but at the outset of the project you often do not know exactly where your project will go, or how it will link to other pieces of work within your organization.\nThe second issue is the second law of Thermodynamics, which applies equally well to project management as it does to the heatdeath of the universe. It takes continual external effort to prevent the contents of this one folder from becoming chaotic and disordered over time.\nThat being said, having a single directory has several benefits which more than justify this additional work."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#properties-of-a-well-orgainsed-project",
    "href": "101-workflows-organising-your-work.html#properties-of-a-well-orgainsed-project",
    "title": "1  Organising your work",
    "section": "\n1.4 Properties of a Well-Orgainsed Project",
    "text": "1.4 Properties of a Well-Orgainsed Project\nWhat are the properties that we would like this single, well-organized project to have? Ideally, we’d like to organize our projects so that I have the following properties:\n\nPortable\nVersion Control Friendly\nReproducible\nIDE friendly.\n\nDon’t worry if you haven’t heard of some of these terms already. We’re going to look at each of them in a little bit of detail.\n\n1.4.1 Portability\nA project is said to be portable if it can be easily moved without breaking.\n\n\n\n\n\n\n\n\nThis might be a small move, like relocating the directory to a different location on your own computer. It might also mean a moderate move, say to another machine if yours dies just before a big deadline. Alternatively, it might be a large shift - to be uses by another person who is using a different operating system.\nFrom this thought experiment you can see that there’s a full spectrum of how portable a project may or may not need to be.\n\n1.4.2 Version Control Friendly\nA project under Version Control has all changes tracked either manually or automatically. This means that snapshots of the project are taken regularly as it gradually develops and evolves over time. Having these snapshots as many, incremental changes are made to the project allow it to be rolled back to a specific previous state if something goes wrong.\nA version controlled pattern of working helps to avoid the horrendous state that we have all found ourselves in - renaming final_version.doc to final_final_version.doc and so on.\nBy organising your workflow around incremental changes helps you to acknowledge that no work is ever finally complete. There will always be small changes that need to be done in the future.\n\n1.4.3 Reproducibility\n\nA study is reproducible if you can take the original data and the computer code used to analyze the data and recreate all of the numerical findings from the study. \nBroman et al (2017). “Recommendations to Funding Agencies for Supporting Reproducible Research”\n\nIn their paper, Broman et al define reproducibility as a project where you can take the original data and code used to perform the analysis and using these we create all of the numerical findings of the study.\nThis definition leads naturally to several follow-up questions.\nWho exactly is you in this definition? Does it specifically mean yourself in the future or should someone else with access to all that data and code be able to recreate your findings too? Also, should this reproducibility be limited to just the numerical results? Or should they also be able to create the associated figures, reports and press releases?\nAnother important question is when this project needs to be reproduced. Will it be in a few weeks time or in 10 years time? Do you need to protect your project from changes in dependencies, like new versions of packages or modules? How about different versions of R or Python? Taking this time scale out even further, what about different operating systems and hardware?\nIt’s unlikely you’d consider someone handing you a floppy disk of code that only runs on Windows XP to be acceptably reproducible. Sure, you could probably find a way to get it to work, but that would be an awful lot of effort on your end.\nThat’s perhaps a bit of an extreme example, but it emphasizes the importance of clearly defining the level of reproducibility that you’re aiming for within every project you work on. This example also highlights the amount of work that can be required to reproduce an analysis, especially after quite some time. It is important to explicitly think about how we dividing that effort between ourselves as the original developer and the person trying to reproduce the analysis in the future.\n\n1.4.4 IDE Friendly\nOur final desirable property is that we’d like our projects to play nicely with integrated development environments.\nWhen you’re coding document and writing your data science projects it’d be possible for you to work entirely in either a plain text editor or typing code directly at the command line. While these approaches to a data science workflow have the benefit of simplicity, they also expect a great deal from you as a data scientist.\nThese workflows expect that you should type everything perfectly accurately every time, that you recall the names and argument orders of every function you use, and that you are constantly aware of the current state of all objects within your working environment.\nIntegrated Development Environments (IDEs) are applications that help to reduce this burden, helping make you a more effective programmer and data scientist. IDEs offer tools like code completion and highlighting to make your code easier to read and to write. They offer tools for debugging, to fix where things are going wrong, and they also offer environment panes so that you don’t have to hold everything in your head all at once. Many IDEs also often have templating facilities. These let you save and reuse snippets of code so that you can avoid typing out repetitive, boilerplate code and introducing errors in the process.\nEven if you haven’t heard of IDEs before, you’ve likely already used one. Some common examples might be RStudio for R-users, PyCharm for python users, or Visual Studio as a more language agnostic coding environment.\nWhichever of these we use, we’d like our project to play nicely with them. This lets us reap their benefits while keeping our project portable, version controlled, and reproducible for someone working with a different set-up."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#project-structure",
    "href": "101-workflows-organising-your-work.html#project-structure",
    "title": "1  Organising your work",
    "section": "\n1.5 Project Structure",
    "text": "1.5 Project Structure\nI’ve given a pretty exhaustive argument for why having a single directory for each project is a good idea. Let’s now take a look inside that directory and define a common starting layout for the content of all of your projects.\nHaving this sort of project directory template will mean that you’ll always know where to find what you’re looking for and other members of your team will too. Again, before we start I’ll reiterate that we’re taking an opinionated approach here and providing a sensible starting point for organizing many projects.\nEvery project is going to be slightly different and some might require slight alterations to what I suggest here. Indeed, even if you start as I suggest then you might have to adapt your project structure as it develops and grows. I think it’s helpful to consider yourself as a tailor when making these changes. I’m providing you with a one size fits all design, that’s great for lots of projects but perfect for none of them. It’s your job to alter and refine this design for each individual case.\nOne final caveat before we get started: companies and businesses will many times have a house style how to write and organize your code or projects. If that’s the case, then follow the style guide that your business or company uses. The most important thing here is to be consistent at both an individual level and across the entire data science team. It’s this consistency that reaps the benefits.\nOkay, so imagine now that you’ve been assigned a shiny new project and have created a single directory in which to house that project. Here we’ve, quite imaginatively, called that directory exciting-new-project. What do we populate this folder with?\n\n\n\n\n\n\n\n\nIn the rest of this video, I’ll define the house-style for organizing the root directory of your data science projects in this module.\n\n\n\n\n\n\n\n\nWithin the project directory there will be some subdirectories, which you can tell a folders in this file structure because they have a forward slash following their names. There will also be some files directly in the root directory. One of these is called readme.md and the another called either makefile or make.r. We’re going to explore each of these files and directories in turn.\n\n1.5.1 README.md\n\n\n\n\n\n\n\n\nLet’s begin with the readme file. This gives a brief introduction to your project and gives information on what the project aims to do. The readme file should describe how to get started using the project and how to contribute to its development.\nThe readme is written either in a plain text format so readme.txt or in markdown format readme.md. The benefit of using markdown is that it allows some light formatting such as sections headers and lists using plain text characters. Here you can see me doing that by using hashes to mark out first and second level headers and using bullet points for a unnumbered list. Whichever format you use, the readme file for your project is always stored in the root directory and is typically named in all uppercase letters.\nThe readme file should be the first thing that someone who’s new to your project reads. By placing the readme in the root directory and capitalising the file name you are increase the visibility of this file and increase the chances of this actually happening.\nAn additional benefit to keeping the readme in the root directory of your project is that code hosting services like GitHub, GitLab or BitBucket will display the contents of that readme file next to the contents of your project. Those services will also nicely format any markdown that you use for you in your readme file.\nWhen writing the readme, it can be useful to imaginge that you are writing this for a new, junior team member. The readme file should let them get started with the project and make some simple contributions after reading only that file. It might also link out to more detailed project documentation that will help the new team member toward a more advanced understanding or complex contribution.\n\n1.5.2 Inside the README\nlet’s take a quick aside to see in more detail what should be covered within a readme file.\nA readme we should include the name of the project, which should be self-explanatory (so nothing like my generic choice of exciting-new-project). The readme should also give the project status, which is just a couple of sentences to say whether your project is still under development, the version oft the current release or, on the other end of the project life-cycle, if the project is being deprecated or closed.\nFollowing this, we should also include a description of your project. This will state the purpose of your work and to provide, or link to, any additional context or references that visitors aren’t assumed to be familiar with.\nIf your project involves code or depends on other packages then you should give some instruction on how to install those dependencies and run your code. This might just be text but it could also include things like screenshots, code snippets, gifs or a video of the whole process.\nIt’s also a good practice to include some simple examples of how to use the code within your project an the expected results, so that new users can confirm that everything is working on their local instance. Keep the examples as simple and minimal as you can so that new users\nFor longer or more complicated examples that aren’t necessary in this short introductory document you can add links to those in the readme and explain them in detail elsewhere.\nThere should ideally be a short description of how people can report issues with the project and also how people can get started in resolving those issues or extend the project in some way.\nThat leads me on to one point that I’ve forgotten to list here. There there should be a section listing the authors of the work and the license in which under which it’s distributed. This is to give credit to all the people who’ve contributed to your project and the license file then says how other people may use your work. The license declares how other may use your project and whether they have to give direct attribution to your work in any modifications that they use.\n\n1.5.3 data\n\n\n\n\n\n\n\n\nMoving back to our project structure, next we have the data directory.\nThe data directory will have two subdirectories one called raw and one called derived. All data that is not generate as part of your project is stored in the raw subdirectory. To ensure that a project is reproducible, data in the Raw folder should never be edited or modified.\n\n\n\n\n\n\n\n\nIn this example we’ve got two different data types: an Excel spreadsheet the XLS file and a JSON file. These files are exacty as we received them from our project stakeholder.\nThe text file metadata.txt is a plain text file explaining the contents and interpretation of each of the raw data sets. This metadata should include descriptions of all the measured variables, the units that are recorded in, the date the file was created or acquired, and the source from which it was obtained.\nThe raw data likely isn’t going to be in a form that’s amenable to analyzing straight away. To get the data into a more pleasant form to work, it will require some data manipulation and cleaning. Any manipulation or cleaning that is applied should be well documented and the resulting cleaned files saved within the derived data directory.\n\n\n\n\n\n\n\n\nIn our exciting new project, we can see the clean versions of the previous data sets which are ready for modelling. There’s also a third file in this folder. This is data that we’ve acquired for ourselves through web scraping, using a script within the project.\n\n1.5.4 src\n\n\n\n\n\n\n\n\nThe src or source directory contains all the source code for your project. This will typically be the functions that you’ve written to make the analysis or modelling code more accessible.\nHere we’ve saved each function in its own R script and, in this project, we’ve used subdirectories to organise these by their use case. We’ve got two functions used in data cleaning: the first replaces NA values with a given value, the second replaces these by the mean of all non-missing values.\nWe also have three helper functions: the first two calculate rolling mean and the geometric mean of a given vector, the third is a function that scrapes the web data we saw in the derived data subdirectory.\n\n1.5.5 tests\n\n\n\n\n\n\n\n\nMoving on then to the tests directory. The structure of this directory mirrors that of the source directory. Each function file has its own counterpart file of tests.\nThese test files provide example sets of inputs and the expected outputs for each function. The test files are used to check edge cases of a function or to assure yourself that you haven’t broken anything while fixing some small bug or adding new capabilities to that function.\n\n1.5.6 analyses\n\n\n\n\n\n\n\n\nThe analyses directory contains what you probably think of as the bulk of your data science work. It’s going to have one subdirectory for each major analysis that’s performed within your project and within each of these there might be a series of steps that we collect into separate scripts.\nThe activity performed at each step is made clear by the name of each script, as is the order in which we’re going to perform these steps. Here we can see the scripts used for the 2021 annual report. First is a script used to take the raw monthly receipts and produce the cleaned version of the same data set that we saw earlier. This is followed by a trend analysis of this cleaned data set.\nSimilarly for the spending review we have a data cleaning step, followed by some forecast modelling and finally the production of some diagnostic plots to compare these forecasts.\n\n1.5.7 outputs\n\n\n\n\n\n\n\n\nThe outputs directory has again one subdirectory for each meta-analysis within the project. These are then further organized by the output type whether that be some data, a figure, or a table.\nDepending on the nature of your project, you might want to use a modified subdirectory structure here. For example, if you’re doing several numerical experiments then you might want to arrange your outputs by experiment, rather than by output type.\n\n1.5.8 reports\n\n\n\n\n\n\n\n\nThe reports directory is then where everything comes together. This is where the written documents that form the final deliverables of your project are created. If these final documents are written in LaTeX or markdown, both the source and the compiled documents can be found within this directory.\nWhen including content in this report, for example figures, I’d recommend against making copies of those figure files within the reports directory. If you do that, then you’ll have to manually update the files every time you modify them. Instead you can use relative file paths to include these figures. Relative file paths specify how to get to the image, starting from your TeX document and moving up and down through the levels of your project directory.\nIf you’re not using markdown or LaTeX to write your reports, but instead use an online platform like overleaf as a latex editor or Google docs to write collaboratively then links to them in the reports directory using additional readme files. Make sure you set the read and write permissions for those links appropriately, too.\nWhen using these online writing systems, you’ll have to manually upload and update your plots whenever you modify any of your earlier analysis. That’s one of the drawbacks of these online tools that has to be traded off against their ease of use.\nIn our exciting new project, here we can see that the annual report is written in a markdown format, which is compiled to both HTML and PDF. The spendiing review is written in LaTeX and we only have the source for it, we don’t have the compiled pdf version of the document.\n\n1.5.9 make file\n\n\n\n\n\n\n\n\nThe final element of our template project structure is a make file. We aren’t going to cover how to read or write make files in this course. Instead, I’ll give you a brief description of what they are and what it is supposed to do.\nAt a high level, the make file is just a text file. What makes it special is what it contains. Similar to a shell or a bash script, make file contains code that could be run at the command line. This code will create or update each element of your project.\nThe make file defines shorthand commands for the full lines of code that create each element of your project. The make file also records the order in which these operations have to happen, and which of these steps are dependent on one another. This means that if one step part of your project is updated then any changes will be propagated through your entire project. This is done in quite a clever way so the only part of your projects that are re-run are those that need to be updated.\nWe’re omitting make files from this course not because they’re fiendishly difficult to write or read, but rather because they require a reasonable foundation in working at the command line to be understood. What I suggest you do instead throughout this course is to create your own R or markdown file called make. This file will define the intended running order and dependencies of your project and if it is an R file, it might also automate some parts of your analysis."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#wrapping-up",
    "href": "101-workflows-organising-your-work.html#wrapping-up",
    "title": "1  Organising your work",
    "section": "\n1.6 Wrapping up",
    "text": "1.6 Wrapping up\nWrapping up then, that’s everything for this chapter.\nI’ve introduced a project structure that will serve you well as a baseline for the vast majority of projects in data science.\n\n\n\n\n\n\n\n\nIn your own work, remember that the key here is standardisation. Working consistently across projects, a company or a group is more important than sticking rigidly to the particular structure that I have defined here.\nThere are two notable exceptions where you probably don’t want to use this project structure. That’s when you’re building an app or you’re building a package. These require specific organisation of the files within your project directory. We’ll explore the project structure used for package development during the live session this week."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#session-information",
    "href": "101-workflows-organising-your-work.html#session-information",
    "title": "1  Organising your work",
    "section": "\n1.7 Session Information",
    "text": "1.7 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), digest(v.0.6.33), jsonlite(v.1.8.7), evaluate(v.0.21), rlang(v.1.1.1), cli(v.3.6.1), renv(v.0.16.0), rstudioapi(v.0.15.0), rmarkdown(v.2.23), tools(v.4.2.2), pander(v.0.6.5), htmlwidgets(v.1.6.2), xfun(v.0.39), yaml(v.2.3.7), fastmap(v.1.1.1), compiler(v.4.2.2), htmltools(v.0.5.5) and knitr(v.1.43)"
  },
  {
    "objectID": "102-workflows-naming-files.html",
    "href": "102-workflows-naming-files.html",
    "title": "2  Naming Files",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "102-workflows-naming-files.html#introduction",
    "href": "102-workflows-naming-files.html#introduction",
    "title": "2  Naming Files",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\nPhil Karlton, Netscape Developer\n\nWhen working on a data science project we can in principle name directories, files, functions and other objects whatever we like. In reality though, using an ad-hoc system of naming is likely to cause confusion, headaches and mistakes. We obviously want to avoid all of those things, in the spirit of being kind to our current colleges and also to our future selves.\nComing up with good names is an art form. Like most art, naming things is an activity that you get better at with practice. Another similarity is that the best naming systems don’t come from giving data scientists free reign over their naming system. Like all art, the best approaches to naming things give you strong guidelines and boundaries within which to express your creativity and skill.\nIn this lecture we’ll explore what these boundaries and what we want them to achieve for us. The content of this lecture is based largely around a talk of the same name given by Jennifer Bryan and the tidyverse style guide, which forms the basis of Google’s style guide for R programming."
  },
  {
    "objectID": "102-workflows-naming-files.html#naming-files",
    "href": "102-workflows-naming-files.html#naming-files",
    "title": "2  Naming Files",
    "section": "\n2.2 Naming Files",
    "text": "2.2 Naming Files\nWe’ll be begin by focusing in on what we call our files. That is, we’ll first focus on the part of the file name that comes before the dot. In the second part of this video, we’ll then cycle back around to discuss file extensions.\n\n2.2.1 What do we want from our file names?\nBefore we dive into naming files, we should first consider what we want from the file names that we choose. There are three key properties that that we would like to satisfy.\n\nMachine Readable\nHuman Readable\nOrder Friendly\n\nThee first desirable property is for file names to be easily readable by computers, the second is for the file names to be easily readable by humans and finally the file names should take advantage of the default ordering imposed on our files.\nThis set of current file names is sorely lacking across all of these properties:\nabstract.docx\nEffective Data Science's module guide 2022.docx \nfig 12.png\nRplot7.png\n1711.05189.pdf\nHR Protocols 2015 FINAL (Nov 2015).pdf\nWe want to provide naming conventions to move us toward the better file names listed below.\n2015-10-22_human-resources-protocols.pdf\n2022_effective-data-science-module-guide.docx\n2022_RSS-conference-abstract.docx \nfig12_earthquake-timeseries.png \nfig07_earthquake-location-map.png\nogata_1984_spacetime-clustering.pdf\nLet’s take a few minutes to examine what exactly we mean by each of these properties.\n\n2.2.2 Machine Readable\nWhat do we mean by machine readable file names?\n\nEasy to compute on by deliberate use of delimiters:\n\n\nunderscores_separate_metadata, hyphens-separate-words.\n\n\nPlay nicely with regular expressions and globbing:\n\navoid spaces, punctuation, accents, cases;\nrm Rplot*.png\n\n\n\nMachine readable names are useful when:\n\nmanaging files: ordering, finding, moving, deleting:\nextracting information directly from file names,\nworking programmatically with file names and regex.\n\nWhen we are operating on a large number of files it is useful to be able to work with them programmatically.\nOne example of where this might be useful is when downloading assessments for marking. This might require me to unzip a large number of zip files, copying the pdf report from each unzipped folder into a single directory and all of the R scripts from each unzipped folder into another directory. The marked scripts and code then need to be paired back up in folders named by student, and re-zipped ready to be returned.\nThis is monotonously dull and might work for ~50 students but not for ~5000. Working programmatically with files is the way to get this job done efficiently. This requires the file names to play nicely with the way that computers interpret file names, which they regard as a string of characters.\nIt is often helpful to have some meta-data included in the file name, for example the student’s id number and the assessment title. We will use an underscore to separate elements of meta-data within the file name and a hyphen to separate sub-elements of meta-data, for example words within the assessment title.\nRegular expressions and globbing are two ideas from string manipulation that you may not have met, but which will inform our naming conventions. Regular expressions allow you to search for strings (in our case file names) that match a particular pattern. Regular expressions can do really complicated searches but become gnarly when you have to worry about special characters like spaces, punctuation, accents and cases, so these should be avoided in file names.\nA special type of regular expression is called globbing where a star is used to replace any number of subsequent characters in a file name, so that here we can delete all png images that begin with Rplot using a single line of code. Globbing becomes particular powerful when you use a consistent structure to create your file names.\nAs in the assessment marking example, having machine readable file names is particularly useful when managing files, such as ordering, finding, moving or deleting them. Another example of this is when your analysis requires you to load a large number of individual data files.\nMachine readable file names are also useful for extracting meta-information from files without having to open them in memory. This is particularly useful when the files might be too large to load into memory, or you only want to load data from a certain year.\nThe final benefit we list here is the scalability, reduction in drudgery and lowered risk for human error when operating on a very large number of files.\n\n2.2.3 Order Friendly\nThe next property we will focus on also links to how computers operate. We’d like our file names to exploit the default orderings used by computers. This means starting file names with character strings or metadata that allow us order our files in some meaningful way.\n\n2.2.3.1 Running Order\nOne example of this is where there’s some logical order in which your code should be executed, as in the example analysis below.\ndiagnositc-plots.R\ndownload.R\nruntime-comparison.R\n...\nmodel-evaluation.R\nwrangle.R\nAdding numbers to the start of these file names can make the intended ordering immediately obvious.\n00_download.R\n01_wrangle.R\n02_model.R\n...\n09_model-evaluation.R\n10_model-comparison-plots.R\nStarting single digit numbers with a leading 0 is a very good idea here to prevent script 1 being sorted in with the tens, script 2 in with the twenties and so on. If you might have over 100 files, for example when saving the output from many simulations, use two or more zeros to maintain this nice ordering.\n\n2.2.3.2 Date Order\nA second example of orderable file names is when the file has a date associated with it. This might be a version of a report or the date on which some data were recorded, cleaned or updated.\n2015-10-22_human-resources-protocols.pdf\n...\n2022-effective-data-science-module-guide.docx\nWhen using dates, in file names or elsewhere, you should conform to the ISO standard date format.\n\nISO 8601 sets an international standard format for dates: YYYY-MM-DD.\n\nThis format uses four numbers for the year, followed by two numbers for the month and two numbers of the day of the month. This structure mirrors a nested file structure moving from least to most specific. It also avoids confusion over the ordering of the date elements. Without using the ISO standard a date like 04-05-22 might be interpreted as the fourth of May 2022, the fifth of April 2022, or the twenty-second of May 2004.\n\n2.2.4 Human Readable\nThe final property we would like our file names to have is human readability. This requires the names of our files to be meaningful, informative and easily read by real people.\nThe first two of these are handled by including appropriate metadata in the file name. The ease with which these are read by real people is determined by the length of the file name and by how that name is formatted.\nThere are lots of formatting options with fun names like camelCase, PascalCase, and snake_case.\n   easilyReadByRealPeople (camelCase)\n   EasilyReadByRealPeople (PascalCase)\n   easily_read_by_real_people (snake_case)\n   easily-read-by-real-people (skewer-case)\nThere is weak evidence to suggest that snake case and skewer case are most the readable. We’ll use a mixture of these two, using snake case between metadata items and skewer case within them. This has a slight cost to legibility, in a trade-off against making computing on these file names easier.\nThe final aspect that you have control over is the length of the name. Having short, evocative and useful file names is not easy and is a skill in itself. For some hints and tips you might want to look into tips for writing URL slugs. These are last part of a web address that are intended to improve accessibility by being immediately and intuitively meaningful to any user.\n\n2.2.5 Naming Files - Summary\n\nFile names should be meaningful, informative and scripts end in .r\nStick to letters, numbers underscores (_) and hyphens (-).\nPay attention to capitalisation file.r \\(\\neq\\) File.r on all operating systems.\nShow order with left-padded numbers or ISO dates."
  },
  {
    "objectID": "102-workflows-naming-files.html#file-extensions-and-where-you-work",
    "href": "102-workflows-naming-files.html#file-extensions-and-where-you-work",
    "title": "2  Naming Files",
    "section": "\n2.3 File Extensions and Where You Work",
    "text": "2.3 File Extensions and Where You Work\nSo far we have focused entirely on what comes before the dot, that is the file name.Equally, if not more, important is what comes after the dot, the file extension.\nexample-script.r\nexample-script.py\n\nproject-writeup.doc\nproject-writeup.tex\nThe file extension describes how information is stored in that file and determines the software that can use, view or run that file.\nYou likely already use file extensions to distinguish between code scripts, written documents, images, and notebook files. We’ll now explore the benefits and drawbacks of various file types with respect to several important features.\n\n2.3.1 Open Source vs Proprietary File Types\nThe first feature we’ll consider is whether the file type is open source, and can be used by anyone without charge, or if specialist software must be paid for in order to interact with those files.\n\n\n\n\n\n\n\n\nIn the figure above, each column represents a different class of file, moving left to right we have example file types for tabular data, list-like data and text documents. File types closer to the top are open source while those lower down rely on proprietary software, which may or may not require payment.\nTo make sure that our work is accessible to as many people as possible we should favour the open source options like csv files over Google sheets or excel, JSON files over Matlab data files, and tex or markdown over a word or Google doc.\nThis usually has a benefit in terms of project longevity and scalability. The open source file types are often somewhat simpler in structure, making them more robust to changes over time less memory intensive.\nTo see this, let’s take a look inside some data files.\n\n2.3.2 Inside Data Files\n\n2.3.2.1 Inside a CSV file\nCSV or comma separated value files are used to store tabular data.\nIn tabular data, each row of the data represents one record and each column represents a data value.\nA csv encodes this by having each record on a separate line and using commas to separate values with that record. You can see this by opening a csv file in a text editor such as notepad.\nThe raw data stores line breaks using \\n and indicates new rows by \\r. These backslashed indicae that these are escape characters with special meanings, and should not be literally interpreted as the letters n and r.\n\n#> [1] \"Name,Number\\r\\nA,1\\r\\nB,2\\r\\nC,3\"\n\nWhen viewed in a text editor, the example file would look something like this.\nName,Number \nA,1\nB,2\nC,3\n\n2.3.2.2 Inside a TSV file\nTSV or tab separated value files are also used to store tabular data.\nLike in a csv each record is given on a new line but in a tsv tabs rather than commas are used to separate values with each record. This can also be seen by opening a tsv file in a text editor such as notepad.\n\n#> [1] \"Name\\tNumber\\r\\nA\\t1\\r\\nB\\t2\\r\\nC\\t3\"\n\nName    Number \nA   1\nB   2\nC   3\nOne thing to note is that tabs are a separate character and are not just multiple spaces. In plain text these can be impossible to tell apart, so most text editors have an option to display tabs differently from repeated spaces, though this is usually not enabled by default.\n\n2.3.2.3 Inside an Excel file\nWhen you open an excel file in a text editor, you will immediately see that this is not a human interpretable file format.\n504b 0304 1400 0600 0800 0000 2100 62ee\n9d68 5e01 0000 9004 0000 1300 0802 5b43\n6f6e 7465 6e74 5f54 7970 6573 5d2e 786d\n6c20 a204 0228 a000 0200 0000 0000 0000\n0000 0000 0000 0000 0000 0000 0000 0000\n.... .... .... .... .... .... .... ....\n0000 0000 0000 0000 ac92 4d4f c330 0c86\nef48 fc87 c8f7 d5dd 9010 424b 7741 48bb\n2154 7e80 49dc 0fb5 8da3 241b ddbf 271c\n1054 1a83 0347 7fbd 7efc cadb dd3c 8dea\n.... .... .... .... .... .... .... ....\nEach entry here is a four digit hexadecimal number and there are a lot more of them than we have entries in our small table.\nThis is because excel files can carry a lot of additional information that a csv or tsv are not able to, for example cell formatting or having multiple tables (called sheets by excel) stored within a single file.\nThis means that excel files take up much more memory because they are carrying a lot more information than is strictly contained within the data itself.\n\n2.3.2.4 Indise a JSON file\nJSON, or Java Script Object Notation, files are an open source format for list-like data. Each record is represented by a collection of key:value pairs. In our example table each entry has two fields, one corresponding to the Name key and one corresponding to the Number key.\n[{\n    \"Name\": \"A\",\n    \"Number\": \"1\"\n}, {\n    \"Name\": \"B\",\n    \"Number\": \"2\"\n}, {\n    \"Name\": \"C\",\n    \"Number\": \"3\"\n}]\nThis list-like structure allows non-tabular data to be stored by using a property called nesting: the value taken by a key can be a single value, a vector of values or another list-like object.\nThis ability to create nested data structures has lead to this data format being used widely in a range of applications that require data transfer.\n\n2.3.2.5 Inside an XML file\nXML files are another open source format for list-like data, where each record is represented by a collection of key:value pairs.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<root>\n  <row>\n    <Name>A</Name>\n    <Number>1</Number>\n  </row>\n  <row>\n    <Name>B</Name>\n    <Number>2</Number>\n  </row>\n  <row>\n    <Name>C</Name>\n    <Number>3</Number>\n  </row>\n</root>\nThe difference from a JSON file is mainly in how those records are formatted within the file. In a JSON file this is designed to look like objects in the Java Script programming language and in XML the formatting is done to look like html, the markup language used to write websites.\n\n2.3.3 A Note on Notebooks\n\nThere are two and a half notebook formats that you are likely to use: .rmd, .ipynb or alternatively .qmd.\nR markdown documents .rmd are plain text files, so are very human friendly.\nJuPyteR notebooks have multi-language support but are not so human friendly (JSON in disguise).\nQuarto documents offer the best of both worlds and more extensive language support. Not yet as established as a format.\n\nIn addition to the files you read and write, the files that you code in will largely determine your workflow.\nThere are three main options for the way that you code: first is typing it directly at the command line, second is using a text editor or IDE to write scripts and third is writing a notebook that mixes code, text and output together in a single document.\nWe’ll compare these methods of working soon, but first let’s do a quick review of what notebooks are available to you and why you might want to use them.\nAs a data scientist, there are two and a half notebook formats that you’re likely to have met before. The first two are Rmarkdown files for those working predominantly in R and interactive Python or jupyter notebooks for those working predominantly in Python. The final half format are quarto markdown documents, which are relatively new and extend the functionality of Rmarkdown files to provide multi-language support.\nThe main benefit of R markdown documents is that they’re plain text files, so they’re very human friendly and work very well with version control software like git. JuPyteR notebooks have the benefit of supporting code written in Julia, Python or R, but are not so human friendly - under the hood these documents are JSON files that should not be edited directly (because a misplaced bracket will break them!).\nQuarto documents offer the best of both worlds, with plain text formatting and even more extensive language support than jupyter notebooks. Quarto is a recent extension of Rmarkdown, which is rapidly becoming popular in the data science community. Quarto also allows you to create a wider range of documents, including websites, these course notes and the associated slides.\nEach format has its benefits and drawbacks depending on the context in which they are used and all have some shared benefits and limitations by nature of them all being notebook documents.\n\n2.3.4 File Extensions and Where You Code\n\n\nProperty\nNotebook\nScript\nCommand Line\n\n\n\nreproducible\n~\n✓\nX\n\n\nreadable\n~\n✓\n~\n\n\nself-documenting\n✓\nX\nX\n\n\nin production\nX\n✓\n~\n\n\nordering / automation\n~\n✓\n~\n\n\n\nThe main benefit of notebook documents is that they are self-documenting, in that they can mix the documentation, code and report all into a single document. Notebooks also provide a level of interactivity when coding that is not possible when working directly at the command line or using a text editor to write scripts. This limitation is easily overcome by using an integrated development environment when scripting, rather than a plain text editor.\nWriting code in .r files is not self-documenting but this separation of code, documentation and outputs has many other benefits. Firstly, the resulting scripts provide a reproducible and automatable workflow, unlike one-off lines of code being run at the command line. Secondly, using an IDE to write these provides you with syntax highlighting and code linting features to help you write readable and accurate code. Finally, the separation of code from documentation and output allows your work to be more easily or even directly put into production.\nIn this course we will advocate for a scripting-first approach to data science, though notebooks and command line work definitely have their place.\nNotebooks are great as teaching and rapid development tools but have strong limitations with being put into production. Conversely, coding directly at the command line is perfect for simple one-time tasks but it leaves no trace of your workflow and leads to an analysis that cannot be easily replicated in the future.\n\n2.3.5 Summary\nFinally, let’s wrap things up by summarising what we have learned about naming files.\nBefore the dot we want to pick file names that machine readable, human friendly and play nicely with the default orderings provided to us.\n\nName files so that they are:\n\nMachine Readable,\nHuman Readable,\nOrder Friendly.\n\n\nAfter the dot, we want to pick file types that are widely accessible, easily read by humans and allow for our entire analysis to be reproduced.\n\nUse document types that are:\n\nWidely accessible,\nEasy to read and reproduce,\nAppropriate for the task at hand.\n\n\nAbove all we want to name our files and pick our file types to best match with the team we are working in and the task that is at hand."
  },
  {
    "objectID": "102-workflows-naming-files.html#session-information",
    "href": "102-workflows-naming-files.html#session-information",
    "title": "2  Naming Files",
    "section": "\n2.4 Session Information",
    "text": "2.4 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: readr(v.2.1.4)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), magrittr(v.2.0.3), hms(v.1.1.3), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), fansi(v.1.0.4), tools(v.4.2.2), xfun(v.0.39), utf8(v.1.2.3), cli(v.3.6.1), htmltools(v.0.5.5), yaml(v.2.3.7), digest(v.0.6.33), tibble(v.3.2.1), lifecycle(v.1.0.3), tzdb(v.0.4.0), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), pander(v.0.6.5), compiler(v.4.2.2), pillar(v.1.9.0), jsonlite(v.1.8.7), renv(v.0.16.0) and pkgconfig(v.2.0.3)"
  },
  {
    "objectID": "103-workflows-organising-your-code.html",
    "href": "103-workflows-organising-your-code.html",
    "title": "3  Code",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#introduction",
    "href": "103-workflows-organising-your-code.html#introduction",
    "title": "3  Code",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\nWe have already described how we might organise an effective data science project at the directory and file level. In this chapter we will delve one step deeper and consider how we can structure our work within those files. In particular, we’ll focus on code files here.\nWe’ll start by comparing the two main approaches to structuring our code, namely functional programming and object oriented programming. We’ll then see how we should order code within our scripts and conventions on how to name the functions and objects that we work with in our code.\nRounding up this chapter, we’ll summarise the main points from the R style guide that we will be following in this course and highlight some useful packages for writing effective code."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#functional-programming",
    "href": "103-workflows-organising-your-code.html#functional-programming",
    "title": "3  Code",
    "section": "\n3.2 Functional Programming",
    "text": "3.2 Functional Programming\nA functional programming style has two major properties:\n\nObject immutability,\nComplex programs written using function composition.\n\nThis first point here states that the original data or objects should never be modified or altered by the code we write. We have met this idea before when making new, cleaner versions of our raw data but taking care to leave the original messy data intact. Object immutability is the exact same idea but in a code context rather than data context.\nSecondly, in functional programming, complex problems are solved by decomposing them into a series of smaller problems. A separate, self-contained function is then written to solve each sub-problem. Each individual function is, in itself, simple and easy to understand. This makes these small functions easy to test and easy to reuse in many places. Code complexity is then built up by composing these functions in various ways.\nIt can be difficult to get into this way of thinking, but people with mathematical training often find it quite natural. This is because mathematicians have many years of experience in working with function compositions in the abstract, mathematical sense.\n\\[y = g(x) = f_3 \\circ f_2 \\circ f_1(x).\\]\n\n3.2.1 The Pipe Operator\nOne issue with functional programming is that lots of nested functions means that there are also lots of nested brackets. These start to get tricky to keep track of when you have upwards of 3 functions being composed. This reading difficulty is only exacerbated if your functions have additional arguments on top of the original inputs.\n\nlog(exp(cos(sin(pi))))\n#> [1] 1\n\nThe pipe operator %>% from the magrittr package helps with this issue. It works exactly like function composition: it takes the whatever is on the left (whether that is an existing object or the output of a function) and passes it to the following function call as the first argument of that function.\n\nlibrary(magrittr)\npi %>% \n  sin() %>% \n  cos() %>% \n  exp() %>% \n  log()\n#> [1] 1\n\n\niris %>% \n  head(n = 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n\nThe pipe operator is often referred to as “syntactic sugar”. This is because it doesn’t add anything to your code in itself, but rather it makes your code so much more palatable to read.\nIn R versions 4.1 and greater, there’s a built-in version of this pipe operator,|>. This is written using the vertical bar symbol followed by a greater than sign. To type the vertical bar, you can usually find it found above backslash on the keyboard. (Just to cause confusion, the vertical bar symbol is also called the pipe symbol and performs a similar operation in general programming contexts.)\n\nlibrary(magrittr)\npi |> \n  sin() |> \n  cos() |> \n  exp() |> \n  log()\n#> [1] 1\n\n\niris |> \n  head(n = 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n\nThe base R pipe usually behaves in the same way as the pipe from magrittr, but there are a few cases where they differ. For reasons of back-compatibility and consistency we’ll stick to the {magrittr} pipe in this course.\n\n3.2.2 When not to pipe\n\n\n\nPipes are designed to put focus on the the actions you are performing rather than the object that you are preforming those operations on. This means that there are two cases where you should almost certainly not use a pipe.\nThe first of these is when you need to manipulate more than one object at a time. Using secondary objects as reference points (but leaving them unchanged) is of course perfectly fine, but pipes should be used when applying a sequence of steps to create a new, modified version of one primary object.\nSecondly, just because you can chain together many actions into a single pipeline, that doesn’t mean that you necessarily should. Very long sequences of piped operations are easier to read than nested functions, however they still burden the reader with the same cognitive load on their short term memory. Be kind and create meaningful, intermediate objects with informative names. This will help the reader to more easily understand the logic within your code."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#object-oriented-programming",
    "href": "103-workflows-organising-your-code.html#object-oriented-programming",
    "title": "3  Code",
    "section": "\n3.3 Object Oriented Programming",
    "text": "3.3 Object Oriented Programming\nThe main alternative to functional programming is object oriented programming.\n\nSolve problems by using lots of simple objects\nR has 3 OOP systems: S3, S4 and R6.\nObjects belong to a class, have methods and fields.\nExample: agent based simulation of beehive.\n\n\n3.3.1 OOP Philosophy\nIn functional programming, we solve complicated problems by using lots of simple functions. In object oriented programming we solve complicated problems using lots of simple objects. Which of these programming approaches is best will depend on the particular type of problem that you are trying to solve.\nFunctional programming is excellent for most types of data science work. Object oriented comes into its own when your problem has many small components interacting with one another. This makes it great for things like designing agent-based simulations, which I’ll come back to in a moment.\nIn R there are three different systems for doing object oriented programming (called S3, S4, and R6), so things can get a bit complicated. We won’t go into detail about them here, but I’ll give you an overview of the main ideas.\nThis approach to programming might be useful for you in the future, for example if you want to extend base R functions to work with new types of input, and to have user-friendly displays. In that case (Advanced R)[https://adv-r.hadley.nz/] by Hadley Wickham is an excellent reference text.\nIn OOP, each object belongs to a class and has a set of methods associated with it. The class defines what an object is and methods describe what that object can do. On top of that, each object has class-specific attributes or data fields. These fields are shared by all objects in a class but the values that they take give information about that specific object.\n\n3.3.2 OOP Example\n\n\n\nThis is is all sounding very abstract. Let’s consider writing some object oriented code to simulate a beehive. Each object will be a bee, and each bee is an instance of one of three bee classes: it might be a queen, a worker or a drone for example. Different bee classes have different methods associated with them, which describe what the bee can do, for example all bees would have 6 methods that let them move up, down, left, right, forward and backward within the hive. An additional “reproduce” method might only be defined for queen bees and a pollinate method might only be defined for workers. Each instance of a bee has its own fields, which give data about that specific bee. All bees have x, y and z coordinate fields giving their location within the hive. The queen class might have an additional field for their number of offspring and the workers might have an additional field for how much pollen they are carrying.\nAs the simulation progresses, methods are applied to each object altering their fields and potentially creating or destroying objects. This is very different from the preservation mindset of functional programming, but hopefully you can see that it is a very natural approach to many types of problem."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#structuring-r-script-headers",
    "href": "103-workflows-organising-your-code.html#structuring-r-script-headers",
    "title": "3  Code",
    "section": "\n3.4 Structuring R Script Headers",
    "text": "3.4 Structuring R Script Headers\n\nTL;DR\n\nStart script with a comment of 1-2 sentences explaining what it > does.\n\nsetwd() and rm(ls()) are the devil’s work.\n“Session” > “Restart R” or Keyboard shortcut: crtl/cmd + shift > + 0\nPolite to gather all library() and source() calls.\nRude to mess with other people’s set up using > install.packages().\nPortable scripts use paths relative to the root directory of the project.\n\n\nFirst things first, let’s discuss what should be at the top of your R scripts.\nIt is almost always a good idea to start your file with a few commented out sentences describing the purpose of the script and, if you work in a large team, perhaps who contact with any questions about this script. (There is more on comments coming up soon, don’t worry!)\nIt is also good practise to move all library() and source() calls to the top of your script. These indicate the packages and helper function that are dependencies of your script; it’s useful to know what you need to have installed before trying to run any code.\nThat segues nicely to the next point, which is never to hard code package installations. It is extremely bad practise and very rude to do so because then your script might alter another person’s R installation. If you don’t know already, this is precisely the difference between an install.packages() and library() call: install.packages() will download the code for that package to the users computer, while library() takes that downloaded code and makes it available in the current R session. To avoid messing with anyone’s R installation, you should always type install.package() commands directly in the console and then place the corresponding library() calls within your scripts.\nNext, it is likely that you, or someone close to you, will commit the felony of starting every script by setting the working directory and clearing R’s global environment. This is very bad practice, it’s indicative of a workflow that’s not project based and it’s problematic for at least two reasons. Firstly, the path you set will likely not work on anyone else’s computer. Secondly, clearing the environment like this may look like it gets you back to fresh, new R session but all of your previously loaded packages will still be loaded and lurking in the background.\nInstead, to achieve your original aim of starting a new R session, go to the menu and select the “Session” drop down then select “Restart R”. Alternatively, you can use keyboard shortcuts to do the same. This is “crtl + shift + 0” on Windows and “cmd + shift + 0” on a mac. The fact that a keyboard shortcut exists for this should quite strongly hint that, in a reproducible and project oriented workflow, you should be restarting R quite often in an average working day. This is the scripting equivalent of “clear all output and rerun all” in a notebook.\nFinally, let’s circle back to the point I made earlier about setting the working directory. The reason that this will not work is because you are likely giving file paths that are specific to your computer, your operating system and your file organisation system. The chances of someone else having all of these the same are practically zero."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#portable-file-paths-with-here",
    "href": "103-workflows-organising-your-code.html#portable-file-paths-with-here",
    "title": "3  Code",
    "section": "\n3.5 Portable File paths with {here}\n",
    "text": "3.5 Portable File paths with {here}\n\n\n# Bad - breaks if project moved\nsource(\"zaks-mbp/Desktop/exciting-new-project/src/helper_functions/rolling_mean.R\")\n\n# Better - breaks if Windows\nsource(\"../../src/helper_functions/rolling_mean.R\")\n\n# Best - but use here:here() to check root directory correctly identified\nsource(here::here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n# For more info on the here package:\nvignette(\"here\")\n\nTo fix the problem of person- and computer-specific file paths you can have two options.\nThe first is to use relative file paths. In this you assume that each R script is being run in its current location and my moving up and down through the levels of your project directory you point to the file that you need.\nThis is good in that it solves the problem of paths breaking because you move the project to a different location on your own laptop. However, it does not fully solve the portability problem because you might move your file to a different location within the same project. It also does not solve the problem that windows uses MacOS and linux use forward slashes in file paths with widows uses backslashes.\nTo resolve these final two issues I recommend using the here() function from the {here} package. This package looks for a .Rproj or .git file to identify the root directory of your project and creates file paths relative to the root of your project, that are suitable for the operating system the code is being run on.\nIt really is quite marvellous. For more information on how to use the here package, explore its chapter in R - What They Forgot, R for Data Science or this project oriented workflow blog post."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#code-body",
    "href": "103-workflows-organising-your-code.html#code-body",
    "title": "3  Code",
    "section": "\n3.6 Code Body",
    "text": "3.6 Code Body\nMoving on now, we will go from the head to the body of the code. Having well named and organised code will facilitate both reading and understanding. Comments and sectioning do the rest of this work.\nThis section is designed as an introduction to the tidyverse style guide and not as a replacement to it. ### Comments\n\n# This is an example script showing good use of comments and sectioning \n\nlibrary(here)\nsource(here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n#===============================================================================  <- 80 characters max for readability\n# Major Section on Comments ----\n#===============================================================================\n\n#-------------------------------------------------------------------------------\n##  Minor Section on inline comments ---- \n#-------------------------------------------------------------------------------\nx <- 1:10 # this is an inline comment\n\n#-------------------------------------------------------------------------------\n##  Minor Section on full line comments ---- \n#-------------------------------------------------------------------------------\nrolling_mean(x)\n# This is an full line comment\n\nComments may be either short in-line comments at the end of a line or full lines dedicated to comments. To create either type of comment in R, simply type hash followed by one space. The rest of that line will not be evaluated and will function as a comment. If multi-line comments are needed simply start multiple lines with a hash and a space.\nThe purpose of these comments is to explain the why of what you are doing, not the what. If you are explaining what you are doing in most of your comments then you perhaps need to consider writing more informative function names, something we will return to in the general advice section.\nComments can also be used to add structure to your code, buy using commented lines of hyphens and equal signs to chunk your files into minor and major sections.\nMarkdown-like section titles can be added to these section and subsection headers. Many IDEs, such as RStudio, will interpret these as a table of contents for you, so that you can more easily navigate your code.\n\n3.6.1 Objects are Nouns\n\nObject names should use only lowercase letters, numbers, and _.\nUse underscores (_) to separate words within a name. (snake_case)\nUse nouns, preferring singular over plural names.\n\n\n# Good\nday_one\nday_1\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1\n\nWhen creating and naming objects a strong guideline in that objects should be named using short but meaningful nouns. Names should not include any special characters and should use underscores to separate words within the object name.\nThis is similar to our file naming guide, but note that hyphens can’t be used in object names because this conflicts with the subtraction operator.\nWhen naming objects, as far as possible use singular nouns. The main reason for this is that the plurisation rules in English are complex and will eventually trip up either you or a user of your code.\n\n3.6.2 Functions are Verbs\n\nFunction names should use only lower-case letters, numbers, and _.\nUse underscores (_) to separate words within a name. (snake_case)\nSuggest imperative mood, as in a recipe.\nBreak long functions over multiple lines. 4 vs 2 spaces.\n\n\n# Good\nadd_row()\npermute()\n\n# Bad\nrow_adder()\npermutation()\n\nlong_function_name <- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\nThe guidelines for naming functions are broadly similar, with the advice that functions should be verbs rather than nouns.\nFunctions should be named in the imperative mood, like in a recipe. This is again for consistency; having function names in a range of moods and tenses leads to coding nightmares.\nAs with object names you should aim to give your functions and their arguments short, evocative names. For functions with many arguments or a long name, you might not be able to fit the function definition on a single line. In this case you can should place each argument on its own double indented line and the function body on a single indented line.\n\n3.6.3 Casing Consistantly\nAs we have mentioned already, we have many options for separating words within names:\n\nCamelCase\npascalCase\nsnakecase\n\nunderscore_separated ❤️\nhyphen-separated\n\npoint.separated 💀\n\nFor people used to working in Python it is tempting to use point separation in function names, in the spirit of methods from object oriented programming. Indeed, some base R functions even use this convention.\nHowever, the reason that we advise against it is because it is already used for methods in some of R’s inbuilt OOP functionality. We will use underscore separation in our work.\n\n3.6.4 Style Guide Summary\n\nUse comments to structure your code\nObjects = Nouns\nFunctions = Verbs\nUse snake case and consistant grammar"
  },
  {
    "objectID": "103-workflows-organising-your-code.html#further-tips-for-friendly-coding",
    "href": "103-workflows-organising-your-code.html#further-tips-for-friendly-coding",
    "title": "3  Code",
    "section": "\n3.7 Further Tips for Friendly Coding",
    "text": "3.7 Further Tips for Friendly Coding\nIn addition to naming conventions the style guide gives lots of other guidance on writing code in a way that is kind to future readers of that code.\nI’m not going to go repeat all of that guidance here, but the motivation for all of these can be boiled down into the following points.\n\nWrite your code to be easily understood by humans.\nUse informative names, typing is cheap.\n\n\n# Bad\nfor (i in dmt) {\n  print(i)\n}\n\n# Good\nfor (temperature in daily_max_temperature) {\n  print(temperature)\n}\n\n\nDivide your work into logical stages, human memory is expensive.\n\nWhen writing your code, keep that future reader in mind. This means using names that are informative and reasonably short, it also means adding white space, comments and formatting to aid comprehension. Adding this sort of structure to your code also helps to reduce the cognitive burden that you are placing on the human reading your code.\nInformative names are more important than short names. This is particularly true when using flow controls, which are things like for loops and while loops. Which of these for loops would you like to encounter when approaching a deadline or urgently fixing a bug? Almost surely the second one, where context is immediately clear.\nA computer doesn’t care if you call a variable by only a single letter, by a random key smash (like aksnbioawb) or by an informative name. A computer also doesn’t care if you include no white space your code - the script will still run. However, doing these things are friendly practices that can help yourself when debugging and your co-workers when collaborating."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#reduce-reuse-recycle",
    "href": "103-workflows-organising-your-code.html#reduce-reuse-recycle",
    "title": "3  Code",
    "section": "\n3.8 Reduce, Reuse, Recycle",
    "text": "3.8 Reduce, Reuse, Recycle\nIn this final section, we’ll look at how you can make your workflow more efficient by reducing the amount of code you write, as well as reusing and recycling code that you’ve already written.\n\n3.8.1 DRY Coding\nThis idea of making your workflow more efficient by reducing, reusing and recycling your code is summarised by the DRY acronym: don’t repeat yourself.\nThis can be boiled down to three main points:\n\n\nif you do something twice in a single script, then write a function to do that thing;\nif you want to use your function elsewhere within your project, then save it in a separate script;\nIf you want to use your function across projects, then add it to a package.\n\n\nOf course, like with scoping projects in the first place, this requires some level of clairvoyance: you have to be able to look into the future and see whether you’ll use a function in another script or project. This is difficult, bordering on impossible. So in practice, this is done retrospectively - you find a second script or project that needs a function then pull it out its own separate file or include it in a package.\nAs a rule of thumb, if you are having to consider whether or not to make the function more widely available then you should do it. It takes much less effort to do this work now, while it’s fresh in your mind, than to have to re-familiarise yourself with the code in several years time.\nLet’s now look at how to implement those sub-bullet points: “when you write a function, document it” and “when you write a function, test it”.\n\n3.8.2 Rememer how to use your own code\nWhen you come to use a function written by somebody else, you likely have to refer to their documentation to teach or to remind yourself of things like what the expected inputs are and how exactly the method is implemented.\nWhen writing your own functions you should create documentation that fills the same need. Even if the function is just for personal use, over time you’ll forget exactly how it works.\n\nWhen you write a function, document it.\n\nBut what should that documentation contain?\n\nInputs\nOutputs\nExample use cases\nAuthor (if not obvious or working in a team)\n\nYour documentation should describe the inputs and outputs of your function, some simple example uses. If you are working in a large team, the documentation should also indicate who wrote the function and who’s responsible for maintaining it over time.\n\n3.8.3 {roxygen2} for documentation\nIn the same way that we used the here package to simplify our file path problems, we’ll use the roxygen2 package to simplify our testing workflow.\nThe {roxygen2} package gives us an easily insert-able temple for documenting our functions. This means we don’t have to waste our time and energy typing out and remembering boilerplate code. It also puts our documentation in a format that allows us to get hints and auto-completion for our own functions, just like the functions we use from packages that are written by other people.\nTo use Roxygen, you only need to install it once - it doesn’t need to be loaded with a library call at the top of your script. After you’ve done this, and with your cursor inside a function definition, you can then insert skeleton code to document that function in one of two ways: you can either use the Rstudio menu or the keyboard short cut for your operating system.\n\ninstall.packages(\"roxygen2\")\nWith cursor inside function: Code > Insert Roxygen Skeleton\nKeyboard shortcut: cmd + option + shift + r or crtl + option + shift + r\n\nFill out relevant fields\n\n3.8.4 An {roxygen2} example\nBelow, we’ve got an example of an Roxygen skeleton to document a function that calculates the geometric mean of a vector. Here, the hash followed by an apostrophe is a special type of comment. It indicates that this is function documentation rather than just a regular comment.\n\n#' Title\n#'\n#' @param x \n#' @param remove_NA \n#'\n#' @return\n#' @export\n#'\n#' @examples\ngeometric_mean <- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n\nWe’ll fill in all of the fields in this skeleton apart from export, which we’ll remove. If we put this function in a R package, then the export field makes it available to users of that package. Since this is just a standalone function we won’t need the export field, though keeping it wouldn’t actually cause us any problems either.\n\n#' Calculate the geometric mean of a numeric vector\n#'\n#' @param x numeric vector\n#' @param remove_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. \n#'\n#' @return the geometric mean of the values in `x`, a numeric scalar value. \n#'\n#' @examples\n#' geometric_mean(x = 1:10)\n#' geometric_mean(x = c(1:10, NA), remove_NA = TRUE)\n#' \ngeometric_mean <- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n\nOnce we have filled in the skeleton documentation it might look something like this. We have described what the function does, what the expected inputs are and what the user can expect as an output. We’ve also given an few simple examples of how the function can be used.\nFor more on Roxygen, see the package documentation or the chapter of R packages on function documentation.\n\n3.8.5 Checking Your Code\n\nIf you write a function, test it.\n\nTesting code has two main purposes:\n\nTo warn or prevent user misuse (e.g. strange inputs),\nTo catch edge cases.\n\nOn top of explaining how our functions should work, we really ought to check that they do work. This is the job of unit testing.\nWhenever you write a function you should test that it works as you intended it to. Additionally, you should test that your function is robust to being misused by the user. Depending on the context, this might be accidental or malicious misuse. Finally, you should check that the function behaves properly for strange, but still valid, inputs. These are known as edge cases.\nTesting can be a bit of a brutal process, you’ve just created a beautiful function and now you’re job is to do your best to break it!\n\n3.8.6 An Informal Testing Workflow\n\nWrite a function\nExperiment with the function in the console, try to break it\nFix the break and repeat.\n\nProblems: Time consuming and not reproducible.\nAn informal approach to testing your code might be to first write a function and then play around with it in the console to check that it behaves well when you give it obvious inputs, edge cases and deliberately wrong inputs. Each time you manage to break the function, you edit it to fix the problem and then start the process all over again.\nThis is testing the code, but only informally. There’s no record of how you have tried to break your code already. The problem with this approach is that when you return to this code to add a new feature, you’ll probably have forgotten at least one of the informal tests you ran the first time around. This goes against our efforts towards reproducibility and automation. It also makes it very easy to break code that used to work just fine.\n\n3.8.7 A Formal Testing Workflow\nWe can formalise this testing workflow by writing our tests in their own R script and saving them for future reference. Remember from the first lecture that these should be saved in the tests/ directory, the structure of which should mirror that of the src/ directory for your project. All of the tests for one function should live in a single file, which is named after that function.\nOne way of writing these tests is to use lots of if statements. The testthat can do some of that syntactic heavy lifting for us. It has lots of helpful functions to test that the output of your function is what you expect.\n\n\n\n\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1, NA), remove_NA = FALSE),\n  expected = NA)\n\n# Error: geometric_mean(x = c(1, NA), remove_NA = FALSE) not equal to NA.\n# Types not compatible: double is not logical\n\nIn this example, we have an error because our function returns a logical NA rather than a double NA. Yes, R really does have different types of NA for different types of missing data, it usually just handles these nicely in the background for you.\nThis subtle difference is probably not something that you would have spotted on your own, until it caused you trouble much further down the line. This rigorous approach is one of the benefits of of using the testthat functions.\nTo fix this test we change out expected output to NA_real_.\n\n\n\nWe’ll revisit the testthat package in the live session this week, when we will learn how to use it to test functions within our own packages."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#summary",
    "href": "103-workflows-organising-your-code.html#summary",
    "title": "3  Code",
    "section": "\n3.9 Summary",
    "text": "3.9 Summary\n\nFunctional and Object Oriented Programming\nStructuring your scripts\nStyling your code\nReduce, reuse, recycle\nDocumenting and testing\n\nLet’s wrap up by summarising what we have learned in this chapter.\nWe started out with a discussion on the differences between functional and object oriented programming. While R is capable of both, data science work tends to have more of a functional flavour to it.\nWe’ve then described how to structure your scripts and style your code to make it as human-friendly and easy to debug as possible.\nFinally, we discussed how to write DRY code that is well documented and tested."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#session-information",
    "href": "103-workflows-organising-your-code.html#session-information",
    "title": "3  Code",
    "section": "\n3.10 Session Information",
    "text": "3.10 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: magrittr(v.2.0.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), pkgload(v.1.3.2.1), timechange(v.0.2.0), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), stringr(v.1.5.0), tools(v.4.2.2), xfun(v.0.39), cli(v.3.6.1), htmltools(v.0.5.5), rprojroot(v.2.0.3), yaml(v.2.3.7), digest(v.0.6.33), assertthat(v.0.2.1), lifecycle(v.1.0.3), crayon(v.1.5.2), brio(v.1.1.3), purrr(v.1.0.1), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), testthat(v.3.1.10), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), emo(v.0.0.0.9000), stringi(v.1.7.12), pander(v.0.6.5), compiler(v.4.2.2), desc(v.1.4.2), generics(v.0.1.3), jsonlite(v.1.8.7), lubridate(v.1.9.2) and renv(v.0.16.0)"
  },
  {
    "objectID": "110-workflows-checklist.html",
    "href": "110-workflows-checklist.html",
    "title": "Workflows Checklist",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "110-workflows-checklist.html#videos-chapters",
    "href": "110-workflows-checklist.html#videos-chapters",
    "title": "Workflows Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\n\nOrganising your work (30 min) [slides]\n\n\nNaming Files (20 min) [slides]\n\n\nOrganising your code (27 min) [slides]"
  },
  {
    "objectID": "110-workflows-checklist.html#reading",
    "href": "110-workflows-checklist.html#reading",
    "title": "Workflows Checklist",
    "section": "Reading",
    "text": "Reading\nUse the workflows section of the reading list to support and guide your exploration of this week’s materials. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "110-workflows-checklist.html#tasks",
    "href": "110-workflows-checklist.html#tasks",
    "title": "Workflows Checklist",
    "section": "Tasks",
    "text": "Tasks\nCore:\n\nFind 3 data science projects on Github and explore how they organise their work. Write a post on the EdStem forum that links to all three, and in a couple of paragraphs describe the content and structure of one project.\nCreate your own project directory (or directories) for this course and its assignments.\nWrite two of your own R functions. The first should calculate the geometric mean of a numeric vector. The second should calculate the rolling arithmetic mean of a numeric vector.\n\nBonus:\n\nRe-factor an old project to match the project organisation and coding guides for this course. This might be a small research project, class notes or a collection of homework assignments. Use an R-based project if possible. If you only have python projects, then either translate these to R or apply the PEP8 style guide. Take care to select a suitably sized project so that this is a meaningful exercise but does not take more than a few hours.\nIf you are able to do so, host your re-factored project publicly and share it with the rest of the class on the EdStem Discussion forum."
  },
  {
    "objectID": "110-workflows-checklist.html#live-session",
    "href": "110-workflows-checklist.html#live-session",
    "title": "Workflows Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then create a minimal R package to organise and test the functions you have written.\nPlease come to the live session prepared to discuss the following points:\n\nDid you make the assignment projects as subdirectories or as their stand alone projects? Why?\nWhat were some terms that you had not met before during the readings? How did you find their meanings?\nWhat did you have to consider when writing your rolling mean function?"
  },
  {
    "objectID": "200-data-introduction.html",
    "href": "200-data-introduction.html",
    "title": "Acquiring and Sharing Data",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nData can be difficult to acquire and gnarly when you get it.\nThe raw material that you work with as a data scientist is, unsurprisingly, data. In this part of the course we will focus on the different ways in which data can be stored, distributed and obtained.\nBeing able to obtain and read a dataset is often a surprisingly large hurdle in getting a new data science project off the ground. The skill of being able to source and read data from many locations is usually sanitised during a statistics programme: you’re given a ready-to-go, cleaned CSV file and all focus is placed on modelling. This week aims to remedy that by equipping you with the skills to acquire and manage your own data.\nWe will begin this week by explore different file types. This dictates what type of information you can store, who can access that information and how they read that it into R. We will then turn our attention to the case when data are not given to you directly. We will learn how to obtain data from a raw webpage and how to request data that via a service known as an API."
  },
  {
    "objectID": "201-data-tabular.html",
    "href": "201-data-tabular.html",
    "title": "4  Tabular Data",
    "section": "",
    "text": "Important\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is undergoing heavy restructuring and may be confusing or incomplete.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "201-data-tabular.html#loading-tabular-data",
    "href": "201-data-tabular.html#loading-tabular-data",
    "title": "4  Tabular Data",
    "section": "\n4.1 Loading Tabular Data",
    "text": "4.1 Loading Tabular Data\n\n\n\n\n\n\nRecall that simpler, open source formats improve accessibility and reproducibility. We will begin by reading in three open data formats for tabular data.\n\nrandom-data.csv\nrandom-data.tsv\nrandom-data.txt\n\nEach of these data sets contains 26 observations of 4 variables:\n\n\nid, a Roman letter identifier;\n\ngaussian, standard normal random variates;\n\ngamma, gamma(1,1) random variates;\n\nuniform, uniform(0,1) random variates.\n\n\n4.1.1 Base R\n\nrandom_df <- read.csv(file = 'random-data.csv')\nprint(random_df)\n#>    id    gaussian      gamma    uniform\n#> 1   a -1.20706575 0.98899970 0.22484576\n#> 2   b  0.27742924 0.03813386 0.08498474\n#> 3   c  1.08444118 1.09462335 0.63729826\n#> 4   d -2.34569770 1.49301101 0.43101637\n#> 5   e  0.42912469 5.40361248 0.07271609\n#> 6   f  0.50605589 1.72386539 0.80240202\n#> 7   g -0.57473996 1.95357133 0.32527830\n#> 8   h -0.54663186 0.07807803 0.75728904\n#> 9   i -0.56445200 0.21198194 0.58427152\n#> 10  j -0.89003783 0.20803673 0.70883941\n#> 11  k -0.47719270 2.08607862 0.42697577\n#> 12  l -0.99838644 0.49463708 0.34357270\n#> 13  m -0.77625389 0.77171305 0.75911999\n#> 14  n  0.06445882 0.37216648 0.42403021\n#> 15  o  0.95949406 1.88207991 0.56088725\n#> 16  p -0.11028549 0.76622568 0.11613577\n#> 17  q -0.51100951 0.50488585 0.30302180\n#> 18  r -0.91119542 0.22979791 0.47880269\n#> 19  s -0.83717168 0.75637275 0.34483055\n#> 20  t  2.41583518 0.62435969 0.60071414\n#> 21  u  0.13408822 0.64638373 0.07608332\n#> 22  v -0.49068590 0.11247545 0.95599261\n#> 23  w -0.44054787 0.11924307 0.02220682\n#> 24  x  0.45958944 4.91805535 0.84171063\n#> 25  y -0.69372025 0.60282666 0.63244245\n#> 26  z -1.44820491 0.64446571 0.31009417\n\nOutput is a data.frame object. (List of vectors with some nice methods)\n\n4.1.2 {readr}\n\n\nrandom_tbl <- readr::read_csv(file = 'random-data.csv')\n#> Rows: 26 Columns: 4\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): id\n#> dbl (3): gaussian, gamma, uniform\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nprint(random_tbl)\n#> # A tibble: 26 × 4\n#>   id    gaussian  gamma uniform\n#>   <chr>    <dbl>  <dbl>   <dbl>\n#> 1 a       -1.21  0.989   0.225 \n#> 2 b        0.277 0.0381  0.0850\n#> 3 c        1.08  1.09    0.637 \n#> 4 d       -2.35  1.49    0.431 \n#> 5 e        0.429 5.40    0.0727\n#> 6 f        0.506 1.72    0.802 \n#> # ℹ 20 more rows\n\nOutput is a tibble object. (List of vectors with some nicer methods)\n\n4.1.2.1 Benefits of readr::read_csv()\n\n\nIncreased speed (approx. 10x) and progress bar.\nStrings are not coerced to factors. No more stringsAsFactors = FALSE\nNo row names and nice column names.\nReproducibility bonus: does not depend on operating system.\n\n4.1.3 WTF: Tibbles\n\n4.1.3.1 Printing\n\nDefault to first 10 rows and as many columns as will comfortably fit on your screen.\nCan adjust this behaviour in the print call:\n\n\n# print first three rows and all columns\nprint(random_tbl, n = 3, width = Inf)\n#> # A tibble: 26 × 4\n#>   id    gaussian  gamma uniform\n#>   <chr>    <dbl>  <dbl>   <dbl>\n#> 1 a       -1.21  0.989   0.225 \n#> 2 b        0.277 0.0381  0.0850\n#> 3 c        1.08  1.09    0.637 \n#> # ℹ 23 more rows\n\nBonus: Colour formatting in IDE and each column tells you it’s type.\n\n4.1.3.2 Subsetting\nSubsetting tibbles will always return another tibble.\n\n# Row Subsetting\nrandom_tbl[1, ] # returns tibble\nrandom_df[1, ]  # returns data.frame\n\n# Column Subsetting\nrandom_tbl[ , 1]      # returns tibble\nrandom_df[ , 1]       # returns vector\n\n# Combined Subsetting\nrandom_tbl[1, 1]      # returns 1x1 tibble\nrandom_df[1, 1]       # returns single value\n\n\nThis helps to avoids edge cases associated with working on data frames.\n\n4.1.4 Other {readr} functions\nSee readr documentation, there are lots of useful additional arguments that can help you when reading messy data.\nFunctions for reading and writing other types of tabular data work analogously.\n\n4.1.4.1 Reading Tabular Data\n\nlibrary(readr)\nread_tsv(\"random-data.tsv\")\nread_delim(\"random-data.txt\", delim = \" \")\n\n\n4.1.4.2 Writing Tabular Data\n\nwrite_csv(random_tbl, \"random-data-2.csv\")\nwrite_tsv(random_tbl, \"random-data-2.tsv\")\nwrite_delim(random_tbl, \"random-data-2.tsv\", delim = \" \")\n\n\n4.1.5 Need for Speed\nSome times you have to load lots of large data sets, in which case a 10x speed-up might not be sufficient.\nIf each data set still fits inside RAM, then check out data.table::fread() which is optimised for speed. (Alternatives exist for optimal memory usage and data too large for working memory, but not covered here.)\nNote: While it can be much faster, the resulting data.table object lacks the consistancy properties of a tibble so be sure to check for edge cases, where the returned value is not what you might expect."
  },
  {
    "objectID": "201-data-tabular.html#tidy-data",
    "href": "201-data-tabular.html#tidy-data",
    "title": "4  Tabular Data",
    "section": "\n4.2 Tidy Data",
    "text": "4.2 Tidy Data\n\n4.2.1 Wide vs. Tall Data\n\n4.2.1.1 Wide Data\n\nFirst column has unique entries\nEasier for humans to read and compute on\nHarder for machines to compute on\n\n4.2.1.2 Tall Data\n\nFirst column has repeating entries\nHarder for humans to read and compute on\nEasier for machines to compute on\n\n4.2.1.3 Examples\nExample 1 (Wide)\n\n\nPerson \nAge \nWeight \nHeight \n\n\n\nBob\n32\n168\n180\n\n\nAlice\n24\n150\n175\n\n\nSteve\n64\n144\n165\n\n\n\nExample 1 (Tall)\n\n\nPerson \nVariable \nValue \n\n\n\nBob\nAge\n32\n\n\nBob\nWeight\n168\n\n\nBob\nHeight\n180\n\n\nAlice\nAge\n24\n\n\nAlice\nWeight\n150\n\n\nAlice\nHeight\n175\n\n\nSteve\nAge\n64\n\n\nSteve\nWeight\n144\n\n\nSteve\nHeight\n165\n\n\n\n[Source: Wikipedia - Wide and narrow data]\nExample 2 (Wide)\n\n\nTeam\nPoints\nAssists\nRebounds\n\n\n\nA\n88\n12\n22\n\n\nB\n91\n17\n28\n\n\nC\n99\n24\n30\n\n\nD\n94\n28\n31\n\n\n\nExample 2 (Tall)\n\n\nTeam\nVariable\nValue\n\n\n\nA\nPoints\n88\n\n\nA\nAssists\n12\n\n\nA\nRebounds\n22\n\n\nB\nPoints\n91\n\n\nB\nAssists\n17\n\n\nB\nRebounds\n28\n\n\nC\nPoints\n99\n\n\nC\nAssists\n24\n\n\nC\nRebounds\n30\n\n\nD\nPoints\n94\n\n\nD\nAssists\n28\n\n\nD\nRebounds\n31\n\n\n\n[Source: Statology - Long vs wide data]\n\n4.2.1.4 Pivoting Wider and Longer\n\nError control at input and analysis is format-dependent.\nSwitching between long and wide formats useful to control errors.\nEasy with the tidyr package functions\n\n\ntidyr::pivot_longer()\ntidyr::pivot_wider()\n\n\n4.2.2 Tidy What?\n\n\n[Image: R4DS - Chapter 12]\n\n\nTidy Data is an opinionated way to store tabular data.\nImage Source: Chapter 12 of R for Data Science.\n\nEach column corresponds to a exactly one measured variable\nEach row corresponds to exactly one observational unit\nEach cell contains exactly one value.\n\nBenefits of tidy data\n\nConsistent data format: Reduces cognitive load and allows specialised tools (functions) to efficiently work with tabular data.\nVectorisation: Keeping variables as columns allows for very efficient data manipulation. (this goes back to data frames and tibbles being lists of vectors)\n\n4.2.3 Example - Tidy Longer\nConsider trying to plot these data as time series. The year variable is trapped in the column names!\n\n\n\n\n#> # A tibble: 3 × 3\n#>   country     `1999` `2000`\n#>   <chr>        <dbl>  <dbl>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\n\nTo tidy this data, we need to pivot_longer(). We will turn the column names into a new year variable and retaining cell contents as a new variable called cases.\n\nlibrary(magrittr)\n\ncountries %>% \n  tidyr::pivot_longer(cols = c(`1999`,`2000`), names_to = \"year\", values_to = \"cases\")\n#> # A tibble: 6 × 3\n#>   country     year   cases\n#>   <chr>       <chr>  <dbl>\n#> 1 Afghanistan 1999     745\n#> 2 Afghanistan 2000    2666\n#> 3 Brazil      1999   37737\n#> 4 Brazil      2000   80488\n#> 5 China       1999  212258\n#> 6 China       2000  213766\n\nMuch better!\n\n4.2.4 Example - Tidy Wider\nThere are other times where we might have to widen our data to tidy it.\nThis example is not tidy. Why not?\n\n\nTeam\nVariable\nValue\n\n\n\nA\nPoints\n88\n\n\nA\nAssists\n12\n\n\nA\nRebounds\n22\n\n\nB\nPoints\n91\n\n\nB\nAssists\n17\n\n\nB\nRebounds\n28\n\n\nC\nPoints\n99\n\n\nC\nAssists\n24\n\n\nC\nRebounds\n30\n\n\nD\nPoints\n94\n\n\nD\nAssists\n28\n\n\nD\nRebounds\n31\n\n\n\nThe observational unit here is a team. However, each variable should be a stored in a separate column, with cells containing their values.\nTo tidy this data we first generate it as a tibble. We use the tribble() function, which allows us to create a tibble row-wise rather than column-wise.\n\n\n\nWe can then tidy it by creating new columns for each value of the current Variable column and taking the values for these from the current Value column.\n\ntournament %>% \n  tidyr::pivot_wider(\n    id_cols = \"Team\", \n    names_from = \"Variable\",\n    values_from = \"Value\")\n#> # A tibble: 4 × 4\n#>   Team  Points Assists Rebounds\n#>   <chr>  <dbl>   <dbl>    <dbl>\n#> 1 A         88      12       22\n#> 2 B         91      17       28\n#> 3 C         99      24       30\n#> 4 D         94      28       31\n\n\n4.2.5 Other helpful functions\nThe pivot_*() family of functions resolve issues with rows (too many observations per row or rows per observation).\n\nThere are similar helper functions to solve column issues:\n\nMultiple variables per column: tidyr::separate(),\nMultiple columns per variable: tidyr::unite().\n\n4.2.6 Missing Data\nIn tidy data, every cell contains a value. Including cells with missing values.\n\nMissing values are coded as NA (generic) or a type-specific NA, such as NA_character_.\nThe readr family of read_*() function have good defaults and helpful na argument.\nExplicitly code NA values when collecting data, avoid ambiguity: ” “, -999 or worst of all 0.\nMore on missing values in EDA videos…"
  },
  {
    "objectID": "201-data-tabular.html#wrapping-up",
    "href": "201-data-tabular.html#wrapping-up",
    "title": "4  Tabular Data",
    "section": "\n4.3 Wrapping Up",
    "text": "4.3 Wrapping Up\n\nReading in tabular data by a range of methods\nIntroduced the tibble and tidy data (+ tidy not always best)\nTools for tidying messy tabular data"
  },
  {
    "objectID": "201-data-tabular.html#session-information",
    "href": "201-data-tabular.html#session-information",
    "title": "4  Tabular Data",
    "section": "\n4.4 Session Information",
    "text": "4.4 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: magrittr(v.2.0.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), pillar(v.1.9.0), compiler(v.4.2.2), tools(v.4.2.2), digest(v.0.6.33), bit(v.4.0.5), jsonlite(v.1.8.7), evaluate(v.0.21), lifecycle(v.1.0.3), tibble(v.3.2.1), pkgconfig(v.2.0.3), rlang(v.1.1.1), cli(v.3.6.1), rstudioapi(v.0.15.0), yaml(v.2.3.7), parallel(v.4.2.2), xfun(v.0.39), fastmap(v.1.1.1), withr(v.2.5.0), dplyr(v.1.1.2), knitr(v.1.43), generics(v.0.1.3), vctrs(v.0.6.3), htmlwidgets(v.1.6.2), hms(v.1.1.3), bit64(v.4.0.5), tidyselect(v.1.2.0), glue(v.1.6.2), R6(v.2.5.1), fansi(v.1.0.4), vroom(v.1.6.3), rmarkdown(v.2.23), pander(v.0.6.5), readr(v.2.1.4), tzdb(v.0.4.0), tidyr(v.1.3.0), purrr(v.1.0.1), htmltools(v.0.5.5), renv(v.0.16.0), utf8(v.1.2.3) and crayon(v.1.5.2)"
  },
  {
    "objectID": "202-data-webscraping.html",
    "href": "202-data-webscraping.html",
    "title": "5  Web Scraping",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "202-data-webscraping.html#scraping-webpage-data-using-rvest",
    "href": "202-data-webscraping.html#scraping-webpage-data-using-rvest",
    "title": "5  Web Scraping",
    "section": "\n5.1 Scraping webpage data using {rvest}",
    "text": "5.1 Scraping webpage data using {rvest}\n\n\n\nYou can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this chapter we will cover the basics of scraping webpages, following the vignette for the {rvest} package."
  },
  {
    "objectID": "202-data-webscraping.html#what-is-a-webpage",
    "href": "202-data-webscraping.html#what-is-a-webpage",
    "title": "5  Web Scraping",
    "section": "\n5.2 What is a webpage?",
    "text": "5.2 What is a webpage?\nBefore we can even hope to get data from a webpage, we first need to understand what a webpage is.\nWebpages are written in a similar way to LaTeX: the content and styling of webpages are handled separately and are coded using plain text files.\nIn fact, websites go one step further than LaTeX. The content and styling of websites are written in different files and in different languages. HTML (HyperText Markup Language) is used to write the content and then CSS (Cascading Style Sheets) are used to control the appearance of that content when it’s displayed to the user."
  },
  {
    "objectID": "202-data-webscraping.html#html",
    "href": "202-data-webscraping.html#html",
    "title": "5  Web Scraping",
    "section": "\n5.3 HTML",
    "text": "5.3 HTML\nA basic HTML page with no styling applied might look something like this:\n<html>\n<head>\n  <title>Page title</title>\n</head>\n<body>\n  <h1 id='first'>A level 1 heading</h1>\n  <p>Hello World!</p>\n  <p>Here is some plain text &amp; <b>some bold text.</b></p>\n  <img src='myimg.png' width='100' height='100'>\n</body>\n\n5.3.1 HTML elements\n\nJust like XML data files, HTML has a hierarchical structure. This structure is crafted using HTML elements. Each HTML element is made up of of a start tag, optional attributes, an end tag.\nWe can see each of these in the first level header, where <h1> is the opening tag, id='first' is an additional attribute and </h1> is the closing tag. Everything between the opening and closing tag are the contents of that element. There are also some special elements that consist of only a single tag and its optional attributes. An example of this is the <img> tag.\nSince < and > are used for start and end tags, you can’t write them directly in an HTML document. Instead, you have to use escape characters. This sounds fancy, but it’s just an alternative way to write characters that serve some special function within a language.\nYou can write greater than &gt; and less than as &lt;. You might notice that those escapes use an ampersand (&). This means that if you want a literal ampersand on your webpage, you have to escape too using &amp;.\nThere are a wide range of possible HTML tags and escapes. We’ll cover the most common tags in this lecture and you don’t need to worry about escapes too much because rvest will automatically handle them for you.\n\n5.3.2 Important HTML Elements\nIn all, there are in excess of 100 HTML elements. The most important ones for you to know about are:\n\nThe <html> element, that must enclose every HTML page. The <html> element must have two child elements within it. The <head> element contains metadata about the document, like the page title that is shown in the browser tab and the CSS style sheet that should be applied. The <body> element then contains all of the content that you see in the browser.\nBlock elements are used to give structure to the page. These are elements like headings, sub-headings and so on from <h1> all the way down to <h6>. This category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.\nFinally, inline tags like <b> for bold, <i> for italics, and <a> for hyperlinks are used to format text inside block elements.\n\nWhen you come across a tag that you’ve never seen before, you can find out what it does with just a little bit of googling. A good resource here is the MDN Web Docs which are produced by Mozilla, the company that makes the Firefox web browser. The W3schools website is another great resource for web development and coding resources more generally."
  },
  {
    "objectID": "202-data-webscraping.html#html-attributes",
    "href": "202-data-webscraping.html#html-attributes",
    "title": "5  Web Scraping",
    "section": "\n5.4 HTML Attributes",
    "text": "5.4 HTML Attributes\nWe’ve seen one example of a header with an additional attribute. More generally, all tags can have named attributes. These attributes are contained within the opening tag and look something like:\n<tag attribute1='value1' attribute2='value2'>element contents</tag>\nTwo of the most important attributes are id and class. These attributes are used in conjunction with the CSS file to control the visual appearance of the page. These are often very useful to identify the elements that you are interested in when scraping data off a page."
  },
  {
    "objectID": "202-data-webscraping.html#css-selectors",
    "href": "202-data-webscraping.html#css-selectors",
    "title": "5  Web Scraping",
    "section": "\n5.5 CSS Selectors",
    "text": "5.5 CSS Selectors\nThe Cascading Style Sheet is used to describe how your HTML content will be displayed. To do this, CSS has it’s own system for selecting elements of a webpage, called CSS selectors.\nCSS selectors define patterns for locating the HTML elements that a particular style should be applied to. A happy side-effect of this is that they can sometimes be very useful for scraping, because they provide a concise way of describing which elements you want to extract.\nCSS Selectors can work on the level of an element type, a class, or a tag and these can be used in a nested (or cascading) way.\n\nThe p selector will select all paragraph <p> elements.\nThe .title selector will select all elements with class “title”.\nThe p.special selector will select all<p> elements with class “special”.\nThe #title selector will select the element with the id attribute “title”.\n\nWhen you want to select a single element id attributes are particularly useful because that must be unique within a html document. Unfortunately, this is only helpful if the developer added an id attribute to the element(s) you want to scrape!\nIf you want to learn more CSS selectors I recommend starting with the fun CSS diner tutorial to build a base of knowledge and then using the W3schools resources as a reference to explore more webpages in the wild."
  },
  {
    "objectID": "202-data-webscraping.html#which-attributes-and-selectors-do-you-need",
    "href": "202-data-webscraping.html#which-attributes-and-selectors-do-you-need",
    "title": "5  Web Scraping",
    "section": "\n5.6 Which Attributes and Selectors Do You Need?",
    "text": "5.6 Which Attributes and Selectors Do You Need?\nTo scrape data from a webpage, you first have to identify the tag and attribute combinations that you are interested in gathering.\nTo find your elements of interest, you have three options. These go from hardest to easiest but also from most to least robust.\n\nright click + “inspect page source” (F12)\nright click + “inspect”\nRvest Selector Gadget (very useful but fallible)\n\nInspecting the source of some familiar websites can be a useful way to get your head around these concepts. Beware though that sophisticated webpages can be quite intimidating. A good place to start is with simpler, static websites such as personal websites, rather than the dynamic webpages of online retailers or social media platforms."
  },
  {
    "objectID": "202-data-webscraping.html#reading-html-with-rvest",
    "href": "202-data-webscraping.html#reading-html-with-rvest",
    "title": "5  Web Scraping",
    "section": "\n5.7 Reading HTML with {rvest}\n",
    "text": "5.7 Reading HTML with {rvest}\n\nWith rvest, reading a html page can be as simple as loading in tabular data.\n\nhtml <- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\n\nThe class of the resulting object is an xml_document. This type of object is from the low-level package xml2, which allows you to read xml files into R.\n\nclass(html)\n#> [1] \"xml_document\" \"xml_node\"\n\nWe can see that this object is split into several components: first is some metadata on the type of document we have scraped, followed by the head and then the body of that html document.\n\nhtml\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#> [2] <body class=\"nav-fixed\">\\n\\n<div id=\"quarto-search-results\"></div>\\n   ...\n\nWe have several possible approaches to extracting information from this document."
  },
  {
    "objectID": "202-data-webscraping.html#extracting-html-elements",
    "href": "202-data-webscraping.html#extracting-html-elements",
    "title": "5  Web Scraping",
    "section": "\n5.8 Extracting HTML elements",
    "text": "5.8 Extracting HTML elements\nIn rvest you can extract a single element with html_element(), or all matching elements with html_elements(). Both functions take a document object and one or more CSS selectors as inputs.\n\nlibrary(rvest)\n\nhtml %>% html_elements(\"h1\")\n#> {xml_nodeset (1)}\n#> [1] <h1>Teaching</h1>\nhtml %>% html_elements(\"h2\")\n#> {xml_nodeset (2)}\n#> [1] <h2 id=\"toc-title\">On this page</h2>\n#> [2] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History</h2>\nhtml %>% html_elements(\"p\")\n#> {xml_nodeset (7)}\n#> [1] <p>I am fortunate to have had the opportunity to teach in a variety of ...\n#> [2] <p>Developing and teaching a number of modules in statistics, data sci ...\n#> [3] <p>Supervising undergraduate, postgraduate and doctoral research proje ...\n#> [4] <p>Adapting and leading short courses on scientific writing and commun ...\n#> [5] <p>Running workshops and computer labs for undergraduate and postgradu ...\n#> [6] <p>Speaking at univerisity open days and providing one-to-one tuition  ...\n#> [7] <p>I am an associate fellow of the Higher Education Academy, which you ...\n\nYou can also combine and nest these selectors. For example you might want to extract all links that are within paragraphs and all second level headers.\n\nhtml %>% html_elements(\"p a,h2\")\n#> {xml_nodeset (3)}\n#> [1] <h2 id=\"toc-title\">On this page</h2>\n#> [2] <a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\" ...\n#> [3] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History</h2>"
  },
  {
    "objectID": "202-data-webscraping.html#extracting-data-from-html-elements",
    "href": "202-data-webscraping.html#extracting-data-from-html-elements",
    "title": "5  Web Scraping",
    "section": "\n5.9 Extracting Data From HTML Elements",
    "text": "5.9 Extracting Data From HTML Elements\nNow that we’ve got the elements we care about extracted from the complete document. But how do we get the data we need out of those elements?\nYou’ll usually get the data from either the contents of the HTML element or else from one of it’s attributes. If you’re really lucky, the data you need will already be formatted for you as a HTML table or list.\n\n5.9.1 Extracting text\nThe functions rvest::html_text() and rvest::html_text2() can be used to extract the plain text contents of an HTML element.\n\nhtml %>% \n  html_elements(\"#teaching li\") %>% \n  html_text2()\n#> [1] \"Developing and teaching a number of modules in statistics, data science and data ethics. These were predominantly at the postgradute-level and include courses designed for in-person and remote learning.\"\n#> [2] \"Supervising undergraduate, postgraduate and doctoral research projects.\"                                                                                                                                   \n#> [3] \"Adapting and leading short courses on scientific writing and communication.\"                                                                                                                               \n#> [4] \"Running workshops and computer labs for undergraduate and postgraduate modules.\"                                                                                                                           \n#> [5] \"Speaking at univerisity open days and providing one-to-one tuition to high school students.\"\n\nThe difference between html_text() and html_text2() is in how they handle whitespace. In HTML whitespace and line breaks have very little influence over how the code is interpreted by the computer (this is similar to R but very different from Python). html_text() will extract the text as it is in the raw html, while html_text2() will do its best to extract the text in a way that gives you something similar to what you’d see in the browser.\n\n5.9.2 Extracting Attributes\nAttributes are also used to record information that you might like to collect. For example, the destination of links are stored in the href attribute and the source of images is stored in the src attribute.\nAs an example of this, consider trying to extract the twitter link from the icon in the page footer. This is quite tricky to locate in the html source, so I used the Selector Gadget to help find the correct combination of elements.\n\nhtml %>% html_element(\".compact:nth-child(1) .nav-link\")\n#> {html_node}\n#> <a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\">\n#> [1] <i class=\"bi bi-twitter\" role=\"img\">\\n</i>\n\nTo extract the href attribute from the scraped element, we use the rvest::html_attr() function.\n\nhtml %>% \n  html_elements(\".compact:nth-child(1) .nav-link\") %>% \n  html_attr(\"href\")\n#> [1] \"https://www.twitter.com/zakvarty\"\n\nNote: rvest::html_attr() will always return a character string (or list of character strings). If you are extracting an attribute that describes a quantity, such as the width of an image, you’ll need to convert this from a string to your required data type. For example, of the width is measures in pixels you might use as.integer().\n\n5.9.3 Extracting tables\nHTML tables are composed in a similar, nested manner to LaTeX tables.\nThere are four main elements to know about that make up an HTML table:\n\n\n<table>,\n\n<tr> (table row),\n\n<th> (table heading),\n\n<td> (table data).\n\nHere’s our simple example data, formatted as an HTML table:\n\nhtml_2 <- minimal_html(\"\n  <table>\n    <tr>\n      <th>Name</th>\n      <th>Number</th>\n    </tr>\n    <tr>\n      <td>A</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>B</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>C</td>\n      <td>3</td>\n    </tr>\n  </table>\n  \")\n\nSince tables are a common way to store data, rvest includes a useful function html_table() that converts directly from an HTML table into a tibble.\n\nhtml_2 %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 3 × 2\n#>   Name  Number\n#>   <chr>  <int>\n#> 1 A          1\n#> 2 B          2\n#> 3 C          3\n\nApplying this to our real scraped data we can easily extract the table of taught courses.\n\nhtml %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 31 × 3\n#>   Year      Course                                     Role    \n#>   <chr>     <chr>                                      <chr>   \n#> 1 \"2022-23\" Data Science                               Lecturer\n#> 2 \"\"        Ethics in Data Science I, II and III       Lecturer\n#> 3 \"\"        Data Ethics for Digital Chemistry          Lecturer\n#> 4 \"\"        Y1 research projects: point process models Lecturer\n#> 5 \"2021-22\" Supervised Learning                        Lecturer\n#> 6 \"\"        Ethics in Data Science I                   Lecturer\n#> # ℹ 25 more rows"
  },
  {
    "objectID": "202-data-webscraping.html#tip-for-building-tibbles",
    "href": "202-data-webscraping.html#tip-for-building-tibbles",
    "title": "5  Web Scraping",
    "section": "\n5.10 Tip for Building Tibbles",
    "text": "5.10 Tip for Building Tibbles\nWhen scraping data from a webpage, your end-goal is typically going to be constructing a data.frame or a tibble.\nIf you are following our description of tidy data, you’ll want each row to correspond some repeated unit on the HTML page. In this case, you should\n\nUse html_elements() to select the elements that contain each observation unit;\nUse html_element() to extract the variables from each of those observations.\n\nTaking this approach guarantees that you’ll get the same number of values for each variable, because html_element() always returns the same number of outputs as inputs. This is vital when you have missing data - when not every observation unit has a value for every variable of interest.\nAs an example, consider this extract of text about the starwars dataset.\n\nstarwars_html <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>\n    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>\n    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>\n    <li><b>R4-P17</b> is a <i>droid</i></li>\n  </ul>\n  \")\n\nThis is an unordered list where each list item corresponds to one observational unit (one character from the starwars universe). The name of the character is given in bold, the character species is specified in italics and the weight of the character is denoted by the .weight class. However, some characters have only a subset of these variables defined: for example Yoda has no species entry.\nIf we try to extract each element directly, our vectors of variable values are of different lengths. We don’t know where the missing values should be, so we can’t line them back up to make a tibble.\n\nstarwars_html %>% html_elements(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %>% html_elements(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %>% html_elements(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"\n\nWhat we should do instead is start by extracting all of the list item elements using html_elements(). Once we have done this, we can then use html_element() to extract each variable for all characters. This will pad with NAs, so that we can collate them into a tibble.\n\nstarwars_characters <- starwars_html %>% html_elements(\"li\")\n\nstarwars_characters %>% html_element(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %>% html_element(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %>% html_element(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\n\n\ntibble::tibble(\n  name = starwars_characters %>% html_element(\"b\") %>% html_text2(),\n  species = starwars_characters %>% html_element(\"i\") %>% html_text2(),\n  weight = starwars_characters %>% html_element(\".weight\") %>% html_text2()\n)\n#> # A tibble: 4 × 3\n#>   name   species weight\n#>   <chr>  <chr>   <chr> \n#> 1 C-3PO  droid   167 kg\n#> 2 R2-D2  droid   96 kg \n#> 3 Yoda   <NA>    66 kg \n#> 4 R4-P17 droid   <NA>"
  },
  {
    "objectID": "202-data-webscraping.html#session-information",
    "href": "202-data-webscraping.html#session-information",
    "title": "5  Web Scraping",
    "section": "\n5.11 Session Information",
    "text": "5.11 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: rvest(v.1.0.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), xml2(v.1.3.5), magrittr(v.2.0.3), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), fansi(v.1.0.4), stringr(v.1.5.0), httr(v.1.4.6), tools(v.4.2.2), xfun(v.0.39), utf8(v.1.2.3), cli(v.3.6.1), selectr(v.0.4-2), htmltools(v.0.5.5), yaml(v.2.3.7), digest(v.0.6.33), tibble(v.3.2.1), lifecycle(v.1.0.3), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), curl(v.5.0.1), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), stringi(v.1.7.12), pander(v.0.6.5), pillar(v.1.9.0), compiler(v.4.2.2), jsonlite(v.1.8.7), renv(v.0.16.0) and pkgconfig(v.2.0.3)"
  },
  {
    "objectID": "203-data-apis.html",
    "href": "203-data-apis.html",
    "title": "6  APIs",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "203-data-apis.html#aquiring-data-via-an-api",
    "href": "203-data-apis.html#aquiring-data-via-an-api",
    "title": "6  APIs",
    "section": "\n6.1 Aquiring Data Via an API",
    "text": "6.1 Aquiring Data Via an API\n\n\n\nWe’ve already established that you can’t always rely on tidy, tabular data to land on your desk.\nSometimes you are going to have to go out and gather data for yourself. We have already seen how to scrape information directly from the HTML source of a webpage. But surely there has to be an easer way. Thankfully, there often is!\nIn this chapter we will cover the basics of obtaining data via an API. This material draws together the Introduction to APIs book by Brian Cooksey and the DIY web data section of STAT545 at the University of British Columbia."
  },
  {
    "objectID": "203-data-apis.html#why-do-i-need-to-know-about-apis",
    "href": "203-data-apis.html#why-do-i-need-to-know-about-apis",
    "title": "6  APIs",
    "section": "\n6.2 Why do I need to know about APIs?",
    "text": "6.2 Why do I need to know about APIs?\n\nAn API, or application programming interface, is a set of rules that allows different software applications to communicate with each other.\n\nAs a data scientist, you will often need to access data that is stored on remote servers or in cloud-based services. APIs provide a convenient way for data scientists to programmatically retrieve this data, without having to manually download data sets or and process them locally on their own computer.\nThis has multiple benefits including automation and standardisation of data sharing.\n\nAutomation: It is much faster for a machine to process a data request than a human. Having a machine handling data requests also scales much better as either the number or the complexity of data requests grows. Additionally, there is a lower risk of introducing human error. For example, a human might accidentally share the wrong data, which can have serious legal repercussions.\nStandardisation: Having a machine process data requests requires the format of these requests and the associated responses to be standardised. This allows data sharing and retrieval to become a reproducible and programmatic aspect of our work."
  },
  {
    "objectID": "203-data-apis.html#what-is-an-api",
    "href": "203-data-apis.html#what-is-an-api",
    "title": "6  APIs",
    "section": "\n6.3 What is an API?",
    "text": "6.3 What is an API?\nSo then, if APIs are so great, what exactly are they?\nIn human-to-human communication, the set of rules governing acceptable behaviour is known as etiquette. Depending on when or where you live, social etiquette can be rather strict. The rules for computer-to-computer communication take this to a whole new level, because with machines there can be no room left for interpretation.\nThe set of rules governing interactions between computers or programmes is known as a protocol.\nAPIs provide a standard protocol for different programs to interact with one another. This makes it easier for developers to build complex systems by leveraging the functionality of existing services and platforms. The benefits of working in a standardised and modular way apply equally well to sharing data as they do to writing code or organising files.\nThere are two sides to communication and when machines communicate these are known as the server and the client.\n\n\n\nServers can seem intimidating, because unlike your laptop or mobile phone they don’t have their own input and output devices; they have no keyboard, no monitor, and no a mouse. Despite this, servers are just regular computers that are designed to store data and run programmes. Servers don’t have their own input or output devices because they are intended to be used remotely, via another computer. There is no need for a screen or a mouse if the user is miles away. Nothing scary going on here!\nPeople often find clients much less intimidating - they are simply any other computer or application that might contact the sever."
  },
  {
    "objectID": "203-data-apis.html#http",
    "href": "203-data-apis.html#http",
    "title": "6  APIs",
    "section": "\n6.4 HTTP",
    "text": "6.4 HTTP\nThis leads us one step further down the rabbit-hole. An API is a protocol that defines the rules of how applications communicate with one another. But how does this communication happen?\nHTTP (Hypertext Transfer Protocol) is the dominant mode communication on the World Wide Web. You can see the secure version of HTTP, HTTPS, at the start of most web addresses up at the top of your browser. For example:\nhttps://www.zakvarty.com/blog\nHTTP is the foundation of data communication on the web and is used to transfer files (such as text, images, and videos) between web servers and clients.\n\n\n\nTo understand HTTP communications, I find it helpful to imagine the client and the server as being a customer and a waiter at a restaurant. The client makes some request to the server, which then tries to comply before giving a response. The server might respond to confirm that the request was completed successfully. Alternatively, the server might respond with an error message, which is (hopefully) informative about why the request could not be completed.\nThis request-response model is the basis for HTTP, the communication system used by the majority of APIs."
  },
  {
    "objectID": "203-data-apis.html#http-requests",
    "href": "203-data-apis.html#http-requests",
    "title": "6  APIs",
    "section": "\n6.5 HTTP Requests",
    "text": "6.5 HTTP Requests\nAn HTTP request consists of:\n\nUniform Resource Locator (URL) [unique identifier for a thing]\nMethod [tells server the type of action requested by client]\nHeaders [meta-information about request, e.g. device type]\nBody [Data the client wants to send to the server]\n\n\n\n\n\n6.5.1 URL\nThe URL in a HTTP request specifies where that request is going to be made, for example http://example.com.\n\n6.5.2 Method\nThe action that the client wants to take is indicated by a set of well-defined methods or HTTP verbs. The most common HTTP verbs are GET, POST, PUT, PATCH, and DELETE.\nThe GET verb is used to retrieve a resource from the server, such as a web page or an image. The POST verb is used to send data to the server, such as when submitting a form or uploading a file. The PUT verb is used to replace a resource on the server with a new one, while the PATCH verb is used to update a resource on the server without replacing it entirely. Finally, the DELETE verb is used to delete a resource from the server.\nIn addition to these common HTTP verbs, there are also several less frequently used verbs. These are used for specialized purposes, such as requesting only the headers of a resource, or testing the connectivity between the client and the server.\n\n6.5.3 Header\nThe request headers contain meta-information about the request. This is where information about the device type would be included within the request.\n\n6.5.4 Body\nFinally, the body of the request contains the data that the client is providing to the server."
  },
  {
    "objectID": "203-data-apis.html#http-responses",
    "href": "203-data-apis.html#http-responses",
    "title": "6  APIs",
    "section": "\n6.6 HTTP Responses",
    "text": "6.6 HTTP Responses\nWhen the server receives a request it will attempt to fulfil it and then send a response back to the client.\n\n\n\nA response has a similar structure to a request apart from:\n\nresponses do not have a URL,\nresponses do not have a method,\nresponses have a status code.\n\n\n6.6.1 Status Codes\nThe status code is a 3 digit number, each of which has a specific meaning. Some common error codes that you might (already have) come across are:\n\n200: Success,\n404: Page not found (all 400s are errors),\n503: Page down.\n\nIn a data science context, a successful response will return the requested data within the data field. This will most likely be given in JSON or XML format."
  },
  {
    "objectID": "203-data-apis.html#authentication",
    "href": "203-data-apis.html#authentication",
    "title": "6  APIs",
    "section": "\n6.7 Authentication",
    "text": "6.7 Authentication\nNow that we know how applications communicate, you might ask how we can control who has access to the API and what types of request they can make. This can be done by the server setting appropriate permissions for each client. But then how does the server verify that the client is really who is claims to be?\nAuthentication is a way to ensure that only authorized clients are able to access an API. This is typically done by the server requiring each client to provide some secret information that uniquely identifies them, whenever they make requests to the API. This information allows the API server to validate the authenticity this user before it authorises the request.\n\n6.7.1 Basic Authentication\nThere are various ways to implement API authentication.\nBasic authentication involves each legitimate client having a username and password. An encrypted version of these is included in the Authorization header of the HTTP request. If the hear matches with the server’s records then the request is processed. If not, then a special status code (401) is returned to the client.\nBasic authentication is dangerous because it does not put any restrictions on what a client can do once they are authorised. Additional, individualised restrictions can be added by using an alternative authentication scheme.\n\n6.7.2 API Key Authentication\nAn API key is long, random string of letters and numbers that is assigned to each authorised user. An API key is distinct from the user’s password and keys are typically issued by the service that provides an API. Using keys rather than basic authentication allows the API provider to track and limit the usage of their API.\nFor example, a provider may issue a unique API key to each developer or organization that wants to use the API. The provider can then limit access to certain data. They could also limit the number of requests that each key can make in a given time period or prevent access to certain administrative functions, like changing passwords or deleting accounts.\nUnlike Basic Authentication, there is no standard way of a client sharing a key with the server. Depending on the API this might be in the Authorization field of the header, at the end of the URL (http://example.com?api_key=my_secret_key), or within the body of the data."
  },
  {
    "objectID": "203-data-apis.html#api-wrappers",
    "href": "203-data-apis.html#api-wrappers",
    "title": "6  APIs",
    "section": "\n6.8 API wrappers",
    "text": "6.8 API wrappers\nWe’ve learned a lot about how the internet works. Fortunately, a lot of the time we won’t have to worry about all of that new information other than for debugging purposes.\nIn the best case scenario, a very kind developer has written a “wrapper” function for the API. These wrappers are functions in R that will construct the HTTP request for you. If you are particularly lucky, the API wrapper will also format the response for you, converting it from XML or JSON back into an R object that is ready for immediate use."
  },
  {
    "objectID": "203-data-apis.html#geonames-wrapper",
    "href": "203-data-apis.html#geonames-wrapper",
    "title": "6  APIs",
    "section": "\n6.9 {geonames} wrapper",
    "text": "6.9 {geonames} wrapper\nrOpenSci has a curated list of many wrappers for accessing scientific data using R. We will focus on the GeoNames API, which gives open access to a geographical database. To access this data, we will use wrapper functions provided by the {geonames} package.\nThe aim here is to illustrate the important steps of getting started with a new API.\n\n6.9.1 Set up\nBefore we can get any data from the GeoNames API, we first need to do a little bit of set up.\n\nInstall and load {geonames} from CRAN\n\n\n#install.packages(\"geonames\")\nlibrary(geonames)\n\n\nCreate a user account for the GeoNames API\n\n\n\n\n\n\n\n\n\n\nActivate the account (see activation email)\n\n\n\n\n\n\n\n\n\n\nEnable the free web services for your GeoNames account by logging in at this link.\nTell R your credentials for GeoNames.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe could use the following code to tell R our credentials, but we absolutely should not.\n\noptions(geonamesUsername = \"example_username\")\n\nThis would save our username as an environment variable, but it also puts our API credentials directly into the script. If we share the script with others (internally, externally or publicly) we would be sharing our credentials too. Not good!\n\n\n\n6.9.2 Keep it Secret, Keep it Safe\nThe solution to this problem is to add our credentials as environment variables in our .Rprofile rather than in this script. The .Rprofile is an R script that is run at the start of every session. It can be created and edited directly, but can also be created and edited from within R.\nTo make/open your .Rprofile use the edit_r_profile() function from the usethis package.\n\nlibrary(usethis)\nusethis::edit_r_profile()\n\nWithin this file, add options(geonamesUsername=\"example_username\") on a new line, remembering to replace example_username with your own GeoNames username.\nThe final step is to check this this file ends with a blank line, save it and restart R. Then we are all set to start using {geonames}.\nThis set up procedure is indicative of most API wrappers, but of course the details will vary between each API. This is why good documentation is important!\n\nIf you are using renv to track the versions of R and the packages you are using take care. For renv to be effective you have to place your project level .Rprofile under version control - this means you might accidentally share your API credentials.\nA workaround for this problem is to create an R file that you source() from within the project level .Rprofile but is included in .gitignore, so that your credentials remain secret.\n\n\n6.9.3 Using {geonames}\n\nGeoNames has a whole host of different geo-datasets that you can explore. As a first example, let’s get all of the geo-tagged wikipedia articles that are within 1km of Imperial College London.\n\nimperial_coords <- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km <- 1\n\nimperial_neighbours <- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\n\nLooking at the structure of imperial_neighbours we can see that it is a data frame with one row per geo-tagged wikipedia article.\n\nstr(imperial_neighbours)\n#> 'data.frame':    204 obs. of  13 variables:\n#>  $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n#>  $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n#>  $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n#>  $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n#>  $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n#>  $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n#>  $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n#>  $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n#>  $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n#>  $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n#>  $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n#>  $ thumbnailImg: chr  NA NA NA NA ...\n#>  $ geoNameId   : chr  NA NA NA NA ...\n\nTo confirm we have the correct location we can inspect the title of the first five neighbours.\n\nimperial_neighbours$title[1:5]\n#> [1] \"Department of Mechanical Engineering, Imperial College London\"             \n#> [2] \"Imperial College Business School\"                                          \n#> [3] \"Exhibition Road\"                                                           \n#> [4] \"Imperial College School of Medicine\"                                       \n#> [5] \"Department of Civil and Environmental Engineering, Imperial College London\"\n\nNothing too surprising here, mainly departments of the college and Exhibition Road, which runs along one side of the campus. These sorts of check are important - I initially forgot the minus in the longitude and was getting results in East London!"
  },
  {
    "objectID": "203-data-apis.html#what-if-there-is-no-wrapper",
    "href": "203-data-apis.html#what-if-there-is-no-wrapper",
    "title": "6  APIs",
    "section": "\n6.10 What if there is no wrapper?",
    "text": "6.10 What if there is no wrapper?\nIf there is not a wrapper function, we can still access APIs fairly easilty using the httr package.\nWe will look at an example using OMDb, which is an open source version of IMDb, to get information about the movie Mean Girls.\nTo use the OMDB API you will once again need to request a free API key, follow a verification link and add your API key to your .Rprofile.\n\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n\nYou can then restart R and safely access your API key from within your R session.\n\n# Load your API key into the current R session\nombd_api_key <- getOption(\"OMDB_API_Key\")\n\nUsing the documentation for the API, requests have URLs of the following form, where terms in angular brackets should be replaced by you.\nhttp://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>\nWith a little bit of effort, we can write a function that composes this type of request URL for us. We will using the glue package to help us join strings together.\n\n\n\nRunning the example we get:\n\nmean_girls_request <- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\n\nWe can then use the httr package to construct our request and store the response we get.\n\n#> [1] 200\n\nThankfully it was a success! If you get a 401 error code here, check that you have clicked the activation link for your API key.\nThe full structure of the response is quite complicated, but we can easily extract the requested data using content()\n\n#> $Title\n#> [1] \"Mean Girls\"\n#> \n#> $Year\n#> [1] \"2004\"\n#> \n#> $Rated\n#> [1] \"PG-13\"\n#> \n#> $Released\n#> [1] \"30 Apr 2004\"\n#> \n#> $Runtime\n#> [1] \"97 min\"\n#> \n#> $Genre\n#> [1] \"Comedy\"\n#> \n#> $Director\n#> [1] \"Mark Waters\"\n#> \n#> $Writer\n#> [1] \"Rosalind Wiseman, Tina Fey\"\n#> \n#> $Actors\n#> [1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n#> \n#> $Plot\n#> [1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n#> \n#> $Language\n#> [1] \"English, German, Vietnamese, Swahili\"\n#> \n#> $Country\n#> [1] \"United States, Canada\"\n#> \n#> $Awards\n#> [1] \"7 wins & 25 nominations\"\n#> \n#> $Poster\n#> [1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n#> \n#> $Ratings\n#> $Ratings[[1]]\n#> $Ratings[[1]]$Source\n#> [1] \"Internet Movie Database\"\n#> \n#> $Ratings[[1]]$Value\n#> [1] \"7.1/10\"\n#> \n#> \n#> $Ratings[[2]]\n#> $Ratings[[2]]$Source\n#> [1] \"Rotten Tomatoes\"\n#> \n#> $Ratings[[2]]$Value\n#> [1] \"84%\"\n#> \n#> \n#> $Ratings[[3]]\n#> $Ratings[[3]]$Source\n#> [1] \"Metacritic\"\n#> \n#> $Ratings[[3]]$Value\n#> [1] \"66/100\"\n#> \n#> \n#> \n#> $Metascore\n#> [1] \"66\"\n#> \n#> $imdbRating\n#> [1] \"7.1\"\n#> \n#> $imdbVotes\n#> [1] \"403,600\"\n#> \n#> $imdbID\n#> [1] \"tt0377092\"\n#> \n#> $Type\n#> [1] \"movie\"\n#> \n#> $DVD\n#> [1] \"01 Aug 2013\"\n#> \n#> $BoxOffice\n#> [1] \"$86,058,055\"\n#> \n#> $Production\n#> [1] \"N/A\"\n#> \n#> $Website\n#> [1] \"N/A\"\n#> \n#> $Response\n#> [1] \"True\""
  },
  {
    "objectID": "203-data-apis.html#wrapping-up",
    "href": "203-data-apis.html#wrapping-up",
    "title": "6  APIs",
    "section": "\n6.11 Wrapping up",
    "text": "6.11 Wrapping up\nWe have learned a bit more about how the internet works, the benefits of using an API to share data and how to request data from Open APIs.\nWhen obtaining data from the internet it’s vital that you keep your credentials safe, and that don’t do more work than is needed.\n\nKeep your API keys out of your code. Store them in your .Rprofile (and make sure this is not under version control!)\nScraping is always a last resort. Is there an API already?\nWriting your own code to access an API can be more painful than necessary.\nDon’t repeat other people, if a suitable wrapper exists then use it."
  },
  {
    "objectID": "203-data-apis.html#session-information",
    "href": "203-data-apis.html#session-information",
    "title": "6  APIs",
    "section": "\n6.12 Session Information",
    "text": "6.12 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: geonames(v.0.999)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), digest(v.0.6.33), R6(v.2.5.1), jsonlite(v.1.8.7), evaluate(v.0.21), httr(v.1.4.6), rlang(v.1.1.1), cli(v.3.6.1), curl(v.5.0.1), renv(v.0.16.0), rstudioapi(v.0.15.0), rmarkdown(v.2.23), tools(v.4.2.2), pander(v.0.6.5), glue(v.1.6.2), htmlwidgets(v.1.6.2), xfun(v.0.39), yaml(v.2.3.7), fastmap(v.1.1.1), compiler(v.4.2.2), htmltools(v.0.5.5) and knitr(v.1.43)"
  },
  {
    "objectID": "210-data-checklist.html",
    "href": "210-data-checklist.html",
    "title": "Checklist",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "210-data-checklist.html#videos-chapters",
    "href": "210-data-checklist.html#videos-chapters",
    "title": "Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nTabular Data (27 min) [slides]\nWeb Scraping (22 min) [slides]\nAPIs (25 min) [slides]"
  },
  {
    "objectID": "210-data-checklist.html#reading",
    "href": "210-data-checklist.html#reading",
    "title": "Checklist",
    "section": "Reading",
    "text": "Reading\nUse the Acquiring and Sharing Data section of the reading list to support and guide your exploration of this week’s topics. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "210-data-checklist.html#tasks",
    "href": "210-data-checklist.html#tasks",
    "title": "Checklist",
    "section": "Tasks",
    "text": "Tasks\nCore:\n\n\nRevisit the Projects that you explored on Github last week. This time look for any data or documentation files.\n\nAre there any file types that are new to you?\nIf so, are there packages or helper function that would let you read this data into R?\nWhy might you not find many data files on Github?\n\n\nPlay CSS Diner to familiarise yourself with some CSS selectors.\nIdentify 3 APIs that give access to data on topics that interest you. Write a post on the discussion forum describing the APIs and use one of them to load some data into R.\n\nScraping Book Reviews:\n\nVisit the Amazon page for R for Data Science. Write code to scrape the percentage of customers giving each “star” rating (5⭐, …, 1⭐).\nTurn your code into a function that will return a tibble of the form:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct\nn_reviews\npercent_5_star\npercent_4_star\npercent_3_star\npercent_2_star\npercent_1_star\nurl\n\n\nexample_name\n1000\n20\n20\n20\n20\n20\nwww.example.com\n\n\n\n\n\nGeneralise your function to work for other Amazon products, where the function takes as input a vector of product names and an associated vector of URLs.\nUse your function to compare the reviews of the following three books: R for Data Science, R packages and ggplot2.\n\nBonus:\n\nAdd this function to the R package you made last week, remembering to add tests and documentation."
  },
  {
    "objectID": "210-data-checklist.html#live-session",
    "href": "210-data-checklist.html#live-session",
    "title": "Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then work through some examples of how to read data from non-standard sources.\nPlease come to the live session prepared to discuss the following points:\n\nRoger Peng states that files can be imported and exported using readRDS() and saveRDS() for fast and space efficient data storage. What is the downside to doing so?\nWhat data types have you come across (that we have not discussed already) and in what context are they used?\nWhat do you have to give greater consideration to when scraping data than when using an API?"
  },
  {
    "objectID": "300-edav-intro.html",
    "href": "300-edav-intro.html",
    "title": "Data Exploration and Visualisation",
    "section": "",
    "text": "Important\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nNow that we have read our raw data into R we can start getting our data science project moving and being to see some initial returns on the time and effort that we have invested so far.\nIn this section we will explore how to wrangle, explore and visualise the data that forms the basis of our projects. These skills are often overlooked by folks coming into data science as being “soft skills” compared to modelling. However, I would argue that this is not the case because each of these tasks requires its own specialist knowledge and tools.\nAdditionally, these task make up the majority of data scientist’s work and are often where we can add the most value to an organisation. At this stage in a project we turn useless, messy data into a form that can be used; we derive initial insights from this data while making minimal assumptions; and we communicate all of this in an accurate and engaging way, to drive decision making both within and outwith the organisation."
  },
  {
    "objectID": "301-edav-wrangling.html",
    "href": "301-edav-wrangling.html",
    "title": "7  Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "301-edav-wrangling.html#what-is-data-wrangling",
    "href": "301-edav-wrangling.html#what-is-data-wrangling",
    "title": "7  Data Wrangling",
    "section": "\n7.1 What is Data Wrangling?",
    "text": "7.1 What is Data Wrangling?\n\n\n\nOkay, so you’ve got some data. That’s a great start!\nYou might have had it handed to you by a collaborator, requested it via an API or scraped it from the raw html of a webpage. In the worst case scenario, you’re an actual scientist (not just a data one) and you spent the last several months of your life painstakingly measuring flower petals or car parts. Now we really want to do something useful with that data.\nWe’ve seen already how you can load the data into R and pivot between wider and longer formats, but that probably isn’t enough to satisfy your curiosity. You want to be able to view your data, manipulate and subset it, create new variables from existing ones and cross-reference your dataset with others. All of these are things possible in R and are known under various collective names including data manipulation, data munging and data wrangling.\nI’ve decided to use the term data wrangling here. That’s because data manipulation sounds boring as heck and data munging is both unpleasant to say and makes me imagine we are squelching through some sort of information swamp.\nIn what follows, I’ll give a fly-by tour of tools for data wrangling in R, showing some examples along the way. I’ll focus on some of the most common and useful operations and link out to some more extensive guides for wrangling your data in R, that you can refer back to as you need them."
  },
  {
    "objectID": "301-edav-wrangling.html#example-data-sets",
    "href": "301-edav-wrangling.html#example-data-sets",
    "title": "7  Data Wrangling",
    "section": "\n7.2 Example Data Sets",
    "text": "7.2 Example Data Sets\n\n\n\n\n\n\n\n\n\n\n\nTo demonstrate some standard skills we will use two datasets. The mtcars data comes built into any R installation. The second data set we will look at is the penguins data from palmerpenguins.\n\n\nlibrary(palmerpenguins)\npengins <- palmerpenguins::penguins\ncars <- datasets::mtcars"
  },
  {
    "objectID": "301-edav-wrangling.html#viewing-your-data",
    "href": "301-edav-wrangling.html#viewing-your-data",
    "title": "7  Data Wrangling",
    "section": "\n7.3 Viewing Your Data",
    "text": "7.3 Viewing Your Data\n\n7.3.1 View()\n\nThe View() function can be used to create a spreadsheet-like view of your data. In RStudio this will open as a new tab.\nView() will work for any “matrix-like” R object, such as a tibble, data frame, vector or matrix. Note the capital letter - the function is called View(), not view().\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 head()\n\nFor large data sets, you might not want (or be able to) view it all at once. You can then use head() to view the first few rows. The integer argument n specifies the number of rows you would like to return.\n\nhead(x = pengins, n = 3)\n#> # A tibble: 3 × 8\n#>   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>   <fct>   <fct>             <dbl>         <dbl>             <int>       <int>\n#> 1 Adelie  Torgers…           39.1          18.7               181        3750\n#> 2 Adelie  Torgers…           39.5          17.4               186        3800\n#> 3 Adelie  Torgers…           40.3          18                 195        3250\n#> # ℹ 2 more variables: sex <fct>, year <int>\n\n\n7.3.3 str()\n\nAn alternative way to view the a large data set, or one with a complicated format is to examine its structure with str(). This is a useful way to inspect the structure of list-like objects, particularly when they’ve got a nested structure.\n\nstr(penguins)\n#> tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n#>  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n#>  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n#>  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n#>  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n#>  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n#>  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n7.3.4 names()\n\nFinally, if you just want to access the variable names you can do so with the names() function from base R.\n\nnames(penguins)\n#> [1] \"species\"           \"island\"            \"bill_length_mm\"   \n#> [4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n#> [7] \"sex\"               \"year\"\n\nSimilarly, you can explicitly access the row and column names of a data frame or tibble using colnames() or rownames().\n\ncolnames(cars)\n#>  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n#> [11] \"carb\"\n\n\nrownames(cars)\n#>  [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n#>  [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n#>  [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n#> [10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n#> [13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n#> [16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n#> [19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n#> [22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n#> [25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n#> [28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n#> [31] \"Maserati Bora\"       \"Volvo 142E\"\n\nIn the cars data, the car model are stored as the row names. This doesn’t really jive with our idea of tidy data - we’ll see how to fix that shortly."
  },
  {
    "objectID": "301-edav-wrangling.html#renaming-variables",
    "href": "301-edav-wrangling.html#renaming-variables",
    "title": "7  Data Wrangling",
    "section": "\n7.4 Renaming Variables",
    "text": "7.4 Renaming Variables\n\n7.4.1 colnames()\n\nThe function colnames() can be used to set, as well as to retrieve, column names.\n\ncars_renamed <- cars \ncolnames(cars_renamed)[1] <- \"miles_per_gallon\"\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cyl\"              \"disp\"            \n#>  [4] \"hp\"               \"drat\"             \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\"\n\n\n7.4.2 dplyr::rename()\n\nWe can also use functions from dplyr to rename columns. Let’s alter the second column name.\n\nlibrary(dplyr)\ncars_renamed <- rename(.data = cars_renamed, cylinders = cyl)\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cylinders\"        \"disp\"            \n#>  [4] \"hp\"               \"drat\"             \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\"\n\nThis could be done as part of a pipe, if we were making many alterations.\n\ncars_renamed <- cars_renamed %>% \n  rename(displacement = disp) %>% \n  rename(horse_power = hp) %>% \n  rename(rear_axel_ratio = drat)\n\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cylinders\"        \"displacement\"    \n#>  [4] \"horse_power\"      \"rear_axel_ratio\"  \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\"\n\nWhen using the dplyr function you have to remember the format new_name = old_name. This matches the format used to create a data frame or tibble, but is the opposite order to the python function of the same name and often catches people out.\nIn the section on creating new variables, we will see an alternative way of doing this by copying the column and then deleting the original."
  },
  {
    "objectID": "301-edav-wrangling.html#subsetting",
    "href": "301-edav-wrangling.html#subsetting",
    "title": "7  Data Wrangling",
    "section": "\n7.5 Subsetting",
    "text": "7.5 Subsetting\n\n7.5.1 Base R\nIn base R you can extract rows, columns and combinations thereof using index notation.\n\n# First row\npenguins[1, ]\n#> # A tibble: 1 × 8\n#>   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>   <fct>   <fct>             <dbl>         <dbl>             <int>       <int>\n#> 1 Adelie  Torgers…           39.1          18.7               181        3750\n#> # ℹ 2 more variables: sex <fct>, year <int>\n\n# First Column \npenguins[ , 1]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # ℹ 338 more rows\n\n# Rows 2-3 of columns 1, 2 and 4\npenguins[2:3, c(1, 2, 4)]\n#> # A tibble: 2 × 3\n#>   species island    bill_depth_mm\n#>   <fct>   <fct>             <dbl>\n#> 1 Adelie  Torgersen          17.4\n#> 2 Adelie  Torgersen          18\n\nUsing negative indexing you can remove rows or columns\n\n# Drop all but first row\npenguins[-(2:344), ]\n#> # A tibble: 1 × 8\n#>   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>   <fct>   <fct>             <dbl>         <dbl>             <int>       <int>\n#> 1 Adelie  Torgers…           39.1          18.7               181        3750\n#> # ℹ 2 more variables: sex <fct>, year <int>\n\n\n# Drop all but first column \npenguins[ , -(2:8)]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # ℹ 338 more rows\n\nYou can also select rows or columns by their names. This can be done using the bracket syntax ([ ]) or the dollar syntax ($).\n\npengins[ , \"species\"]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # ℹ 338 more rows\npenguins$species\n#>   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n#> [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n#> [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [344] Chinstrap\n#> Levels: Adelie Chinstrap Gentoo\n\nSince penguins is a tibble, these return different types of object. Subsetting a tibble with bracket syntax will return a tibble, while extracting a column using the dollar syntax returns a vector of values.\n\n7.5.2 filter() and select()\n\ndplyr has two functions for subsetting, filter() subsets by rows and select() subsets by column.\nIn both functions you list what you would like to retain. Filter and select calls can be piped together to subset based on row and column values.\n\npenguins %>% \n  select(species, island, body_mass_g)\n#> # A tibble: 344 × 3\n#>   species island    body_mass_g\n#>   <fct>   <fct>           <int>\n#> 1 Adelie  Torgersen        3750\n#> 2 Adelie  Torgersen        3800\n#> 3 Adelie  Torgersen        3250\n#> 4 Adelie  Torgersen          NA\n#> 5 Adelie  Torgersen        3450\n#> 6 Adelie  Torgersen        3650\n#> # ℹ 338 more rows\n\n\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(body_mass_g > 6000)\n#> # A tibble: 2 × 3\n#>   species island body_mass_g\n#>   <fct>   <fct>        <int>\n#> 1 Gentoo  Biscoe        6300\n#> 2 Gentoo  Biscoe        6050\n\nSubsetting rows can be inverted by negating the filter() statement\n\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000))\n#> # A tibble: 340 × 3\n#>   species island    body_mass_g\n#>   <fct>   <fct>           <int>\n#> 1 Adelie  Torgersen        3750\n#> 2 Adelie  Torgersen        3800\n#> 3 Adelie  Torgersen        3250\n#> 4 Adelie  Torgersen        3450\n#> 5 Adelie  Torgersen        3650\n#> 6 Adelie  Torgersen        3625\n#> # ℹ 334 more rows\n\nand dropping columns can done by selecting all columns except the one(s) you want to drop.\n\n#> # A tibble: 340 × 1\n#>   body_mass_g\n#>         <int>\n#> 1        3750\n#> 2        3800\n#> 3        3250\n#> 4        3450\n#> 5        3650\n#> 6        3625\n#> # ℹ 334 more rows"
  },
  {
    "objectID": "301-edav-wrangling.html#creating-new-variables",
    "href": "301-edav-wrangling.html#creating-new-variables",
    "title": "7  Data Wrangling",
    "section": "\n7.6 Creating New Variables",
    "text": "7.6 Creating New Variables\n\n7.6.1 Base R\nWe can create new variables in base R by assigning a vector of the correct length to a new column name.\n\ncars_renamed$weight <- cars_renamed$wt\n\nIf we then drop the original column from the data frame, this gives us an alternative way of renaming columns.\n\ncars_renamed <- cars_renamed[ ,-which(names(cars_renamed) == \"wt\")]\nhead(cars_renamed, n = 5)\n#>                   miles_per_gallon cylinders displacement horse_power\n#> Mazda RX4                     21.0         6          160         110\n#> Mazda RX4 Wag                 21.0         6          160         110\n#> Datsun 710                    22.8         4          108          93\n#> Hornet 4 Drive                21.4         6          258         110\n#> Hornet Sportabout             18.7         8          360         175\n#>                   rear_axel_ratio  qsec vs am gear carb weight\n#> Mazda RX4                    3.90 16.46  0  1    4    4  2.620\n#> Mazda RX4 Wag                3.90 17.02  0  1    4    4  2.875\n#> Datsun 710                   3.85 18.61  1  1    4    1  2.320\n#> Hornet 4 Drive               3.08 19.44  1  0    3    1  3.215\n#> Hornet Sportabout            3.15 17.02  0  0    3    2  3.440\n\nOne thing to be aware of is that this operation does not preserve column ordering.\nGenerally speaking, code that relies on columns being in a specific order is fragile - it breaks easily. If possible, you should try to write your code in another way that’s robust to column reordering. I’ve done that here when removing the wt column by looking up the column index as part of my code, rather than assuming it will always be the fourth column.\n\n7.6.2 dplyr::mutate()\n\nThe function from dplyr to create new columns is mutate(). Let’s create another column that has the car’s weight in kilogrammes rather than tonnes.\n\ncars_renamed <- cars_renamed %>% \n  mutate(weight_kg = weight * 1000)\n\ncars_renamed %>% \n  select(miles_per_gallon, cylinders, displacement, weight, weight_kg) %>% \n  head(n = 5)\n#>                   miles_per_gallon cylinders displacement weight weight_kg\n#> Mazda RX4                     21.0         6          160  2.620      2620\n#> Mazda RX4 Wag                 21.0         6          160  2.875      2875\n#> Datsun 710                    22.8         4          108  2.320      2320\n#> Hornet 4 Drive                21.4         6          258  3.215      3215\n#> Hornet Sportabout             18.7         8          360  3.440      3440\n\nYou can also create new columns that are functions of multiple other columns.\n\ncars_renamed <- cars_renamed %>% \n  mutate(cylinder_adjusted_mpg = miles_per_gallon / cylinders)\n\n\n7.6.3 rownames_to_column()\n\nOne useful example of adding an additional row to a data frame is to convert its row names to a column of the data fame.\n\ncars %>% \n  mutate(model = rownames(cars_renamed)) %>% \n  select(mpg, cyl, model) %>% \n  head(n = 5)\n#>                    mpg cyl             model\n#> Mazda RX4         21.0   6         Mazda RX4\n#> Mazda RX4 Wag     21.0   6     Mazda RX4 Wag\n#> Datsun 710        22.8   4        Datsun 710\n#> Hornet 4 Drive    21.4   6    Hornet 4 Drive\n#> Hornet Sportabout 18.7   8 Hornet Sportabout\n\nThere’s a neat function called rownames_to_column() in tibble which will add this as the first column and remove the row names all in one step.\n\ncars %>% \n  tibble::rownames_to_column(var = \"model\") %>% \n  head(n = 5)\n#>               model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n7.6.4 rowids_to_column()\n\nAnother function from tibble adds the row id of each observation as a new column. This is often useful when ordering or combining tables.\n\ncars %>% \n  tibble::rowid_to_column(var = \"row_id\") %>% \n  head(n = 5)\n#>   row_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "301-edav-wrangling.html#summaries",
    "href": "301-edav-wrangling.html#summaries",
    "title": "7  Data Wrangling",
    "section": "\n7.7 Summaries",
    "text": "7.7 Summaries\nThe summarise() function allows you to collapse a data frame into a single row, which using a summary statistic of your choosing.\nWe can calculate the average bill length of all penguins in a single summarise() function call.\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm))\n#> # A tibble: 1 × 1\n#>   average_bill_length_mm\n#>                    <dbl>\n#> 1                     NA\n\nSince we have missing values, we might instead want to calculate the mean of the recorded values.\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n#> # A tibble: 1 × 1\n#>   average_bill_length_mm\n#>                    <dbl>\n#> 1                   43.9\n\nWe can also use summarise() to gather multiple summaries in a single data frame.\n\nbill_length_mm_summary <- penguins %>% \n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\nbill_length_mm_summary\n#> # A tibble: 1 × 8\n#>    mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6\n\nIn all, this isn’t overly exciting. You might, rightly, wonder why you’d want to use these summarise() calls when we could just use the simpler base R calls directly.\nOne benefit is that the summarise calls ensure consistent output. However, the main advantage comes when you want to apply these summaries to distinct subgroups of the data."
  },
  {
    "objectID": "301-edav-wrangling.html#grouped-operations",
    "href": "301-edav-wrangling.html#grouped-operations",
    "title": "7  Data Wrangling",
    "section": "\n7.8 Grouped Operations",
    "text": "7.8 Grouped Operations\nThe real benefit of summarise() comes from its combination with group_by(). This allows to you calculate the same summary statistics for each level of a factor with only one additional line of code. Here we’re re-calculating the same set of summary statistics we just found for all penguins, but for each individual species.\n\npenguins %>% \n  group_by(species) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#> # A tibble: 3 × 9\n#>   species    mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  \n#> 2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\nYou can group by multiple factors to calculate summaries for each distinct combination of levels within your data set. Here we group by combinations of species and the island to which they belong.\n\npenguin_summary_stats <- penguins %>% \n  group_by(species, island) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#> `summarise()` has grouped output by 'species'. You can override using the\n#> `.groups` argument.\n\npenguin_summary_stats\n#> # A tibble: 5 × 10\n#> # Groups:   species [3]\n#>   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#> 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#> 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#> 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\n7.8.1 Ungrouping\nBy default, each call to summarise() will undo one level of grouping. This means that our previous result was still grouped by species.\n(We can see this in the tibble output above, and also by examining the structure of the returned data frame. This tells us that this is an S3 object of class “grouped_df”, which inherits its properties from a “tbl_df”, “tbl”, and “data.frame” objects.)\n\nclass(penguin_summary_stats)\n#> [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nSince we have grouped by two variables, R expects us to use two summaries before returning a data frame (or tibble) that is not grouped. One way to satisfy this is to use apply a second summary at the species level of grouping.\n\npenguin_summary_stats %>% \n  summarise_all(mean, na.rm = TRUE)\n#> # A tibble: 3 × 10\n#>   species   island  mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>      <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2\n#> 2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\nHowever, we won’t always want to do apply another summary. In that case, we can undo the grouping using ungroup(). Remembering to ungroup is a common gotcha and cause of confusion when working with multiple-group summaries.\n\nungroup(penguin_summary_stats)\n#> # A tibble: 5 × 10\n#>   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#> 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#> 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#> 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\nThere’s an alternative method to achieve the same thing in a single step when using dplyr versions 1.0.0 and above. This is to to set the .groups parameter of the summarise() function call, which determines the grouping of the returned data frame.\nThe .groups parameter and can take 4 possible values:\n\n“drop_last”: dropping the last level of grouping (The only option before v1.0.0);\n“drop”: All levels of grouping are dropped;\n“keep”: Same grouping structure as .data;\n“rowwise”: Each row is its own group.\n\nBy default, “drop_last” is used if all the results have 1 row and “keep” is used otherwise."
  },
  {
    "objectID": "301-edav-wrangling.html#reordering-factors",
    "href": "301-edav-wrangling.html#reordering-factors",
    "title": "7  Data Wrangling",
    "section": "\n7.9 Reordering Factors",
    "text": "7.9 Reordering Factors\nR stored factors as integer values, which it then maps to a set of labels. Only factor levels that appear in your data will be assigned a coded integer value and the mapping between factor levels and integers will depend on the order that the labels appear in your data.\nThis can be annoying, particularly when your factor levels relate to properties that aren’t numerical but do have an inherent ordering to them. In the example below, we have the t-shirt size of twelve people.\n\ntshirts <- tibble::tibble(\n  id = 1:12, \n  size = as.factor(c(\"L\", NA, \"M\", \"S\", \"XS\", \"M\", \"XXL\", \"L\", \"XS\", \"M\", \"L\", \"S\"))\n)\n\nlevels(tshirts$size)\n#> [1] \"L\"   \"M\"   \"S\"   \"XS\"  \"XXL\"\n\nIrritatingly, the sizes aren’t in order and extra large isn’t included because it’s not included in this particular sample. This leads to awkward looking summary tables and plots.\n\ntshirts %>% group_by(size) %>% summarise(count = n())\n#> # A tibble: 6 × 2\n#>   size  count\n#>   <fct> <int>\n#> 1 L         3\n#> 2 M         3\n#> 3 S         2\n#> 4 XS        2\n#> 5 XXL       1\n#> 6 <NA>      1\n\nWe can fix this by creating a new variable with the factors explicitly coded in the correct order. We also need to specify that we should not drop empty groups as part of group_by().\n\n#> # A tibble: 7 × 2\n#>   size_tidy count\n#>   <fct>     <int>\n#> 1 XS            2\n#> 2 S             2\n#> 3 M             3\n#> 4 L             3\n#> 5 XL            0\n#> 6 XXL           1\n#> # ℹ 1 more row"
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-factors",
    "href": "301-edav-wrangling.html#be-aware-factors",
    "title": "7  Data Wrangling",
    "section": "\n7.10 Be Aware: Factors",
    "text": "7.10 Be Aware: Factors\nAs we have seen a little already, categorical variables can cause issues when wrangling and presenting data in R. All of these problems are solvable using base R techniques but the forcats package provides tools for the most common of these problems. This includes functions for changing the order of factor levels or the values with which they are associated.\nSome examples functions from the package include:\n\n\nfct_reorder(): Reordering a factor by another variable.\n\nfct_infreq(): Reordering a factor by the frequency of values.\n\nfct_relevel(): Changing the order of a factor by hand.\n\nfct_lump(): Collapsing the least/most frequent values of a factor into “other”.\n\nExamples of each of these can be found in the forcats vignette or the factors chapter of R for data science."
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-strings",
    "href": "301-edav-wrangling.html#be-aware-strings",
    "title": "7  Data Wrangling",
    "section": "\n7.11 Be Aware: Strings",
<<<<<<< HEAD
    "text": "7.11 Be Aware: Strings\nWorking with and analysing text data is a skill unto itself. However, it is useful to be able to do some basic manipulation of character strings programatically.\nBecause R was developed as a statistical programming language, it is well suited to the computational and modelling aspects of working with text data but the base R string manipulation functions can be a bit unwieldy at times.\nThe stringr package aims to combat this by providing useful helper functions for a range of text management problems. Even when not analysing text data these can be useful, for example to remove prefixes on a lot of column names.\n\n\n\nSuppose we wanted to keep only the text following an underscore in these column names. We could do that by using a regular expression to extract lower-case or upper-case letters which follow an underscore.\n\nhead(poorly_named_df)\n#> # A tibble: 6 × 11\n#>   observation_id   V1_A    V2_B   V3_C   V4_D   V5_E   V6_F    V7_G   V8_H\n#>            <int>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\n#> 1              1 -1.26  -1.89   -0.112 -0.335  1.97   0.103  0.569  -0.630\n#> 2              2 -0.974  0.792   0.853  0.938  1.66   0.259  0.889   0.689\n#> 3              3  0.497 -0.882   2.74   0.169 -1.06  -1.64  -1.89   -0.178\n#> 4              4 -0.285  0.651  -0.269  0.126 -2.27  -0.764 -0.489   1.32 \n#> 5              5  0.549 -0.0597  1.30   0.170 -1.27  -0.967 -0.0801  1.31 \n#> 6              6 -0.246  0.579  -1.04  -0.897 -0.702  1.83  -0.606   0.145\n#> # ℹ 2 more variables: V9_I <dbl>, V10_J <dbl>\n\n\n#>  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n\nAlternatively, can avoid using regular expressions. We can split each column name at the underscore and keep only the second part of each string.\n\n# split column names at underscores and inspect structure of resuting object\nsplit_strings <- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n#> List of 11\n#>  $ : chr [1:2] \"observation\" \"id\"\n#>  $ : chr [1:2] \"V1\" \"A\"\n#>  $ : chr [1:2] \"V2\" \"B\"\n#>  $ : chr [1:2] \"V3\" \"C\"\n#>  $ : chr [1:2] \"V4\" \"D\"\n#>  $ : chr [1:2] \"V5\" \"E\"\n#>  $ : chr [1:2] \"V6\" \"F\"\n#>  $ : chr [1:2] \"V7\" \"G\"\n#>  $ : chr [1:2] \"V8\" \"H\"\n#>  $ : chr [1:2] \"V9\" \"I\"\n#>  $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n#>  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n\nAgain, unless you plan to work extensively with text data, I would recommend that you look up such string manipulations as you need them. The strings section of R for Data Science is a useful starting point."
=======
    "text": "7.11 Be Aware: Strings\nWorking with and analysing text data is a skill unto itself. However, it is useful to be able to do some basic manipulation of character strings programatically.\nBecause R was developed as a statistical programming language, it is well suited to the computational and modelling aspects of working with text data but the base R string manipulation functions can be a bit unwieldy at times.\nThe stringr package aims to combat this by providing useful helper functions for a range of text management problems. Even when not analysing text data these can be useful, for example to remove prefixes on a lot of column names.\nSuppose we wanted to keep only the text following an underscore in these column names. We could do that by using a regular expression to extract lower-case or upper-case letters which follow an underscore.\n\nhead(poorly_named_df)\n#&gt; # A tibble: 6 × 11\n#&gt;   observation_id   V1_A    V2_B   V3_C     V4_D   V5_E    V6_F   V7_G    V8_H\n#&gt;            &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1              1  0.219  1.13    0.759 -0.642   -0.350 -0.412   0.262 -0.640 \n#&gt; 2              2 -0.243 -0.0898 -0.206  2.25     0.393 -0.469  -0.132 -1.09  \n#&gt; 3              3  1.04  -0.817  -0.298 -1.38     0.197 -0.920  -1.17   0.525 \n#&gt; 4              4  1.46   0.116   1.84  -0.449   -1.96   1.85   -0.790  0.0298\n#&gt; 5              5 -2.17  -0.752  -1.26   0.884    1.13  -0.0235 -1.27  -0.899 \n#&gt; 6              6 -0.465 -0.772  -0.969  0.00666  0.651  0.699  -1.29  -2.13  \n#&gt; # ℹ 2 more variables: V9_I &lt;dbl&gt;, V10_J &lt;dbl&gt;\n\n\n#&gt;  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n\nAlternatively, can avoid using regular expressions. We can split each column name at the underscore and keep only the second part of each string.\n\n# split column names at underscores and inspect structure of resuting object\nsplit_strings &lt;- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n#&gt; List of 11\n#&gt;  $ : chr [1:2] \"observation\" \"id\"\n#&gt;  $ : chr [1:2] \"V1\" \"A\"\n#&gt;  $ : chr [1:2] \"V2\" \"B\"\n#&gt;  $ : chr [1:2] \"V3\" \"C\"\n#&gt;  $ : chr [1:2] \"V4\" \"D\"\n#&gt;  $ : chr [1:2] \"V5\" \"E\"\n#&gt;  $ : chr [1:2] \"V6\" \"F\"\n#&gt;  $ : chr [1:2] \"V7\" \"G\"\n#&gt;  $ : chr [1:2] \"V8\" \"H\"\n#&gt;  $ : chr [1:2] \"V9\" \"I\"\n#&gt;  $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n#&gt;  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n\nAgain, unless you plan to work extensively with text data, I would recommend that you look up such string manipulations as you need them. The strings section of R for Data Science is a useful starting point."
>>>>>>> 114321ad5d8b42044c4328125c2e93922034bdf5
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-date-times",
    "href": "301-edav-wrangling.html#be-aware-date-times",
    "title": "7  Data Wrangling",
    "section": "\n7.12 Be Aware: Date-Times",
    "text": "7.12 Be Aware: Date-Times\nRemember all the fuss we made about storing dates in the ISO standard format? That was because dates and times are complicated enough to work with before adding extra ambiguity.\n\\[ \\text{YYYY} - \\text{MM} - \\text{DD}\\]\nDates, times and time intervals have to reconcile two factors: the physical orbit of the Earth around the Sun and the social and geopolitical mechanisms that determine how we measure and record the passing of time. This makes the history of date and time records fascinating and can make working with this type of data complicated.\nMoving from larger to smaller time spans: leap years alter the number of days in a year, months are of variable length (with February’s length changing from year to year). If your data are measured in a place that uses daylight saving, then one day a year will be 23 hours long and another will be 25 hours long. To make things worse, the dates and the hour at which the clocks change are not uniform across countries, which might be in distinct time zones that themselves change over time.\nEven at the level of minutes and seconds we aren’t safe - since the Earth’s orbit is gradually slowing down a leap second is added approximately every 21 months. Nor are things any better when looking at longer time scales or across cultures, where we might have to account for different calendars: months are added removed and altered over time, other calendar systems still take different approaches to measuring time and using different units and origin points.\nWith all of these issues you have to be very careful when working with date and time data. Functions to help you with this can be found in the lubridate package, with examples in the dates and times chapter of R for data science."
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-relational-data",
    "href": "301-edav-wrangling.html#be-aware-relational-data",
    "title": "7  Data Wrangling",
    "section": "\n7.13 Be Aware: Relational Data",
    "text": "7.13 Be Aware: Relational Data\nWhen the data you need are stored across two or more data frames you need to be able to cross-reference those and match up values for observational unit. This sort of data is know as relational data, and is used extensively in data science.\nThe variables you use to match observational units across data frames are known as keys. The primary key belongs to the first table and the foreign key belongs to the secondary table. There are various ways to join these data frames, depending on if you want to retain.\n\n7.13.0.1 Join types\nYou might want to keep only observational units that have key variables values in both data frames, this is known as an inner join.\n\n\n\n\nInner join diagram. Source: R for Data Science\n\n\n\n\nYou might instead want to keep all units from the primary table but pad with NAs where there is not a corresponding foreign key in the second table. This results in an (outer) left-join.\n\n\n\n\nDiagram for left, right and outer joins. Source: R for Data Science\n\n\n\n\nConversely, you might keep all units from the second table but pad with NAs where there is not a corresponding foreign key in the primary table. This is imaginatively named an (outer) right-join.\nIn the (outer) full join, all observational units from either table are retained and all missing values are padded with NAs.\nThings get more complicated when keys don’t uniquely identify observational units in either one or both of the tables. I’d recommend you start exploring these ideas with the relational data chapter of R for Data Science.\n\n7.13.0.2 Why and where to learn more\nWorking with relational data is essential to getting any data science up and running out in the wilds of reality. This is because businesses and companies don’t store all of their data in a huge single csv file. For one this isn’t very efficient, because most cells would be empty. Secondly, it’s not a very secure approach, since you can’t grant partial access to the data. That’s why information is usually stored in many data frames (more generically known as tables) within one or more databases.\nThese data silos are created, maintained, accessed and destroyed using a relational data base management system. These management systems use code to manage and access the stored data, just like we have seen in the dplyr commands above. You might well have heard of the SQL programming language (and its many variants), which is a popular language for data base management and is the inspiration for the dplyr package and verbs.\nIf you’d like to learn more then there are many excellent introductory SQL books and courses, I’d recommend picking one that focuses on data analysis or data science unless you really want to dig into efficient storage and querying of databases."
  },
  {
    "objectID": "301-edav-wrangling.html#wrapping-up",
    "href": "301-edav-wrangling.html#wrapping-up",
    "title": "7  Data Wrangling",
    "section": "\n7.14 Wrapping up",
    "text": "7.14 Wrapping up\nWe have:\n\nLearned how to wrangle tabular data in R with dplyr\nMet the idea of relational data and dplyr’s relationship to SQL\nBecome aware of some tricky data types and packages that can help."
  },
  {
    "objectID": "301-edav-wrangling.html#session-information",
    "href": "301-edav-wrangling.html#session-information",
    "title": "7  Data Wrangling",
    "section": "\n7.15 Session Information",
    "text": "7.15 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: dplyr(v.1.1.2) and palmerpenguins(v.0.1.1)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), magrittr(v.2.0.3), tidyselect(v.1.2.0), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), fansi(v.1.0.4), stringr(v.1.5.0), tools(v.4.2.2), xfun(v.0.39), utf8(v.1.2.3), cli(v.3.6.1), withr(v.2.5.0), htmltools(v.0.5.5), yaml(v.2.3.7), digest(v.0.6.33), tibble(v.3.2.1), lifecycle(v.1.0.3), purrr(v.1.0.1), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), stringi(v.1.7.12), pander(v.0.6.5), compiler(v.4.2.2), pillar(v.1.9.0), generics(v.0.1.3), jsonlite(v.1.8.7), renv(v.0.16.0) and pkgconfig(v.2.0.3)"
  },
  {
    "objectID": "302-edav-analysis.html",
    "href": "302-edav-analysis.html",
    "title": "8  Exploratory Data Analysis",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "302-edav-analysis.html#introduction",
    "href": "302-edav-analysis.html#introduction",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.1 Introduction",
    "text": "8.1 Introduction\nExploratory data analysis is an essential stage in any data science project. It allows you to become familiar with the data you are working with while also to identify potential strategies for progressing the project and flagging any areas of concern.\nIn this chapter we will look at three different perspectives on exploratory data analysis: its purpose for you as a data scientist, its purpose for the broader team working on the project and finally its purpose for the project itself."
  },
  {
    "objectID": "302-edav-analysis.html#get-to-know-your-data",
    "href": "302-edav-analysis.html#get-to-know-your-data",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.2 Get to know your data",
    "text": "8.2 Get to know your data\nLet’s first focus on an exploratory data analysis from our own point of view, as data scientists.\nExploratory data analysis (or EDA) is a process of examining a data set to understand its overall structure, contents, and the relationships between the variables it contains. EDA is an iterative process that’s often done before building a model or making other data-driven decisions within a data science project.\n\nExploratory Data Analysis: quick and simple exerpts, summaries and plots to better understand a data set.\n\nOne key aspect of EDA is generating quick and simple summaries and plots of the data. These plots and summary statistics can help to quickly understand the distribution of and relationships between the recorded variables. Additionally, during an exploratory analysis you will familiarise yourself with the structure of the data you’re working with and how that data was collected.\n\n\n\n\nInvestigating marginal and pairwise relationships in the Iris dataset.\n\n\n\n\nSince EDA is an initial and iterative process, it’s rare that any component of the analysis will be put into production. Instead, the goal is to get a general understanding of the data that can inform the next steps of the analysis.\nIn terms of workflow, this means that using one or more notebooks is often an effective way of organising your work during an exploratory analysis. This allows for rapid iteration and experimentation, while also providing a level of reproducibility and documentation. Since notebooks allow you to combine code, plots, tables and text in a single document, this makes it easy to share your initial findings with stakeholders and project managers."
  },
  {
    "objectID": "302-edav-analysis.html#start-a-conversation",
    "href": "302-edav-analysis.html#start-a-conversation",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.3 Start a conversation",
    "text": "8.3 Start a conversation\n\nAn effective EDA sets a precedent for open communication with the stakeholder and project manager.\n\nWe’ve seen the benefits of an EDA for you as a data scientist, but this isn’t the only perspective.\nOne key benefit of an EDA is that it can kick-start your communication with subject matter experts and project managers. You can build rapport and trust early in a project’s life cycle by sharing your preliminary findings with these stakeholders . This can lead to a deeper understanding of both the available data and the problem being addressed for everyone involved. If done well, it also starts to build trust in your work before you even begin the modelling stage of a project.\n\n8.3.1 Communicating with specialists\nSharing an exploratory analysis will inevitably require a time investment. The graphics, tables, and summaries you produce need to be presented to a higher standard and explained in a way that is clear to a non-specialist. However, this time investment will often pay dividends because of the additional contextual knowledge that the domain-expert can provide. They have a deep understanding of the business or technical domain surrounding the problem. This can provide important insights that aren’t in the data itself, but which are vital to the project’s success.\nAs an example, these stakeholder conversations often reveal important features in the data generating or measurement process that should be accounted for when modelling. These details are usually left out of the data documentation because they would be immediately obvious to any specialist in that field.\n\n8.3.2 Communicating with project manager\nAn EDA can sometimes allow us to identify cases where the strength of signal within the available data is clearly insufficient to answer the question of interest. By clearly communicating this to the project manager, the project can be postponed while different, better quality or simply more data are collected. It’s important to note that this data collection is not trivial and can have a high cost in terms of both time and capital. It might be that collecting the data needed to answer a question will cost more than we’re likely to gain from knowing that answer. Whether the project is postponed or cancelled, this constitutes a successful outcome for the project, the aim is to to produce insight or profit - not to fit models for their own sake."
  },
  {
    "objectID": "302-edav-analysis.html#scope-your-project",
    "href": "302-edav-analysis.html#scope-your-project",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.4 Scope Your Project",
    "text": "8.4 Scope Your Project\n\nEDA is an initial assessment of whether the available data measure the correct values, in sufficient quality and quantity, to answer a particular question.\n\nA third view on EDA is as an initial assessment of whether the available data measure the correct values, with sufficient quality and quantity, to answer a particular question. In order for EDA to be successful, it’s important to take a few key steps.\nFirst, it’s important to formulate a specific question of interest or line of investigation and agree on it with the stakeholder. By having a clear question in mind, it will be easier to focus the analysis and identify whether the data at hand can answer it.\nNext, it’s important to make a record (if one doesn’t already exist) of how the data were collected, by whom it was collected, what each recorded variable represents and the units in which they are recorded. This meta-data is often known as a data sheet. Having this information in written form is crucial when adding a new collaborator to a project, so that they can understand the data generating and measurement processes, and are aware of the quality and accuracy of the recorded values."
  },
  {
    "objectID": "302-edav-analysis.html#investigate-your-data",
    "href": "302-edav-analysis.html#investigate-your-data",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.5 Investigate Your Data",
    "text": "8.5 Investigate Your Data\n\nEDA is an opportunitiy to quantify data completeness and investigate the possibility of informative missingness.\n\nIn addition, it’s essential to investigate and document the structure, precision, completeness and quantity of data available. This includes assessing the degree of measurement noise or misclassification in the data, looking for clear linear or non-linear dependencies between any of the variables, and identifying if any data are missing or if there’s any structure to this missingness. Other data features to be aware of are the presence of any censoring or whether some values tend to be missing together.\nFurthermore, a more advanced EDA might include a simulation study to estimate the amount of data needed to detect the smallest meaningful effect. This is more in-depth than a typical EDA but if you suspect that the signals within your data are weak relative to measurement noise, can help to demonstrate the limitations of the current line of enquiry with the information that is currently available."
  },
  {
    "objectID": "302-edav-analysis.html#what-is-not-eda",
    "href": "302-edav-analysis.html#what-is-not-eda",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.6 What is not EDA?",
    "text": "8.6 What is not EDA?\nIt’s important to understand that an exploratory data analysis is not the same thing as modelling. In particular is not the construction of your baseline model, which is sometimes called initial data analysis.\nThough it might inform the choice of baseline model, EDA is usually not model based. Simple plots and summaries are used to identify patterns in the data that inform how you approach the rest of the project.\nSome degree of statistical rigour can be added through the use of non-parametric techniques; methods like rolling averages, smoothing or partitioning can to help identify trends or patterns while making minimal assumptions about the data generating process.\n\n\n\n\nDaily change in Dow Jones Index with smoothed estimate of mean and 95% confidence interval.\n\n\n\n\n\n\n\nMean and standard deviation of daily change in Dow Jones Index, before and after 1st of June 1998.\n\nafter_june_98\nmean\nsd\n\n\n\nFALSE\n5.916798\n65.19093\n\n\nTRUE\n3.972929\n119.56067\n\n\n\n\n\nThough the assumptions in an EDA are often minimal it can help to make them explicit. For example, in this plot a moving averages is shown with a confidence band, but the construction of this band makes the implicit assumption that, at least locally, our observations have the same distribution and so are exchangeable.\nFinally, EDA is not a prescriptive process. While I have given a lot of suggestions on what you might usually want to consider, there is no correct way to go about an EDA because it is so heavily dependent on the particular dataset, its interpretation and the task you want to achieve with it. This is one of the parts of data science that make it a craft that you hone with experience, rather than an algorithmic process. When you work in a particular area for a long time you develop a knowledge for common data quirks in that area, which may or may not translate to other applications.\nNow that we have a better idea of what is and what is not EDA, let’s talk about the issue that an EDA tries to resolve and the other issues that it generates."
  },
  {
    "objectID": "302-edav-analysis.html#issue-forking-paths",
    "href": "302-edav-analysis.html#issue-forking-paths",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.7 Issue: Forking Paths",
    "text": "8.7 Issue: Forking Paths\nIn any data science project you have a sequence of very many decisions that you must make, each with many potential options and is difficult to decide upon a priori.\n\n\n\nFocusing in on only one small part of the process, we might consider picking a null or baseline model, which we will then try and improve on. Should that null model make constant predictions, incorporate a simple linear trend or is something more flexible obviously needed? Do you have the option to try all of these or are you working under time constraints? Are there contextual clues that rule some of these null models out on contextual grounds?\nAn EDA lets you narrow down your options by looking at your data and helps you to decide what might be reasonable modelling approaches.\n\n\n\nThe problem that sneaks in here is data leakage. Formally this is where training data is included in test set, but this sort of information leak can happen informally too. Usually this is because you’ve seen the data you’re trying to model or predict and then selected your modelling approach based on that information.\nStandard, frequentist statistical methods for estimation and testing assume no “peeking” of this type has occurred. If we use these methods without acknowledging that we have already observed our data then we will artificially inflate the significance of our findings. For example, we might be comparing two models: the first of which makes constant predictions with regard to a predictor, while the second includes a linear trend. We will of course use a statistical test to confirm that what we are seeing is unlikely by chance. However, we must be aware this test was only performed because we had previously examined at the data and noticed what looked to be a trend.\nSimilar issues arise in Bayesian approaches, particularly when constructing or eliciting prior distributions for our model parameters. One nice thing that we can do in the Bayesian setting is to simulating data from the prior predictive distribution and then get an expert to check that these datasets seem seem reasonable. However, it is often the case this expert is also the person who collected the data we will soon be modelling. It’s very difficult for them to ignore what they have seen, which leads to similar, subtle leakage problems."
  },
  {
    "objectID": "302-edav-analysis.html#correction-methods",
    "href": "302-edav-analysis.html#correction-methods",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.8 Correction Methods",
    "text": "8.8 Correction Methods\nThere are various methods or corrections that we can apply during our testing and estimation procedures to ensure that our error rates or confidence intervals account for our previous “peeking” during EDA.\nExamples of these corrections have been developed across many fields of statistics. In medical statistics we have approaches like the Bonferroni correction, to account for carrying out multiple hypothesis tests. In the change-point literature there are techniques for estimating a change location given that a change has been detected somewhere in a time series. While in the extreme value literature there are methods to estimate the required level of protection against rare events, given that the analysis was triggered by the current protections having been compromised.\n\n\n\n\nExample sea-height datasets where an analysis has been triggered by an extreme value (above) or a visually identified change in mean (below).\n\n\n\n\n::: ::::\nAll of these corrections require us to make assumptions about the nature of the peeking. They are either very specific about the process that has occurred, or else are very pessimistic about how much information has been leaked. Developing such corrections to account for EDA isn’t really possible, given its adaptive and non-prescriptive nature.\nIn addition to being either highly specific or pessimistic, these corrections can also be hard to derive and complicated to implement. This is why in settings where the power of tests or level of estimation is critically important, the entire analysis is pre-registered. In clinical trials, for example, every step of the analysis is specified before any data are collected. In data science this rigorous approach is rarely taken.\nAs statistically trained data scientists, it is important for us to remain humble about our potential findings and to suggest follow up studies to confirm the presence of any relationships we do find."
  },
  {
    "objectID": "302-edav-analysis.html#learning-more",
    "href": "302-edav-analysis.html#learning-more",
    "title": "8  Exploratory Data Analysis",
    "section": "\n8.9 Learning More",
    "text": "8.9 Learning More\nIn this chapter we have acknowledged that exploratory analyses are an important part of the data science workflow; this is true not only for us as data scientists, but also for the other people who are involved with our projects.\nWe’ve also seen that an exploratory analysis can help to guide the progression of our projects, but that in doing so we must take care to prevent and acknowledge the risk of data leakage.\nIf you want to explore this topic further, it can be quite challenging: examples of good, exploratory data analyses can be difficult to come by. This is because they are not often made publicly available in the same way that papers and technical reports are. Additionally, they are often kept out of public repositories because they are not as “polished” as the rest of the project. Personally, I think this is a shame and the culture on this is slowly changing.\nFor now, your best approach to learning about what makes a good exploratory analysis is to do lots of your own and to talk to you colleagues about their approaches.\nThere are lots of list-articles out there claiming to give you a comprehensive list of steps for any exploratory analysis. These can be good for inspiration, but I strongly suggest you don’t treat these as gospel.\nDespite the name of the chapter, Roger Peng’s EDA check-list gives an excellent worked example of an exploratory analysis in R. The the discussion article “Exploratory Data Analysis for Complex Models”, Andrew Gelman makes a more abstract discussion of both exploratory analyses (which happen before modelling) and confirmatory analyses (which happen afterwards)."
  },
  {
    "objectID": "303-edav-visualisation.html",
    "href": "303-edav-visualisation.html",
    "title": "9  Data Visualisation",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "303-edav-visualisation.html#introduction",
    "href": "303-edav-visualisation.html#introduction",
    "title": "9  Data Visualisation",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\n\n9.1.1 More than a pretty picture\nData visualisation is an integral part of your work as a data scientist.\n\n\n\nWarming stripes graphic on the cover of “The Climate Book”\n\n\n\nYou’ll use visualisations to rapidly explore new data sets, to understand their structure and to establish which types of model might be suitable for the task at hand. Visualisation is also vital during model evaluation and when check the validity of the assumptions on which that model is based. These are relatively technical uses of visualisation, but graphics have a much broader role within your work of an effective data scientist.\nWhen well designed, plots, tables and animations can tell compelling stories that were once trapped within your data. They can also intuitively communicate the strength of evidence for your findings and draw attention to the most salient parts your argument.\nData visualisation is an amalgamation of science, statistics, graphic design and storytelling. It’s multi-disciplinary nature means that we have to draw on all of our skills to ensure success. While there are certainly many ways to go wrong when visualising data, there are many more ways to get it right.\nThis chapter won’t be a a step-by-step tutorial of how to visualise any type of data. Nor will it be a line-up of visualisations gone wrong. Instead, I hope to pose some questions that’ll get you thinking critically about exactly what you want from each graphic that you produce.\nThere are five things that you should think about when producing any sort of data visualisation. We will consider each of these in turn."
  },
  {
    "objectID": "303-edav-visualisation.html#your-tools",
    "href": "303-edav-visualisation.html#your-tools",
    "title": "9  Data Visualisation",
    "section": "\n9.2 Your Tools 🔨",
    "text": "9.2 Your Tools 🔨\n\n9.2.1 Picking the right tool for the job\nWhen you think of data visualisation, you might immediately think of impressive animations or complex, interactive dashboards that allow users to explore relationships within the data for themselves.\nSuch tools are no doubt impressive but they are by no means necessary for an effective data visualisation. In many cases there is no technology is needed at all. The history of data visualisation vastly pre-dates that of computers and some of the most effective visualisations remain analogue creations.\n\n\n\n\nCoffee consumption, visualised. Jaime Serra Palou.\n\n\n\n\nThis visualisation of a year’s coffee consumption is an ideal example. Displaying the number of cups of coffee in a bar chart or line graph would have been a more accurate way to collect and display this data, but that wouldn’t have the same resonance or impact and it certainly wouldn’t have been as memorable.\n\n9.2.2 Analogue or Digital\n\n9.2.2.1 Analogue Data Viz\nHere we have another example of an analogue data visualisation that is created as part of data collection. Each member of the department is invited to place a Lego brick on a grid to indicate how much caffeine they have consumed and how much sleep they have had. The beauty of using Lego bricks here is that they are stackable and so create a bar plot over two dimensions.\n\n\n\n\nCaffeination vs sleep, shown in lego. Elsie Lee-Robbins\n\n\n\n\nA third example can be found next to the tills in many supermarkets. Each customer is given a token as they pay for their goods. They can then drop this token into one of three large perspex containers as they leave the shop, each representing a different charity. At the end of the month £10,000 is split between the charities in proportion to the number of tokens. Because the containers are made from a transparent material you can see how the tokens are distributed, giving a visualisation of the level of support for each of the charities.\nThere are many other way of constructing a physical, analogue visualisation of your data and this doesn’t need to be done as part of the data collection process. The simplest and perhaps most obvious most obvious is to create a plot of tabular data using a pen and paper.\n\n9.2.2.2 Digital Data Viz\nWhen it comes to digital tools for data visualisation you have a plethora of options. The most similar to pen-and-paper plotting is to draw your visualisations using a Photoshop, or an open source equivalent like Inkscape. The benefit here is that if you misplace a line or dot you can correct this small error without having to start all over again.\nThere are then more data-focused tools that have point-and-click interfaces. These are things like Excel’s chart tools, or specialist visualisation software like Tableau. These are great because they scale with the quantity of data, so that you can plot larger amounts of raw data values that you wouldn’t have the time or patience to do by hand - whether that’s in an analogue or digital format.\nAnalogue and point-and-click approaches to visualisation have the shared limitation of not being reproducible, at least not without extensive documentation explaining how the graphic was created.\nUsing code to create your visualisations can resolve this reproducibility issue, and includes visualisation as a part of your larger, reproducible workflow for data science. Scripted visualisations also scale easily to large data sets and are easy to alter if any changes are required. The downside here is that there is a relatively steep learning curve to creating such visualisations, which is exactly what point-and-click methods are trying to avoid.\nNo matter how you produce your visualisations, the time cost of developing your skills in that medium is what buys you the ability to control and customise what you create. This upfront time investment will also often make you faster at producing future graphics in that medium.\nWhenever you approach a new visualisation problem, you should pick your tools and medium judiciously. You have to balance your immediate needs for speed, accuracy and reproducibility against your current skill level and improving those skills in the medium to long term. Unfortunately, the only way to make good visualisations is to make lots of bad ones and even more mediocre ones first.\n\n9.2.3 ggplot2\nIf your aim is to produce a wide range of high quality data visualisation using R, then the ggplot2 package is one of the most versatile and well documented tools available to you.\nThe g’s at the start of the package name stand for grammar of graphics. This is an opinionated, abstract approach to constructing data visualisations programmatically, by building them up slowly and adding additional plot elements one layer at a time.\n\n\n\nThis idea of a “grammar of Graphics” was originally introduced by Leland Wilkinson. The paper shown by Hadley Wickham, and the associated ggplot2 package popularised this approach within the R community. Like many of the tidyverse collection of packages, ggplot2 provides simple but specialised modular functions that can be composed to create complex visualisations.\nIf you’d like to learn how to use ggplot2, I wouldn’t recommend starting with the paper nor would I recommend trying to get started with the docs alone. Instead, I would suggest you work through an introductory tutorial, or one of the resources linked within the package documentation. Once you have a grasp of the basic principles the best way to improve is to try making your own plots, using reference texts and other people’s work as a guide. A great source of inspiration here is the Tidy Tuesday data visualisation challenge. You can search for the challenge on Github to inspect both the plots made by other people and the code that was used to make them.\nLearning ggplot2:\n\nResources\nTutorial\nTidy Tuesday Github"
  },
  {
    "objectID": "303-edav-visualisation.html#your-medium",
    "href": "303-edav-visualisation.html#your-medium",
    "title": "9  Data Visualisation",
    "section": "\n9.3 Your Medium 📽",
    "text": "9.3 Your Medium 📽\n\n9.3.1 Where is your visualisation going?\nThe second aspect that I recommend you think about before starting a data visualisation is where that graphic is going to be used. The intended location for your visualisation will influence both the composition of your graphic and also the amount of effort that you dedicate to it.\n\n\n\nFor example, consider making an exploratory plot at the start of a project to improve your own understanding of the structure within your data. In this case you do not need to spend much time worrying about refining axis labels, colour schemes or which in file format to save your work.\nWhen working on figure that will be included in a daily stand-up meeting with your team, then you should take a little more care to ensure that your work can be clearly understood by others, for example that the legend and axis labels are both large and sufficiently informative.\nFurther refinement again will be required of this is for an external presentation. Is the message of the visualisation immediately clear? Will the graphic it still be clear when displayed in a boardroom or conference hall, or will it pixellate? Finally, how long will the audience have to interpret the visualisation while you are speaking? Even if slide decks are made available, very few audience members will actually refer to them before or after the presentation.\nThe opposing consideration has to be made when preparing a visualisation for a report or scientific paper. In this case plots and tables can be very small, particularly in two-column or grid layouts. You have to be wary about the legibility of your smallest text (think values on axes) and your visualisation can be clearly understood, whether the document is being read zoomed-in on a computer screen or printed out in black and white.\n\n9.3.2 File Types\n\n\n\nA low resoloution bitmap image.\n\n\n\nTo ensure that your graphics are suitable for the intended medium it is helpful to know a little bit about image file types.\nThere are two dominant types of image file: vector graphics and bitmap graphics.\nBitmap graphics store images as a grid of little squares and each of these pixels takes a single, solid colour. If you make a bitmap image large enough, either by zooming in or by using a really big screen, then these individual pixels become visible. Usually this isn’t going to be your intention, so you need to ensure that the resolution of your graphic (its dimensions counted in pixels) is sufficiently large.\nVector graphics create images using continuous paths and then filling the areas that they enclose with flat colour. These vector images can be enlarged as much as you like without image quality becoming compromised. This is great for simple simple designs like logos, which have to be clear when used on both letterhead and billboards.\nHowever, these vector graphics are more memory intensive than bitmap images, particularly when there are many distinct colours or objects within the image. This can be a particular problem in data science, for example when creating a scatter plot with many thousands of data points.\nIt can often be useful to save both a bitmap and vector version of your graphics. This way you can use bitmap when you need small files that load quickly (like when loading a webpage) and vector graphics when you need your visualisation to stay sharp when enlarged (like when creating a poster or giving a presentation in an auditorium)."
  },
  {
    "objectID": "303-edav-visualisation.html#your-audience",
    "href": "303-edav-visualisation.html#your-audience",
    "title": "9  Data Visualisation",
    "section": "\n9.4 Your Audience 👥",
    "text": "9.4 Your Audience 👥\n\n9.4.1 Know Your Audience\nData visualisations are a tool for communicating information. To make this communication as effective as possible, you have to target your delivery to the intended audience.\n\nWho is the intended audience for your visualisation?\nWhat knowledge do they bring with them?\nWhat assumptions and biases do they hold?\n\nCreating personas for distinct user groups can be a helpful way to answer these questions, particularly when the user population are heterogeneous.\n\n\n\nTo know how to target your delivery to a particular audience, you fist have to identify exactly who that is.\nTo make a compelling data visualisation you have to have some idea of the background knowledge that the viewer brings. Are they a specialist in statistics or data science, or does their expertise lie in area application? Are the findings that you’re presenting going to come as a surprise to them, or act confirmation of their pre-existing body of knowledge.\nIt’s worth considering these prior beliefs and how strongly they are held when constructing your visualisation. Take the time to consider how this existing knowledge could alter or influence their interpretation of what you’re showing to them. On the flip-side, you might be presenting information on a topic that the viewer is the best case ambivalent about or in the worst case is actively bored by. In that case, you can take special care to compose engaging visualisations to capture and hold the attention of the audience.\n\n9.4.2 Preattentive Attributes\nWhen crafting a visualisation we want to require as little work as possible from the viewer. To do this, we can use pre-attentive attributes, such as colour, shape size and position to encode our data values.\n\n\n\n\nExamples of preattentive attributes\n\n\n\n\nThese preattentive attributes are properties of lines and shapes that provide immediate visual contrast without requiring active thought from the viewer. As we will see, care needs to be taken here to ensure that we are don’t mislead the viewer with how we use these attributes.\n\n9.4.3 Example: First Impressions Count\n\n\n\nIssues with scales, area and perspective\n\n\n\nThis figure presents a bar chart of the mean height of males in several countries, but has swapped out the bars for human outlines. While the visualisation has an attractive, minimal design and a pleasant colour scheme, it doesn’t do a good job of immediately conveying the relevant information to the viewer.\nThe three main issues with this plot are all caused by swapping the bars of this plot for male silhouettes, and are linked to the difference in how lengths and areas are perceived by humans. Typically, we make immediate pre-attentive comparisons based on area but draw more accurate, considered comparisons when comparing lengths.\nBy replacing bars with human outlines and not starting the height scale at zero, this plot breaks the proportionality of length and area that is inherent in a bar plot. This causes dissonance between immediate and considered interpretation of this plot. An additional issue is that the silhouettes overlap, creating a forced perspective that makes it seem like the outlines are also further back and therefore even larger if this perspective is taken into account.\nThese three issues are important to consider when constructing your own visualisations. Are you showing all the values that the data could take, or focusing on a smaller interval to provide better contrast? If you are using the size of a circle to represent a value, are you changing the diameter or area in proportion to the data value? And finally, if you are making a plot that appears three-dimensional, have you done so on purpose and, if so, can one of those dimensions be better represented by an attribute that isn’t position?\n\n9.4.4 Visual Perception\nWhen reducing the dimensionality of your plot you may wish to represent a data value using colour rather than position. When deciding on how to use colour, you should keep your audience in mind.\nIs your aim to two or more categories? In that case, you’ll need to select your finite set of colours and ensure that these can be distinguished.\nAre you are representing a data value that is continuous or has an associated ordering? Then you will again have to select your palette to provide sufficient contrast to all viewers of your work.\nIf you are representing a measurement that has a reference value (for example 0 for temperature in centigrade) then a diverging colour palette can be used to represent data that are above or below this reference point. This requires some cultural understanding of how the colours will be interpreted, for example you are likely to cause confusion if you an encoding of red for cold and blue for hot.\nFor colour scales without such a reference point then a gradient in a single colour is likely the best option. In either case, it is important to check that a unit change in data value represents a consistent change in colour across all values. This is not the case for the rainbow palette here (which is neither a single gradient or diverging).\n\n\n\n\nSome default colour scales in R\n\n\n\n\nTo ensure accessibility of your designs, I would recommend one of the many on-line tools to simulate colour vision deficiency or using a pre-made palette where this has been considered for you. A good, low-tech rule of thumb is to design your visualisations so that they’re still easily understood when printed in grey-scale. This can mean picking appropriate colours or additionally varying the point shape, line width or line types used.\n\n\n\n\nDesatureated colour scales in R\n\n\n\n\nFor a practical guide on setting colours see this chapter of exploratory data analysis by Roger Peng.\n\n9.4.5 Alt-text, Titles and Captions\n\nCaptions describe a figure or table so that it may be identified in a list of figures and (where appropriate).\nAlternative text describes the content of an image for a person who cannot view it. (Guide to writing alt-text)\nTitles give additional context or identify key findings. Active titles are preferable.\n\nWhen visualisations are included in a report, article or website, they are often accompanied by three pieces of text. The title, the caption and the alt-text all help the audience to understand a visualisation but each serves a distinct purpose.\n\n9.4.5.1 Captions\nA caption is short description of a visualisation. Captions usually displayed directly above or below the figure or table that they describe. These captions serve two purposes: in a report, the caption can be used to look up the visualisation from a list of figures or tables. The second purpose of a caption is to add additional detail that you don’t want to add to the plot directly. For example caption might be “Time series of GDP in the United States of America, 2017-2022. Lines show daily (solid), monthly (dashed) and five-year (dotted) mean values.”\n\n9.4.5.2 Alt-text\nAlt text or alternative text is used to describe the content of an image to a person who can’t view it. This text is helpful for people with a visual impairment, particularly those who uses a screen reader. Screen reading software reads digital text out loud but can’t interpret images. Such software replaces the image with the provided alternative text. Alt text is also valuable in cases when the image can’t be found or loaded, for example because of an incorrect file path or a slow internet connection, because it’ll be displayed in place of the image.\nThe purpose of alt-text is different from a caption. It’s designed as a replacement for the image, not just a shorthand or to provide additional information. If there is an important trend or conclusion to be drawn from the visualisation (that is not already mentioned in the main text) this should be identified in the alt-text. This sort of interpretation is a key aspect of alt-text that shouldn’t be included in a caption.\n\n9.4.5.3 Titles\nTitles give additional context that is not conveyed by the axis labels or chart annotations. Alternatively the title can be used like a newspaper headline to deliver the key findings of the visualisation. One example of this might be when looking at a visualisation that is composed of many smaller plots, each showing the GDP for a US state over the last five years. Title for each smaller plot would identify the state it is describing, while the overall title might be something like “All US states have increased GDP in the period 2017-2022”.\nIf you are including this type of interpretive title, make sure that the same interpretation is clear in the alt-text."
  },
  {
    "objectID": "303-edav-visualisation.html#your-story",
    "href": "303-edav-visualisation.html#your-story",
    "title": "9  Data Visualisation",
    "section": "\n9.5 Your Story 📖",
    "text": "9.5 Your Story 📖\nThe fourth aspect of a successful data visualisation is that it must tell a story. This story doesn’t need to be a multi-generational novel or even a captivating novella. If a picture speaks a thousand words, really what your aiming for is an engaging anecdote.\nYour visualisation should be something the grabs viewers attention and through its through its composition or content alters their knowledge or view of world in some way.\nTelling effective stories requires planning. How you construct your narrative depends on what effect you want to have on your audience. I’d encourage you to think like a data journalist and go into your work with the intended effect clear in your mind. Is your purpose to inform them of a fact, to persuade them to use a different methodology or entertain them by presenting dull or commonplace data in a fresh and engaging way?\n\n\n\nThree goals of data visualisation: to entertain, persuade and perform\n\n\n\nIn reality your goal will be some mixture of these, at an interior point of this triangle. Clearly identifying this point will help you to present your visual story in a way that works towards your aims, rather than against them.\nOn the point of presentation, it is important to realise that there is no neutral way to present information. In creating a visualisation you’re choosing which aspects of the data to emphasise, what gets summarised and what is not presented at all. This is how you construct a plot that tells a clear and coherent story. However, there is more than one story that you could tell from a single dataset.\nAs an example of this, let’s consider a time-series showing the price of two stocks and in particular the choice of scale on the y-axis. Suppose the two stock have values fluctuating around $100 per share. Choosing a scale that goes from $90 to $110 would emphasise the differences between the two stocks. Setting the lower limit to $0 would instead emphasise that these variations are small relative to the overall value of the stocks. Both are valid approaches but tell different stories. Be clear and be open about which of these you are telling and why you have chosen that over the alternative.\nA final cross-over from data journalism is that your visualisations will be competing for your viewers attention. You have to compete against everything else that is going on in their lives. Establish a clear “hook” within your visualisation to attract your viewer’s attention and immediately deliver the core message. This might be done with a contrasting trend-line or an intriguing title. Lead their attention first to the key message and then the supporting evidence."
  },
  {
    "objectID": "303-edav-visualisation.html#your-guidelines",
    "href": "303-edav-visualisation.html#your-guidelines",
    "title": "9  Data Visualisation",
    "section": "\n9.6 Your Guidelines 📝",
    "text": "9.6 Your Guidelines 📝\n\n9.6.1 Standardise and Document\nThe final consideration when creating visualisations is to reduce the number of considerations that you have to make in the future. This is done by thinking carefully about each of the decisions that you make and writing guidelines so that you make these choices consistently.\nThe choices that go into making an effective data visualisation are important and deserve careful consideration. However, this consideration comes at a cost. To the employer this is the literal, financial cost of paying for your time. More broadly this is the opportunity cost of all the other things that you could have been doing instead.\nTo be efficient in our visualisation design, we should extend our DRY coding principles the design processes. Make choices carefully and document your decisions to externalise the cognitive work required of you in the future.\nMany companies aware of these financial and opportunity costs and provide style guides for visualisations in a similar manner to a coding or writing style guide. This not only externalises and formalises many decisions, but it also leads to a more uniform style across visualisations and the data scientists producing them. This leads to a unified, house-style for graphic design and a visual brand that is easily identifiable. This is beneficial for large companies or personal projects alike.\n\n9.6.2 Example Style Guides\nI’d highly recommend exploring some visualisation guides to get an idea of how these are constructed and how you might develop your own.\nUnsurprisingly some of the best guides come from media outlets and government agencies. These businesses are used to writing style guides for text to create and maintain a distinctive style across all of their writers.\n\n\nBBC\n\nInfographics Guidelines\nR Cookbook\n{bbplot}\n\n\nThe Economist\nThe Office for National Statistics\nEurostat\nUrban Institute\nThe Pudding (learning resources)\n\nThe level of detail and technicality varies wildly between these examples. For instance, the BBC do not provide strong guidelines on the details of the final visualisation but do provide a lot of technical tools and advice on how to construct those in a consistent way across the corporation. They’ve even gone so far as to write their own theme for ggplot and to publish this as an R package!"
  },
  {
    "objectID": "303-edav-visualisation.html#wrapping-up",
    "href": "303-edav-visualisation.html#wrapping-up",
    "title": "9  Data Visualisation",
    "section": "\n9.7 Wrapping Up",
    "text": "9.7 Wrapping Up\n🔨 Think about your tools.\n📽 Think about your medium.\n👥 Think about your audience.\n📖 Think about your story.\n📝 Think about your guidelines.\nData visualisation might seem like a soft skill in comparison to data acquisition, wrangling or modelling. However, it is often effective visualisations that have the greatest real world impact.\nIt is regularly the highly effective figures within reports and presentations that determine which projects are funded or renewed. Similarly, visualisations in press releases can determine whether the result of your study are trusted, correctly interpreted, and remembered by the wider public.\nWhen constructing visualisations it is important to consider whether there are existing guidelines that provide helpful constraints to your work. From there, determine the story that you wish to tell and exactly who it is that your are telling that story to. Once this is decided you can select the medium and the tools that you use to craft your visualisation so that you have the greatest chance of achieving your intended effect."
  },
  {
    "objectID": "310-edav-checklist.html",
    "href": "310-edav-checklist.html",
    "title": "Checklist",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "310-edav-checklist.html#videos-chapters",
    "href": "310-edav-checklist.html#videos-chapters",
    "title": "Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nData Wrangling (20 min) [slides]\nData Exploration (25 min) [slides]\nData Visualisation (27 min) [slides]"
  },
  {
    "objectID": "310-edav-checklist.html#reading",
    "href": "310-edav-checklist.html#reading",
    "title": "Checklist",
    "section": "Reading",
    "text": "Reading\nUse the Data Exploration and Visualisation section of the reading list to support and guide your exploration of this week’s topics. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "310-edav-checklist.html#activities",
    "href": "310-edav-checklist.html#activities",
    "title": "Checklist",
    "section": "Activities",
    "text": "Activities\nCore:\n\nNormConf is a conference dedicated to the unglamorous but essential aspects of working in the data sciences. The in December 2022 conference talks are available as a Youtube Playlist. Find a talk that interests you and watch it, then post a short summary to EdStem, which includes what you learned from the talk and one thing that you still do not understand.\n\nWork through this ggplot2 tutorial for beautiful plotting in R by Cédric Scherer, recreating the examples for yourself.\n\n\nUsing your rolling_mean() function as inspiration, write a rolling_sd() function that calculates the rolling standard deviation of a numeric vector.\n\n\nExtend your rolling_sd() function to optionally return approximate point-wise confidence bands for your rolling standard deviations. These should be \\(\\pm2\\) standard errors by default and may be computed using analytical or re-sampling methods.\n\nCreate a visualisation using your extended rolling_sd() function to assess whether the variability in the daily change in Dow Jones Index is changing over time. [data]\n\n\n\nBonus:\n\n\nAdd your rolling_sd() function to your R package, adding documentation and tests.\n\nDuring an exploratory analysis, we often need to assess the validity of an assumed distribution based on a sample of data. Write your own versions of qqnorm() and qqplot(), which add point-wise tolerance intervals to assess whether deviation from the line \\(y=x\\) are larger than expected.\n\nAdd your own versions of qqnorm() and qqplot() to your R package, along with documentation and tests."
  },
  {
    "objectID": "310-edav-checklist.html#live-session",
    "href": "310-edav-checklist.html#live-session",
    "title": "Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then break into small groups for two data visualisation exercises.\n(Note: For one of these exercises, it would be helpful to bring a small selection of coloured pens or pencils, of you have access to some. If not, please don’t worry - inventive use of black, blue and shading are perfectly acceptable alternatives!)\nPlease come to the live session prepared to discuss the following points:\n\nWhich NormConf video did you watch and what did you learn from it?\nOther than ggplot2, what else have you used to create data visualisations? What are their relative strengths and weaknesses?\nHow did you implement your rolling_sd() function and what conclusions did you draw when applying it to the Dow Jones data?"
  },
  {
    "objectID": "400-production-introduction.html",
    "href": "400-production-introduction.html",
    "title": "Preparing for Production",
    "section": "",
    "text": "Important\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "401-production-reproducibility.html",
    "href": "401-production-reproducibility.html",
    "title": "10  Reproducibility",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "401-production-reproducibility.html#the-data-scientific-method",
    "href": "401-production-reproducibility.html#the-data-scientific-method",
    "title": "10  Reproducibility",
    "section": "\n10.1 The Data Scientific Method",
    "text": "10.1 The Data Scientific Method\nIn what we have covered so far we have been very much focused on the first aspect of data science: the data. When we come to consider about whether our work can be reproduced or our results can be replicated, this shifts our focus to the second other, the science.\n\n\n\n\nCycle of scientific enquiry.\n\n\n\n\nAs data scientists, we like to think that we are applying the scientific method in our work.\nWe start with a question we want to answer or a problem we want to solve. This is followed by a search of the existing literature: is this is a well-known problem that lots of people have solved before? If it is, fantastic, we can learn from their efforts. If not, then we proceed to gather our own evidence and combine this with whatever existing knowledge we could scrape together. Finally, we draw conclusions from this synthesised information.\nWhen doing so we acknowledge that the conclusions we reach are not the truth, just our current best approximation of it. We have a usefully simplified model of the messy reality that we can share with the world. We will happily update our model as we become aware of new evidence, whether that new information supports or contradicts our current way of thinking.\nThat sounds excellent and is, in an ideal world, how both science and data science would progress. However, just like our models this is a simplified (and in this case idealised) description of what really happens."
  },
  {
    "objectID": "401-production-reproducibility.html#issue-multiple-dependent-tests",
    "href": "401-production-reproducibility.html#issue-multiple-dependent-tests",
    "title": "10  Reproducibility",
    "section": "\n10.2 Issue: Multiple, Dependent Tests",
    "text": "10.2 Issue: Multiple, Dependent Tests\n\nProjects are usually not a single hypothesis test\nSequence of dependent decisions\ne.g. Model development\nCan fool ourselves by looking lots of times or ignoring sequential and dependent structure.\n\nThe aims of a data science project are rarely framed as a clear an unambiguous hypothesis, for which we will design a study and perform a single statistical analysis. Apart from in special cases, like A/B testing, we have a much more general aim for our data science projects.\n\n{Abstract text of Gelman and Loken (2013).}\n\nWe might want to construct a model for a given phenomenon and we’ll try many variants of that model along the way. By taking a more relaxed approach to data analysis, data scientists can run the risk of finding spurious relationships within our data that don’t hold more generally. If you look for a relationship between the quantity you are trying to predict and enough samples of random noise will almost surely find a significant relationship, even though there is no true link."
  },
  {
    "objectID": "401-production-reproducibility.html#issues-p-hacking-and-publiction-bias",
    "href": "401-production-reproducibility.html#issues-p-hacking-and-publiction-bias",
    "title": "10  Reproducibility",
    "section": "\n10.3 Issues: \\(p\\)-hacking and Publiction Bias",
    "text": "10.3 Issues: \\(p\\)-hacking and Publiction Bias\n\n{Bar chart of p-values in medical publications, showing a large drop between 4-5 percent and 5-6 percent.}\n\nOkay, so our methods of investigation as data scientists might not be completely sound, but that should be balanced by the results of other studies that exist, right? Well, the second worrisome aspect of this process is that we can’t always trust the published literature to be a fair representation of what people have tried in the past.\nStudies that don’t provide strong evidence against the null hypothesis rarely make it into publications, reports or the news. This is largely because of the way that scientific enquiry is rewarded in the the academy, business and the media. Funding and attention tend to go to studies with new findings, rather than those which aim to confirm or strengthen the findings of existing studies.\nThis systemic problem incentives scientists to ‘massage’ numbers to obtain a \\(p\\)-value less than 0.05 so that a result can be reported as statistically significant. This process is known as \\(p\\)-hacking and can occur through deliberate malpractice but more often it’s a result of scientists generally not receiving adequate training in statistics. As statistically trained data scientists we know that a declaration of significance is no indication of a meaningful effect size and that the conventional significance level of 5% is entirely arbitrary. However, we need to be aware that this is not the case across science and that even we aren’t immune to the societal and systematic influences that favour the publication of novel results over confirmatory ones.\nThese influences also lead to a much more subtle problem than direct \\(p\\)-hacking. Consider a model with an obvious but unnecessary additional property to add: a example here might be adding an unnecessary term to a regression. Because this extension is such low hanging fruit, many scientists independently design experiments to test it out. Most of these experiments provide insufficient evidence against the null hypothesis and don’t get developed into published papers or technical reports, because they just support the status-quo. However, after enough people have attempted this some scientist will get “lucky” and find a significant (and potentially relevant) benefit. Of all the experiments that were done, this is the only one that makes it onto the public record.\nAll of these studies being left in the proverbial desk drawer induces publication bias in the scientific literature. When we come to assess the state of existing knowledge, we are unable to properly assess the importance of findings, because we lack the context of all those null results that went unreported.\nThis same process means that the results of many more scientific studies than we would expect cannot be recreated. This is what is known as the scientific replication crisis."
  },
  {
    "objectID": "401-production-reproducibility.html#reproducibility",
    "href": "401-production-reproducibility.html#reproducibility",
    "title": "10  Reproducibility",
    "section": "\n10.4 Reproducibility",
    "text": "10.4 Reproducibility\n\nReproducibility: given the original raw data and code, can you get all of the results again?\n\n\nReproducible != Correct\n“Code available on request” is the new “Data available on request”\nReproducible data analysis requires effort, time and skill.\n\nThis idea of reproducibility requires us to be able to recover the exact same numerical summaries as the original investigator. In particular this means we should be able to reproduce the exact same point estimates and measures of uncertainty that they did, which ensures we’ll draw the same conclusions as that original investigator.\nWhen putting our work into production there are several reasons why we might require it to be reproducible. The first is logistical: production code needs to be robust and efficient - this often means your code will be re-factored, rewritten or translated into another language. If your results are not reproducible then there is no way to verify that this has been done correctly. Secondly, if a problem is identified in your work (say a customer raises a complaint that their loan application was incorrectly rejected) you need to be able to accurately recreate that instance to diagnose if there is a problem and exactly what caused it.\nNote that just because findings are reproducible, that doesn’t by any means imply that they’re correct. We could have a very well documented but flawed analysis that is entirely reproducible but is also completely unsuitable or just plain wrong.\nIn our data science projects, we have already taken several steps that greatly improve the reproducibility of our work. Although we scripted, tested and documented our work to improve the management of our project, these decisions improve the scientific quality of our work too. This puts us in a strong position relative to the scientific literature as a whole.\nAt a point now where it is almost standard to publish data along with papers, but for a long time this was not the case and data if data were available at all, this was only by request. We are now in a similar situation when it comes to code. It’s still far from standard for the analysis code to be required and put up to detailed scrutiny as part of the peer-review process.\nWith a little more context this isn’t so unreasonable. Across many scientific disciplines, code-based approaches to analysis is not standard; statistical software with a graphical user interface is used instead. The idea here is to allow scientists to analyse their own data by providing tools trough a combination of menus and buttons. However, these interfaces often leave no record of how the data were manipulated and the software itself can be highly specialised or proprietary. This combination means that even when full datasets are provided, it is often impossible for others to reproduce the original analysis.\nNone of this is meant to scold or disparage scientists who use this type of software to allow them to perform statistical analyses. You’re well aware of how much time and effort it takes to learn how to use and implement statistical methods correctly. This is time that other scientists invest in learning their subject, so that they can get to the point of doing research in the first place. This is one of the wonders of data science: the ability to work in multi-disciplinary teams where individual members are specialised in different areas.\nThis is where we need to pause and check ourselves, because the same fate can easily befall us as data scientists. Yes, it take time to learn the skills and practices to ensure reproducibility, but it also takes time to implement them and the time of an expert data practitioner doesn’t come cheap. If you wait until the end of a project before you make it reproducible then you’ll usually be too late - time or money will have run out."
  },
  {
    "objectID": "401-production-reproducibility.html#replicability",
    "href": "401-production-reproducibility.html#replicability",
    "title": "10  Reproducibility",
    "section": "\n10.5 Replicability",
    "text": "10.5 Replicability\n\nReplicable: if the experiment were repeated by an independent investigator, you would get slightly different data but would the substative conclusions be the same?\n\n\nIn the specific sense, this is the core worry for a statistician!\nAlso used more generally: are results stable to perturbations in population / study design / modelling / analysis?\nOnly real test is to try it. Control risk with shadow and parallel deployment. Statisticians are well aware that if we were to repeat an experiment we would get slightly different data. This would lead to slightly different estimates and slightly different results.\n\nUltimately, this is the core problem that statisticians get paid to worry about: will those changes be small enough that the substantive conclusions are not impacted? Yes, point estimates will vary slightly but do your conclusions about the existence, direction or magnitude of an effect still hold? Alternatively, if you are estimating a relationship between two variables, is the same functional form chosen as the most suitable?\nIn a general scientific context, replication takes a more broad meaning and asks whether the key properties of your results could be replicated by another person. In the context of getting your work put into production, we acre concerned about whether your the results of your findings will also hold when applied to future instances that might differ from those you have seen already.\nIf you come to data science from a statistical background then you are well accustomed to these sorts of considerations. Whenever you perform a hypothesis test or compare two models, you take steps to make sure the comparison is not only valid for this particular sample, but that is also true out-of-sample. This is the whole reason data scientists make training and test sets in the first place, as an approximate test for this sort of generalisation. We we do any of these things we are asking of ourselves: will this good performance replicate if we had different inputs?\nOf course train-test split, bootstrap resampling or asymptotic arguments can only ever approximate the ways in which our sample differs from the production population, to which our models will be applied. The only way to truly assess the out-of-sample performance of our models or generalisability of our findings is to put that work into production.\nThis opens us up to risk: what if our findings don’t generalise and our new model is actually much worse than the current one? It’s not possible to both deploy our new model and also avoid this risk entirely. However, we can take some steps to mitigate our level of exposure.\n\n10.5.1 Shadow deployment\nIn the most risk-adverse setting we might implement a shadow deployment of our new model. In this case, the current model is still used for all decision making but our candidate model is also run in the background so that we can see how it might behave in the wild. This is good in that we can identify any points of catastrophic failure for the new model, but is also expensive to run and can give us only limited information.\nSuppose, for example, our model is a recommender system on a retail website. A shadow deployment will let us check that the new system functions correctly and we can gather data on what products are recommended to each customer and investigate how these differ from those recommended by the current system. A shadow deployment cannot in this case tell us what the customer would have done had they been shown those products instead. This means that a shadow deployment doesn’t allow is to investigate whether the new system leads to equivalent or greater revenue than the current system.\n\n10.5.2 Parallel deployment\nParallel deployment or A/B tests have both the current and the proposed new models running at the same time. This allows us to truly test whether our findings generalise to the production population while controlling our level of risk exposure by setting the proportion of times each model is used. The more instances we assign to the new model the faster we will learn about its performance but this also increases our risk exposure."
  },
  {
    "objectID": "401-production-reproducibility.html#reproduction-and-replication-in-statistical-data-science",
    "href": "401-production-reproducibility.html#reproduction-and-replication-in-statistical-data-science",
    "title": "10  Reproducibility",
    "section": "\n10.6 Reproduction and Replication in Statistical Data Science",
<<<<<<< HEAD
    "text": "10.6 Reproduction and Replication in Statistical Data Science\n\n10.6.1 Monte Carlo Methods\nIn data science we rely a lot on the use of stochastic methods. These are often used to increase the chance of our findings being replicated by another person or in production. However, they also make it more difficult to ensure that our exact results can be reproduced, whether by another person or our future selves.\n\n\n\n\n\nMonte Carlo approximation of \\(\\pi\\)\n\n\n\n\n\nMonte Carlo methods are any modelling, estimation or approximation technique that leverages randomness in some way.\nWe have seen examples of this to improve the probability of successful replication. The most obvious example of this is the random allocation of data in a Train/Test split for model selection.\nAnother example focused on improving replication is the use of bootstrap resampling to approximate the sampling distribution of a test statistic. This might be a parametric bootstrap, where alternative datasets are generated by sampling values from the fitted model of our observed data. Alternatively, a non-parametric bootstrap would generate these alternative datasets by sampling from the original data with replacement.\nMonte Carlo methods can also be used to express uncertainties more generally, or to approximate difficult integrals. These are both common applications of Monte Carlo methods in Bayesian modelling, where (unless our models are particularly simple) the posterior or posterior-predictive distribution has to be approximated by a collection of values sampled from that distribution.\nEach time we run any of these analyses we’ll get slightly different outcomes. For our work to be replicable we need to quantify this level of variation. For example, if we had a different sample from the posterior distribution, how much would the estimated posterior mean change by? Sometimes, as in this case, we can appeal to the law of large numbers to help us out. If we take more samples, the variation between realisations will shrink. We can then collect enough samples to our estimated mean is stable across realisations, up to our desired number of significant figures.\nTo make our results reproducible we will have to ensure that we can reproduce the remaining, unstable digits for any particular realisation. We can do this by setting the seed of our random number generator, an idea we will return to shortly.\n\n10.6.2 Optimisation\nOptimisation is the second aspect of data science that can be very difficult to ensure is reproducible and replicable.\nIs the optimum you find stable over:\n\nruns of the procedure?\nstarting points?\nstep size / learning rate?\nrealisations of the data?\n\n\n\n\nA poorly drawn contour plot. Local modes make this optimiation unstable to the choice of starting point.\n\n\n\nIf optimisation routines are used in parameter estimation, we have to ensure that the results they find for a particular data set are reproducible for a given configuration. This might be a given data set, initial set of starting parameters and step size, the learning rate (which controls how that step size changes) and the maximum number of iterations to perform.\nWe additionally need to be concerned about replication here. If we had chosen a different starting point would the optimisation still converge, and would it converge to the same mode? Our original method may have found a local optimum but how can we confirm that this is a global optimum?\nIf the optimisation fails to converge, can your reproduce that case to diagnose the problem? This can be particularly tricky when the optimisation routine itself uses Monte Carlo methods, such as stochastic gradient descent or simulated annealing.\n\n10.6.3 (Pseudo-)Random Numbers\nSometimes we have stochastic elements to our work that we can’t use brute force to eliminate. Perhaps this is beyond our computational abilities or else the number of realisations is an important, fixed aspect of our study.\nFortunately, in computing it’s not common to have truly random numbers. Instead, what we usually have is a complex but deterministic function that generates a sequence of numbers that are statistically indistinguishable from a series of independent random variates.\nThe next term in this sequence of pseudo-random numbers is generated generated based on value the current one. This means that, by setting the starting point, we can always get the same sequence of pseudo-random variates each time we run the code. In R we set this starting point using set.seed()\nThis is especially useful for simulations that involve random variables, as it allows us to recreate the same results exactly. This not only makes it possible for others to recreate our results but it can also make it much easier to test and debug our own code.\n\n\n# different values\nrnorm(n = 4)\n#> [1]  0.224524548  0.453273900 -0.422685049  0.006343482\nrnorm(n = 4)\n#> [1]  0.5437293  0.2516571 -0.0326228  1.0069814\n\n\n\n# the same value\nset.seed(1234)\nrnorm(n = 4)\n#> [1] -1.2070657  0.2774292  1.0844412 -2.3456977\n\nset.seed(1234)\nrnorm(n = 4)\n#> [1] -1.2070657  0.2774292  1.0844412 -2.3456977"
=======
    "text": "10.6 Reproduction and Replication in Statistical Data Science\n\n10.6.1 Monte Carlo Methods\nIn data science we rely a lot on the use of stochastic methods. These are often used to increase the chance of our findings being replicated by another person or in production. However, they also make it more difficult to ensure that our exact results can be reproduced, whether by another person or our future selves.\n\n\n\n\n\nMonte Carlo approximation of \\(\\pi\\)\n\n\n\n\nMonte Carlo methods are any modelling, estimation or approximation technique that leverages randomness in some way.\nWe have seen examples of this to improve the probability of successful replication. The most obvious example of this is the random allocation of data in a Train/Test split for model selection.\nAnother example focused on improving replication is the use of bootstrap resampling to approximate the sampling distribution of a test statistic. This might be a parametric bootstrap, where alternative datasets are generated by sampling values from the fitted model of our observed data. Alternatively, a non-parametric bootstrap would generate these alternative datasets by sampling from the original data with replacement.\nMonte Carlo methods can also be used to express uncertainties more generally, or to approximate difficult integrals. These are both common applications of Monte Carlo methods in Bayesian modelling, where (unless our models are particularly simple) the posterior or posterior-predictive distribution has to be approximated by a collection of values sampled from that distribution.\nEach time we run any of these analyses we’ll get slightly different outcomes. For our work to be replicable we need to quantify this level of variation. For example, if we had a different sample from the posterior distribution, how much would the estimated posterior mean change by? Sometimes, as in this case, we can appeal to the law of large numbers to help us out. If we take more samples, the variation between realisations will shrink. We can then collect enough samples to our estimated mean is stable across realisations, up to our desired number of significant figures.\nTo make our results reproducible we will have to ensure that we can reproduce the remaining, unstable digits for any particular realisation. We can do this by setting the seed of our random number generator, an idea we will return to shortly.\n\n10.6.2 Optimisation\nOptimisation is the second aspect of data science that can be very difficult to ensure is reproducible and replicable.\nIs the optimum you find stable over:\n\nruns of the procedure?\nstarting points?\nstep size / learning rate?\nrealisations of the data?\n\n\n\n\nA poorly drawn contour plot. Local modes make this optimiation unstable to the choice of starting point.\n\n\nIf optimisation routines are used in parameter estimation, we have to ensure that the results they find for a particular data set are reproducible for a given configuration. This might be a given data set, initial set of starting parameters and step size, the learning rate (which controls how that step size changes) and the maximum number of iterations to perform.\nWe additionally need to be concerned about replication here. If we had chosen a different starting point would the optimisation still converge, and would it converge to the same mode? Our original method may have found a local optimum but how can we confirm that this is a global optimum?\nIf the optimisation fails to converge, can your reproduce that case to diagnose the problem? This can be particularly tricky when the optimisation routine itself uses Monte Carlo methods, such as stochastic gradient descent or simulated annealing.\n\n10.6.3 (Pseudo-)Random Numbers\nSometimes we have stochastic elements to our work that we can’t use brute force to eliminate. Perhaps this is beyond our computational abilities or else the number of realisations is an important, fixed aspect of our study.\nFortunately, in computing it’s not common to have truly random numbers. Instead, what we usually have is a complex but deterministic function that generates a sequence of numbers that are statistically indistinguishable from a series of independent random variates.\nThe next term in this sequence of pseudo-random numbers is generated generated based on value the current one. This means that, by setting the starting point, we can always get the same sequence of pseudo-random variates each time we run the code. In R we set this starting point using set.seed()\nThis is especially useful for simulations that involve random variables, as it allows us to recreate the same results exactly. This not only makes it possible for others to recreate our results but it can also make it much easier to test and debug our own code.\n\n\n# different values\nrnorm(n = 4)\n#&gt; [1] -0.82709175 -0.09371399 -1.06679465 -0.90155450\nrnorm(n = 4)\n#&gt; [1] -0.45846240  0.07014213 -0.61417537 -0.89035348\n\n\n\n# the same value\nset.seed(1234)\nrnorm(n = 4)\n#&gt; [1] -1.2070657  0.2774292  1.0844412 -2.3456977\n\nset.seed(1234)\nrnorm(n = 4)\n#&gt; [1] -1.2070657  0.2774292  1.0844412 -2.3456977"
>>>>>>> 114321ad5d8b42044c4328125c2e93922034bdf5
  },
  {
    "objectID": "401-production-reproducibility.html#beware",
    "href": "401-production-reproducibility.html#beware",
    "title": "10  Reproducibility",
    "section": "\n10.7 Beware",
    "text": "10.7 Beware\nWhen running code sequentially and interactively, setting the seed is about all you will need to solve reproducibility problems. However, I’d advise you to take great care when combining this with some of the methods we’ll see for speeding up your code. In some cases, the strategies to optimize your code performance can interfere with the generation of random numbers and lead to unintended results.\nWhen writing code that’s executed in parallel across multiple cores or processors, you have to carefully consider is whether or not to give each the same seed value. The correct decision here is context specific and depends on the interpretation of the random variates you will be generating. If you are making a comparison between iterations it might be important the the random aspects are kept as similar as possible, while if you are paralleling only for speed gains this might not be important at all.\nFinally, it’s important to be wary of the quality of pseudo-random number generation and the interfacing R with other programming languages. R was developed as a statistical programming language and most other languages are not as statistically focused. Different languages may use different algorithms for generating pseudo-random numbers, and the quality of the generated numbers can vary. It’s important to make sure that seeds are appropriately passed between languages to ensure that the correct sequence of random numbers is generated."
  },
  {
    "objectID": "401-production-reproducibility.html#wrapping-up",
    "href": "401-production-reproducibility.html#wrapping-up",
    "title": "10  Reproducibility",
    "section": "\n10.8 Wrapping Up",
    "text": "10.8 Wrapping Up\nTo get our work put into production it should be both reproducible and replicable.\n\nReproducible: can recreate the same results from the same code and data\nReplicable: core results remain valid when using different data\n\nWhile randomness is a key part of most data science workflows but can lead to reproducibility nightmares. We can manage these by appealing to the stability of averages in large samples and by explicitly setting the sequence of pseudo-random numbers that we generate using set.seed().\nFinally, we should take special care when we have to combine efficiency with replicable workflows.\nWith these ideas you can now use good data to do good science."
  },
  {
    "objectID": "402-production-explainability.html",
    "href": "402-production-explainability.html",
    "title": "11  Explainability",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "402-production-explainability.html#what-are-we-explaining-and-to-whom",
    "href": "402-production-explainability.html#what-are-we-explaining-and-to-whom",
    "title": "11  Explainability",
    "section": "\n11.1 What are we explaining and to whom?",
    "text": "11.1 What are we explaining and to whom?\nThere are many reasons you might need to explain the behaviour of your model before it can be put into production. As an example, we can consider a credit scoring system that determines whether or not customers should be given a line of credit.\n\nRegulatory or legal requirements to describe how your model works (e.g. ban on “black-box” modelling).\nUnderstanding how your model works to improve it.\nExplaining to individual load decisions to customers.\n\nIn each of these cases, what exactly do we mean by an explanation? It’s likely not the same thing in each example.\n\nData scientists we might be interested to know exactly what types of mapping between covariates and responses can be represented by the neural network architecture underlying the credit scoring system.\nStakeholders within the company or regulators are likely indifferent to this and are more concerned about understanding the general behaviour of the model across large numbers of loan applications.\nFinally, individual customers might have some investment in the overall behaviour of the scoring model but would also like to know what actions they can take to increase their chance of securing a loan.\n\nBetween each of these examples, the level of technical detail differs but more importantly the fundamental nature of the explanations are different."
  },
  {
    "objectID": "402-production-explainability.html#explaining-a-decision-tree",
    "href": "402-production-explainability.html#explaining-a-decision-tree",
    "title": "11  Explainability",
    "section": "\n11.2 Explaining a Decision Tree",
    "text": "11.2 Explaining a Decision Tree\nWith some models giving an explanation is relatively straightforward. Decision trees are perhaps the easiest model to explain because they mimic human decision making and can be represented like flow-charts that make sequential, linear partitions of the predictor space.\n\n\n\n\nFigure 11.1: An example of a decision tree, optimised to correctly identify category 1 ambulance calls in as few questions as possible.\n\n\n\n\nThese models use the same sort of logic that is used for medical triage when you call an ambulance, to determine the urgency of the call. The binary decisions used in this type of triage are optimised to identify critical calls as soon as possible, but this is just one form of loss function we could use. We might instead pick these partitions to get the most accurate overall classification of calls to urgency categories. This might not be an appropriate loss function for ambulance calls but might be when deciding which loan applicants to grant credit to.\nThe issue is that these decision trees are limited in the relationships they can represent (linear relationships approximated by step function) and are sensitive to small changes in the training data. To overcome these deficiencies we can use a bootstrap aggregation or a random forest model to make predictions based on a collection of these trees. This leads to models that are more stable and flexible but also removes any chance of a simple and human-friendly explanation."
  },
  {
    "objectID": "402-production-explainability.html#explaining-regression-models",
    "href": "402-production-explainability.html#explaining-regression-models",
    "title": "11  Explainability",
    "section": "\n11.3 Explaining Regression Models",
    "text": "11.3 Explaining Regression Models\nAnother model that is relatively straightforward to interpret is a linear regression. We can interpret this model using the estimated regression coefficients, which describe how the predicted outcome changes with a unit change in each covariate while the values of all other covariates are held constant.\n\n\n\n\nFigure 11.2: Linear models have global, conditional explanations, provided by the estimated regression coefficients.\n\n\n\n\nThis is a global and a conditional explanation.\nIt is global because the effect of increasing a covariate by one unit is the same no matter what the starting value of that covariate. The explanation is the same in all parts of the covariate space.\nThe explanation is conditional because it assumes that all other values are held constant. This can lead to some odd behaviour in our explanations, they are dependent on what other terms are included (or left out of) our model.\nThis can be contrasted against non-linear regression, where covariate effects are still interpreted conditional on the value of other covariates but the size or direction of that effect might vary depending on the value of the covariate.\n\n\n\n\nFigure 11.3: Non-linear models have local, conditional explanations, provided by the estimated regression coefficients.\n\n\n\n\nHere we have an example where a unit increase in the covariate is associated with a large change in the model response at low values of the covariate, but a much smaller change at large values of the covariate."
  },
  {
    "objectID": "402-production-explainability.html#example-cherrywood-regression",
    "href": "402-production-explainability.html#example-cherrywood-regression",
    "title": "11  Explainability",
    "section": "\n11.4 Example: Cherrywood regression",
    "text": "11.4 Example: Cherrywood regression\nAs an example of this we can look at the height, girth and volume of some cherry trees.\nIf we are wanting to use a lathe to produce pretty, cherry wood ornaments we might be interested in understanding how the girth of the trees varies with their height and total volume. Using a linear model, we see that both have a positive linear association with girth.\n\n\n\n\nFigure 11.4: Cherry tree girth can be well modelled as a linear function of either tree height or harvestable volume.\n\n\n\n\n\nlm(Girth ~ 1 + Height, data = trees)\n#> \n#> Call:\n#> lm(formula = Girth ~ 1 + Height, data = trees)\n#> \n#> Coefficients:\n#> (Intercept)       Height  \n#>     -6.1884       0.2557\n\n\nlm(Girth ~ 1 + Volume, data = trees)\n#> \n#> Call:\n#> lm(formula = Girth ~ 1 + Volume, data = trees)\n#> \n#> Coefficients:\n#> (Intercept)       Volume  \n#>      7.6779       0.1846\n\nHowever, when we include both terms in our model, our interpretation changes dramatically.\n\nlm(Girth ~ 1 + Height + Volume, data = trees)\n#> \n#> Call:\n#> lm(formula = Girth ~ 1 + Height + Volume, data = trees)\n#> \n#> Coefficients:\n#> (Intercept)       Height       Volume  \n#>    10.81637     -0.04548      0.19518\n\nHeight is no longer positively associated with girth. This is because the size, direction and significance of our estimated effects is conditional on what other terms are included in the model. For a fixed volume of wood, a taller tree necessarily has to have a smaller girth.\nTechniques such as SHAP try to quantify the importance of a predictor by averaging over all combinations of predictors that might be included within the model. You can read more about such techniques in Interpretable Machine Learning by Christoph Molnar."
  },
  {
    "objectID": "402-production-explainability.html#simpsons-paradox",
    "href": "402-production-explainability.html#simpsons-paradox",
    "title": "11  Explainability",
    "section": "\n11.5 Simpson’s Paradox",
    "text": "11.5 Simpson’s Paradox\nThis effect is related to Simpson’s Paradox, where a trend appears in several groups of data but disappears or reverses when the groups are combined.\nThis regularly arises in fields like epidemiology, where population level trends are assumed to apply at the level of individuals or small groups, where this is known as the ecological fallacy.\nActually, Simpson’s parodox is a terrible name, because it isn’t actually a paradox at all. It’s not surprising that we have two different answers to two different questions, the supposed contradiction only arises when we fail to distinguish between those questions.\n\n\n\n\nFigure 11.5: A simulated example in which a covariate has a negative trend at the population level but a positive trend within each sub-group.\n\n\n\n\nWhat I hope to have highlighted here is that for some of the simplest models we might use as data scientists, explanations are very much possible but must be made with care and attention to detail - correctly interpreting these models in context can be far from straightforward."
  },
  {
    "objectID": "402-production-explainability.html#what-hope-do-we-have",
    "href": "402-production-explainability.html#what-hope-do-we-have",
    "title": "11  Explainability",
    "section": "\n11.6 What hope do we have?",
    "text": "11.6 What hope do we have?\nAt this stage, you might be asking yourself what hope we have of explaining more complex models like random forests or neural networks, given how difficult it is to explain even the simple models we might take as our benchmark. You’d be right to worry about this and it is important to remain humble in what we can and cannot know about these complex systems that we are building.\nAll hope isn’t lost though - we still have a few tricks up our sleeve!\n\n11.6.1 Permutation Testing\nSuppose we’re asked by our product manager to determine which predictors or covariates are usually the most import in our model when determining credit scores and loan outcomes.\nOne way to do this would be to remove each covariate from the model and investigate how that changes the predictions made by our model. However, we’ve seen already that removing a predictor can substantively change some models.\nSo instead, we could answer this question by using permutation methods.\nIf we take the observed values of a covariate, say income, and randomly allocate these among all our training examples then this will destroy any association between income and loan outcome. This allows us to remove the information provided by income but without altering the overall structure of our model. We can then refit our model to this modified data set and investigate how rearranging the covariate alters our predictions. If there is a large performance drop, then the covariate is playing an important role within our model.\nThere are many variations on this sort of permutation test. They can be simple but powerful tools for understanding the behaviour of all sorts of model.\n\n11.6.2 Meta-modelling\nMeta-models are, as the name suggests, models of models and these can be effective methods of providing localised explanations for complex models.\nThe idea here is to look at a small region of the covariate space that is covered by a complex and highly flexible model, such as a neural network. We can’t easily give a global explanation for this complex model’s behaviour - it is just too complicated.\nHowever, we can interrogate the model’s behaviour within a small region and construct a simplified version of the model (a meta-model) that lets us explain the model within that small region. Often this meta-model is chosen as a linear model.\n\n\n\n\nFigure 11.6: A local linear approximation in two dimensions\n\n\n\n\nThis is partly for convenience and familiarity but also has a theoretical backing: if our complex model is sufficiently smooth then we can appeal to Taylor’s Theorem to say that in a small enough region the mapping can be well approximated by a linear function.\nThis sort of local model explanation is particularly useful where we want to explain individual predictions or decisions made by our model: for example why a single loan applicant was not granted a loan. By inspecting the coefficients of this local surrogate model we can identify which covariates were the most influential in the decision and suggest how the applicant could increase their chance of success by summarising those covariates that were both influential and are within the ability of the applicant to change. This is exactly the approach taken by the LIME methodology developed by Ribiero et al. \n\n11.6.3 Aggregating Meta-models\nUsing local or conditional explanations of our model’s behaviour can be useful in some circumstances but they don’t give a broader understanding of what is going on overall. What if we want to know how a covariate influences all outcomes not a particular one? What if we care about the covariate’s expected effect over all loan applicants, or the distribution of effects over all applicants?\nThis is where local and conditional explanations are particularly nice. By making these explanations at many points we can aggregate these explanations to understand the global and marginal behaviour of our models.\nTo aggregate our conditional effects into a marginal effect, or a local effect into a global effect we must integrate these over the joint distribution of all covariates. If this sounds a bit scary to you, then you are right. Integration is hard enough at the best of times without adding in the fact that we don’t know the joint distribution of the covariates we are using as predictors.\nDon’t worry though, we can take the easy way out and do both of these things approximately. We can approximate the joint distribution of the covariates by the empirical distribution that we observe in our sample, and then our nasty integrals simplify to averages over the measurement units in our data (the loan applicants in our case).\nIf we construct a local, conditional model for each loan applicant in our data set, we can approximate the marginal effect of each covariate by averaging the conditional effects we obtain for each loan applicant.\n\n\n\n\nFigure 11.7: Local approximations around each observation can be combined to understand global model behaviour.\n\n\n\n\nThis gives us a global understanding of how each covariate influences the response of our model. It does this over all possible values of the other covariates and appropriately weights these according to their frequency within the sample (and also within the population, if our sample is representative)."
  },
  {
    "objectID": "402-production-explainability.html#wrapping-up",
    "href": "402-production-explainability.html#wrapping-up",
    "title": "11  Explainability",
    "section": "\n11.7 Wrapping Up",
    "text": "11.7 Wrapping Up\nWe’ve seen that as our models become more flexible they also become more difficult to explain, whether that is to ourselves, and subject expert or a user of our model.\nThat’s not to say that simple models are always easy to explain or interpret, simple models can be deceptively tricky to communicate accurately.\nFinally, we looked at a couple of techniques for explaining more complex models.\nWe can use permutation tests measure feature importance in our models: shuffling the predictor values breaks any relationship to our response, and we can measure how much this degrades our model performance. A big dip implies that feature was doing a lot of explaining.\nWe can also look at the local behaviour of models by making surrogate or meta models, that are interpretable, and aggregate these to understand the model globally.\nEffective explanations are essential if you want your model to be used in production and to feed into real decisions decision making. This requires some level of skill with rhetoric to tailor your explanation so that it is clear to the person who requested it. But this isn’t a soft skill, it also requires a surprising amount of computational and mathematical skill to extract such explanations from complex modern models."
  },
  {
    "objectID": "403-production-scalability.html",
    "href": "403-production-scalability.html",
    "title": "12  Scalability",
    "section": "",
    "text": "Important\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "403-production-scalability.html#scalability-and-production",
    "href": "403-production-scalability.html#scalability-and-production",
    "title": "12  Scalability",
    "section": "\n12.1 Scalability and Production",
    "text": "12.1 Scalability and Production\nWhen put into production code gets used more and on more data. We will likely have to consider scalability of our methods in terms of\n\nComputation time\nMemory requirements\n\nWhen doing so we have to balance a trade-off between development costs and usage costs.\n\n12.1.1 Example: Bayesian Inference\n\nMCMC originally takes ~24 hours\nIdentifying and amending bottlenecks in code reduced this to ~24 minutes.\n\nIs this actually better? That will depend on a number of factors, including:\n\nhuman hours invested\nfrequency of use\nsafe / stable / general / readable\ntrade for scalability\n\n12.1.2 Knowing when to worry\nSub-optimal optimisation can be worse than doing nothing\n\n… programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimisation is the root of all evil (or at least most of it) in programming. - Donald Knuth\n\n\n\n\n\n\n\n\n\n\n12.1.3 Our Focus\nWriting code that scales well in terms of computaiton time or memory used is a huge topic. In this section we restrict our aims to:\n\nBasic profiling to find bottlenecks.\nStrategies for writing scalable (R) code.\nSignpost advanced methods & further reading."
  },
  {
    "objectID": "403-production-scalability.html#basics-of-code-profiling",
    "href": "403-production-scalability.html#basics-of-code-profiling",
    "title": "12  Scalability",
    "section": "\n12.2 Basics of Code Profiling",
<<<<<<< HEAD
    "text": "12.2 Basics of Code Profiling\n\n12.2.1 R as a stopwatch\nThe simplest way to profile your code is to time how long it takes to run. There are three common ways to do this.\nFirstly, you could record the time before your code starts executing, the time it completes and look at the difference of those.\n\nt_start <- Sys.time()\nSys.sleep(0.5) # YOUR CODE\nt_end <- Sys.time()\n\nt_end - t_start\n#> Time difference of 0.5083079 secs\n\nThe system.time function provides a shorthand for this if your code runs sequentially and extends the functionality to work for parallel code too.\n\nsystem.time(\n  Sys.sleep(0.5)\n)\n#>    user  system elapsed \n#>   0.000   0.000   0.503\n\nThe tictoc package has similar features, but also allows you to add intermediate timers to more understand which parts of your code are taking the most time to run.\n\nlibrary(tictoc)\n\ntic() \nSys.sleep(0.5) # YOUR CODE \ntoc()\n#> 0.511 sec elapsed\n\nWith tictoc we can get fancy\n\ntic(\"total\")\ntic(\"first, easy part\")\nSys.sleep(0.5)\ntoc(log = TRUE)\n#> first, easy part: 0.508 sec elapsed\ntic(\"second, hard part\")\nSys.sleep(3)\ntoc(log = TRUE)\n#> second, hard part: 3.005 sec elapsed\ntoc()\n#> total: 3.52 sec elapsed\n\nIf your code is already very fast (but will be run very many times, so further efficiency gains are required) then the methods may fail because they do not sample the state of the code at a high enough frequency. In those cases you might want to explore the {mircobenchmark} package."
=======
    "text": "12.2 Basics of Code Profiling\n\n12.2.1 R as a stopwatch\nThe simplest way to profile your code is to time how long it takes to run. There are three common ways to do this.\nFirstly, you could record the time before your code starts executing, the time it completes and look at the difference of those.\n\nt_start &lt;- Sys.time()\nSys.sleep(0.5) # YOUR CODE\nt_end &lt;- Sys.time()\n\nt_end - t_start\n#&gt; Time difference of 0.5031278 secs\n\nThe system.time function provides a shorthand for this if your code runs sequentially and extends the functionality to work for parallel code too.\n\nsystem.time(\n  Sys.sleep(0.5)\n)\n#&gt;    user  system elapsed \n#&gt;   0.000   0.000   0.505\n\nThe tictoc package has similar features, but also allows you to add intermediate timers to more understand which parts of your code are taking the most time to run.\n\nlibrary(tictoc)\n\ntic() \nSys.sleep(0.5) # YOUR CODE \ntoc()\n#&gt; 0.508 sec elapsed\n\nWith tictoc we can get fancy\n\ntic(\"total\")\ntic(\"first, easy part\")\nSys.sleep(0.5)\ntoc(log = TRUE)\n#&gt; first, easy part: 0.506 sec elapsed\ntic(\"second, hard part\")\nSys.sleep(3)\ntoc(log = TRUE)\n#&gt; second, hard part: 3.009 sec elapsed\ntoc()\n#&gt; total: 3.52 sec elapsed\n\nIf your code is already very fast (but will be run very many times, so further efficiency gains are required) then the methods may fail because they do not sample the state of the code at a high enough frequency. In those cases you might want to explore the {mircobenchmark} package."
>>>>>>> 114321ad5d8b42044c4328125c2e93922034bdf5
  },
  {
    "objectID": "403-production-scalability.html#profiling-your-code",
    "href": "403-production-scalability.html#profiling-your-code",
    "title": "12  Scalability",
    "section": "\n12.3 Profiling Your Code",
    "text": "12.3 Profiling Your Code\nTo diagnose scaling issues you have to understand what your code is doing.\n\n\nStop the code at time \\(\\tau\\) and examine the call-stack.\n\nThe current function being evaluated, the function that called that, the function that called that, …, top level function.\n\n\nDo this a lot and you can measure (estimate) the proportion of working memory (RAM) uses over time and the time spent evaluating each function.\n\n\nlibrary(profvis)\nlibrary(bench)\n\n\n12.3.1 Profiling: Toy Example\nSuppose we have the following code in a file called prof-vis-example.R.\n\nh <- function() {\n  profvis::pause(1)\n}\n\ng <- function() {\n  profvis::pause(1)\n  h()\n}\n\nf <- function() {\n  profvis::pause(1)\n  g()\n  profvis::pause(1)\n  h()\n}\n\nThen the call stack for f() would look something like this.\n\n\n\n\nCallstack for f()\n\n\n\n\nWe can examine the true call stack using the profvis() function from the profvis package. By saving the code in a separate file and sourcing it into our session, this function will also give us line-by-line information about the time and memory demands of our code.\n\nsource(\"prof-vis-example.R\")\nprofvis::profvis(f())\n\n\n\n\n\n\n\n\n\nIn both the upper histogram and the lower flame plot we can see that the majority of time is being spent in pause() and h(). What we have to be careful of here is that the upper plot shows the total amount of time in each function call, so h() appears to take longer than g(), but this is because it is called more often in the code snippet we are profiling."
  },
  {
    "objectID": "403-production-scalability.html#notes-on-time-profiling",
    "href": "403-production-scalability.html#notes-on-time-profiling",
    "title": "12  Scalability",
    "section": "\n12.4 Notes on Time Profiling",
<<<<<<< HEAD
    "text": "12.4 Notes on Time Profiling\nWe will get slightly different results each time you run the function\n\nChanges to internal state of computer\nUsually not a big deal, mainly effects fastest parts of code\nBe careful with stochastic simulations\nUse set.seed() to make a fair comparison over many runs.\n\n\n12.4.1 Source code and compiled functions\nIf you write a function you can see the source of that function by calling it’s name\n\npad_with_NAs <- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\n\npad_with_NAs\n#> function(x, n_left, n_right){\n#>   c(rep(NA, n_left), x, rep(NA, n_right))\n#> }\n\nThis is equally true for functions within packages.\n\neds::pad_with_NAs\n#> function (x, n_left, n_right) \n#> {\n#>     stopifnot(n_left >= 0)\n#>     stopifnot(n_right >= 0)\n#>     stopifnot(class(x) %in% c(\"character\", \"complex\", \"integer\", \n#>         \"logical\", \"numeric\", \"factor\"))\n#>     c(rep(NA, n_left), x, rep(NA, n_right))\n#> }\n#> <bytecode: 0x7f819210acc0>\n#> <environment: namespace:eds>\n\nSome functions use compiled code that is written in another language. This is the case for dplyr’s arrange(), which calls some compiled C++ code.\n\ndplyr::arrange\n#> function (.data, ..., .by_group = FALSE) \n#> {\n#>     UseMethod(\"arrange\")\n#> }\n#> <bytecode: 0x7f8192175cf0>\n#> <environment: namespace:dplyr>\n\nIt is also true for many functions from base R, for which there is (for obvious reason) no R source code.\n\nmean\n#> function (x, ...) \n#> UseMethod(\"mean\")\n#> <bytecode: 0x7f818fd37330>\n#> <environment: namespace:base>\n\n\nThese compiled functions have no R source code, and the profiling methods we have used here don’t extend into compiled code. See {jointprof} if you really need this profiling functionality."
=======
    "text": "12.4 Notes on Time Profiling\nWe will get slightly different results each time you run the function\n\nChanges to internal state of computer\nUsually not a big deal, mainly effects fastest parts of code\nBe careful with stochastic simulations\nUse set.seed() to make a fair comparison over many runs.\n\n\n12.4.1 Source code and compiled functions\nIf you write a function you can see the source of that function by calling it’s name\n\npad_with_NAs &lt;- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\n\n\npad_with_NAs\n#&gt; function(x, n_left, n_right){\n#&gt;   c(rep(NA, n_left), x, rep(NA, n_right))\n#&gt; }\n\nThis is equally true for functions within packages.\n\neds::pad_with_NAs\n#&gt; function (x, n_left, n_right) \n#&gt; {\n#&gt;     stopifnot(n_left &gt;= 0)\n#&gt;     stopifnot(n_right &gt;= 0)\n#&gt;     stopifnot(class(x) %in% c(\"character\", \"complex\", \"integer\", \n#&gt;         \"logical\", \"numeric\", \"factor\"))\n#&gt;     c(rep(NA, n_left), x, rep(NA, n_right))\n#&gt; }\n#&gt; &lt;bytecode: 0x7fd07d4b5de8&gt;\n#&gt; &lt;environment: namespace:eds&gt;\n\nSome functions use compiled code that is written in another language. This is the case for dplyr’s arrange(), which calls some compiled C++ code.\n\ndplyr::arrange\n#&gt; function (.data, ..., .by_group = FALSE) \n#&gt; {\n#&gt;     UseMethod(\"arrange\")\n#&gt; }\n#&gt; &lt;bytecode: 0x7fd07d56b820&gt;\n#&gt; &lt;environment: namespace:dplyr&gt;\n\nIt is also true for many functions from base R, for which there is (for obvious reason) no R source code.\n\nmean\n#&gt; function (x, ...) \n#&gt; UseMethod(\"mean\")\n#&gt; &lt;bytecode: 0x7fd075c17470&gt;\n#&gt; &lt;environment: namespace:base&gt;\n\n\nThese compiled functions have no R source code, and the profiling methods we have used here don’t extend into compiled code. See {jointprof} if you really need this profiling functionality."
>>>>>>> 114321ad5d8b42044c4328125c2e93922034bdf5
  },
  {
    "objectID": "403-production-scalability.html#memory-profiling",
    "href": "403-production-scalability.html#memory-profiling",
    "title": "12  Scalability",
    "section": "\n12.5 Memory Profiling",
    "text": "12.5 Memory Profiling\nprofvis() can similarly measure the memory usage of your code.\n\nx <- integer()\nfor (i in 1:1e4) {\n  x <- c(x, i)\n}\n\n\n\n\n\n\n\n\n\n\nCopy-on-modify behaviour makes growing objects slow.\n\nPre-allocate storage where possible.\nStrategies and structures, see R inferno and Effecient R."
  },
  {
    "objectID": "403-production-scalability.html#tips-to-work-at-scale",
    "href": "403-production-scalability.html#tips-to-work-at-scale",
    "title": "12  Scalability",
    "section": "\n12.6 Tips to work at scale",
    "text": "12.6 Tips to work at scale\nTL;DR: pick your object types carefully, vectorise your code and as a last resort implement your code in a faster language.\n\n12.6.1 Vectorise\nTwo bits of code do the same task, but the second is much faster, because it involves fewer function calls.\n\nx <- 1:10\ny <- 11:20 \nz <- rep(NA, length(x))\n\nfor (i in seq_along(x)) {\n  z[i] <- x[i] * y[i]\n}\n\n\nx <- 1:10\ny <- 11:20 \nz <- x * y\n\nWhere possible write and use functions to take advantage of vectorised inputs. E.g.\n\nrnorm(n = 100, mean = 1:10, sd = rep(1, 10))\n\nBe careful of recycling!\n\n12.6.2 Linear Algebra\n\nX <- diag(x = c(2, 0.5))\ny <- matrix(data = c(1, 1), ncol = 1)\n\nX %*% y\n#>      [,1]\n#> [1,]  2.0\n#> [2,]  0.5\n\nMore on vectorising: Noam Ross Blog Post"
  },
  {
    "objectID": "403-production-scalability.html#for-loops-in-disguise",
    "href": "403-production-scalability.html#for-loops-in-disguise",
    "title": "12  Scalability",
    "section": "\n12.7 For loops in disguise",
<<<<<<< HEAD
    "text": "12.7 For loops in disguise\n\n12.7.1 The apply family\nFunctional programming equivalent of a for loop. [apply(), mapply(), lapply(), …]\nApply a function to each element of a list-like object.\n\nA <- matrix(data = 1:12, nrow = 3, ncol = 4)\nA\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    4    7   10\n#> [2,]    2    5    8   11\n#> [3,]    3    6    9   12\n\n\n# MARGIN = 1 => rows,  MARGIN = 2 => columns\napply(X = A, MARGIN = 1, FUN = sum)\n#> [1] 22 26 30\n\nThis generalises functions from matrixStats, where for some special operations we can do all to the necessary calculation in C++.\n\nrowSums(A)\n#> [1] 22 26 30\n\n\n12.7.2 {purrr}\n\nIterate over a single object with map().\n\nmu <- c(-10, 0, 10)\npurrr::map(.x = mu, .f = rnorm, n = 5)\n#> [[1]]\n#> [1]  -8.876572  -9.424607  -8.682438 -11.258476 -10.638463\n#> \n#> [[2]]\n#> [1] -0.61653992  0.03389615 -1.17953166  1.24555576  0.38054170\n#> \n#> [[3]]\n#> [1]  8.956084 10.046883  8.211559  9.355451  8.800315\n\nIterate over multiple objects map2() and pmap().\n\nmu <- c(-10, 0, 10)\nsigma <- c(0, 0.1, 0)\npurrr::map2(.x = mu, .y = sigma, .f = rnorm, n = 5)\n#> [[1]]\n#> [1] -10 -10 -10 -10 -10\n#> \n#> [[2]]\n#> [1] -0.05801018  0.11961581  0.15206135 -0.02410871  0.09876034\n#> \n#> [[3]]\n#> [1] 10 10 10 10 10\n\n\nmu <- c(-10, 0, 10)\nsigma <- c(0, 0.1, 0)\n\npurrr::pmap(\n  .f = rnorm, \n  n = 5,\n  .l = list(\n    mean = mu, \n    sd = sigma))\n#> [[1]]\n#> [1] -10 -10 -10 -10 -10\n#> \n#> [[2]]\n#> [1] -0.04583593  0.14986969 -0.10910054  0.08660517 -0.10153561\n#> \n#> [[3]]\n#> [1] 10 10 10 10 10\n\nFor more details and variants see Advanced R chapters 9-11 on functional programming."
=======
    "text": "12.7 For loops in disguise\n\n12.7.1 The apply family\nFunctional programming equivalent of a for loop. [apply(), mapply(), lapply(), …]\nApply a function to each element of a list-like object.\n\nA &lt;- matrix(data = 1:12, nrow = 3, ncol = 4)\nA\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    4    7   10\n#&gt; [2,]    2    5    8   11\n#&gt; [3,]    3    6    9   12\n\n\n# MARGIN = 1 =&gt; rows,  MARGIN = 2 =&gt; columns\napply(X = A, MARGIN = 1, FUN = sum)\n#&gt; [1] 22 26 30\n\nThis generalises functions from matrixStats, where for some special operations we can do all to the necessary calculation in C++.\n\nrowSums(A)\n#&gt; [1] 22 26 30\n\n\n12.7.2 {purrr}\n\nIterate over a single object with map().\n\nmu &lt;- c(-10, 0, 10)\npurrr::map(.x = mu, .f = rnorm, n = 5)\n#&gt; [[1]]\n#&gt; [1] -10.422868 -11.638088  -9.932571  -9.025392 -10.781733\n#&gt; \n#&gt; [[2]]\n#&gt; [1] -0.2863980  0.3685672  0.3742572  0.1421569 -1.3022845\n#&gt; \n#&gt; [[3]]\n#&gt; [1]  9.843356  7.370247 10.087178 11.008547  9.804995\n\nIterate over multiple objects map2() and pmap().\n\nmu &lt;- c(-10, 0, 10)\nsigma &lt;- c(0, 0.1, 0)\npurrr::map2(.x = mu, .y = sigma, .f = rnorm, n = 5)\n#&gt; [[1]]\n#&gt; [1] -10 -10 -10 -10 -10\n#&gt; \n#&gt; [[2]]\n#&gt; [1] -0.0993421  0.1714979  0.0774610  0.1056376  0.0265688\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 10 10 10 10 10\n\n\nmu &lt;- c(-10, 0, 10)\nsigma &lt;- c(0, 0.1, 0)\n\npurrr::pmap(\n  .f = rnorm, \n  n = 5,\n  .l = list(\n    mean = mu, \n    sd = sigma))\n#&gt; [[1]]\n#&gt; [1] -10 -10 -10 -10 -10\n#&gt; \n#&gt; [[2]]\n#&gt; [1]  0.03410495  0.12024365 -0.01413384  0.28021118 -0.06291448\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 10 10 10 10 10\n\nFor more details and variants see Advanced R chapters 9-11 on functional programming."
>>>>>>> 114321ad5d8b42044c4328125c2e93922034bdf5
  },
  {
    "objectID": "403-production-scalability.html#easy-parallelisation-with-furrr",
    "href": "403-production-scalability.html#easy-parallelisation-with-furrr",
    "title": "12  Scalability",
    "section": "\n12.8 Easy parallelisation with furrr",
<<<<<<< HEAD
    "text": "12.8 Easy parallelisation with furrr\n\n{parallel} and {futures} allow parallel coding over multiple cores.\nPowerful, but steep learning curve.\nfurrr makes this very easy, just add future_ to purrr verbs.\n\n\nmu <- c(-10, 0, 10)\nfurrr::future_map(\n  .x = mu, \n  .f = rnorm,\n  .options = furrr::furrr_options(seed = TRUE),\n  n = 5) \n#> [[1]]\n#> [1] -11.593290  -9.595670 -10.074479 -10.376344  -9.894132\n#> \n#> [[2]]\n#> [1]  0.8514637 -0.3718945  0.2349390  0.9178362  1.1322401\n#> \n#> [[3]]\n#> [1]  8.947493 10.704697  8.935551 11.370871 10.152588\n\nThis is, of course excessive for this small example!\nOne thing to be aware of is that we need to be very careful handling random number generation in relation to parallelisation. There are many options for how you might want to set this up, see R-bloggers for more details."
=======
    "text": "12.8 Easy parallelisation with furrr\n\n{parallel} and {futures} allow parallel coding over multiple cores.\nPowerful, but steep learning curve.\nfurrr makes this very easy, just add future_ to purrr verbs.\n\n\nmu &lt;- c(-10, 0, 10)\nfurrr::future_map(\n  .x = mu, \n  .f = rnorm,\n  .options = furrr::furrr_options(seed = TRUE),\n  n = 5) \n#&gt; [[1]]\n#&gt; [1]  -9.513171  -9.097403 -10.186770  -9.450393 -10.604382\n#&gt; \n#&gt; [[2]]\n#&gt; [1] -1.1308262  0.4800375 -0.2833952  1.3670356 -0.1714894\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 10.082269  8.010454 10.641328 10.446820  7.758835\n\nThis is, of course excessive for this small example!\nOne thing to be aware of is that we need to be very careful handling random number generation in relation to parallelisation. There are many options for how you might want to set this up, see R-bloggers for more details."
>>>>>>> 114321ad5d8b42044c4328125c2e93922034bdf5
  },
  {
    "objectID": "403-production-scalability.html#sometimes-r-doesnt-cut-it",
    "href": "403-production-scalability.html#sometimes-r-doesnt-cut-it",
    "title": "12  Scalability",
    "section": "\n12.9 Sometimes R doesn’t cut it",
    "text": "12.9 Sometimes R doesn’t cut it\n\n\n\nRCPP: An API for running C++ code in R. Useful when you need:\n\nloops to be run in order\nlots of function calls (e.g. deep recursion)\noptimised data structures\n\nRewriting R code in C++ and other low-level programming languages is beyond our scope, but good to know exists. Starting point: Advanced R Chapter 25."
  },
  {
    "objectID": "403-production-scalability.html#wrapping-up",
    "href": "403-production-scalability.html#wrapping-up",
    "title": "12  Scalability",
    "section": "\n12.10 Wrapping up",
    "text": "12.10 Wrapping up\nSummary\n\nPick you battles wisely\nTarget your energy with profiling\nScale loops with vectors\nScale loops in parallel processing\nScale in another language\nHelp!\n\nArticles and blog links\nThe R inferno (Circles 2-4)\n\nAdvanced R (Chapters 23-25),\nEfficient R (Chapter 7)."
  },
  {
    "objectID": "410-production-checklist.html",
    "href": "410-production-checklist.html",
    "title": "Checklist",
    "section": "",
    "text": "Important\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "410-production-checklist.html#videos-chapters",
    "href": "410-production-checklist.html#videos-chapters",
    "title": "Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nReproducibility (26 min) [slides]\nExplainability (16 min) [slides]\nScalability (30 min) [slides]"
  },
  {
    "objectID": "410-production-checklist.html#reading",
    "href": "410-production-checklist.html#reading",
    "title": "Checklist",
    "section": "Reading",
    "text": "Reading\nUse the Preparing for Production section of the reading list to support and guide your exploration of this week’s topics. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "410-production-checklist.html#activities",
    "href": "410-production-checklist.html#activities",
    "title": "Checklist",
    "section": "Activities",
    "text": "Activities\nThis week has fewer activities, since you will be working on the first assessment.\nCore\n\nRead the LIME paper, which we will discuss during the live session.\nWork through the understanding LIME R tutorial\nUse code profiling tools to assess the performance of your rolling_mean() and rolling_sd() functions. Identify any efficiencies that can be made.\n\nBonus:\n\nWrite two functions to simulate a homogeneous Poisson process with intensity \\(\\lambda >0\\) on the interval \\((t_1, t_2) \\subset \\mathbb{R}\\). The first should use the exponential distribution of inter-event times to simulate events in sequence. The second should use the Poisson distribution of the total event count to first simulate the number of events and then randomly allocate locations over the interval. Evaluate and compare the reproducibility and scalability of each implementation."
  },
  {
    "objectID": "410-production-checklist.html#live-session",
    "href": "410-production-checklist.html#live-session",
    "title": "Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then break into small groups for a reading group style discussion of the LIME paper that was set as reading for this week."
  },
  {
    "objectID": "500-ethics-introduction.html",
    "href": "500-ethics-introduction.html",
    "title": "Data Science Ethics",
    "section": "",
    "text": "Important\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "501-ethics-privacy.html",
    "href": "501-ethics-privacy.html",
    "title": "13  Privacy",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "501-ethics-privacy.html#privacy-and-data-science",
    "href": "501-ethics-privacy.html#privacy-and-data-science",
    "title": "13  Privacy",
    "section": "\n13.1 Privacy and Data Science",
    "text": "13.1 Privacy and Data Science\nData privacy is all about keeping people’s personal information confidential and secure. It’s the idea that people have control over their personal data, and that companies and organizations are responsible for protecting it. Personal data can include things like names, addresses, phone numbers, email addresses, medical information, and even online activity.\n\nWhat is personal data?\n\nName, national insurance number, passport number\nContact details: address, phone number, email address\nMedical history\nOnline activity, GPS data, finger print or face-ID,\n\nShould not be collected, analysed or distributed without consent.\n\nUnder the concept of data privacy, individuals have the right to know what information is being collected about them, how it’s being used, who it’s being shared with, and to have the ability to control that information. Companies and organizations, on the other hand, have a responsibility to keep this information secure and to use it only for the purpose it was intended.\nIt’s important to remember that with the growing use of machine learning and data science methods, personal information is being collected, stored, and shared more than ever before. This makes data privacy a critical issue to our work as data scientists. By ensuring that personal information is handled responsibly and with respect for people’s privacy, we can build trust and confidence in the digital world. In this section of the course I hope to introduce you to some key ideas around data privacy and use some case studies to demonstrate that privacy is not an easy thing to ensure."
  },
  {
    "objectID": "501-ethics-privacy.html#privacy-as-a-human-right",
    "href": "501-ethics-privacy.html#privacy-as-a-human-right",
    "title": "13  Privacy",
    "section": "\n13.2 Privacy as a Human Right",
    "text": "13.2 Privacy as a Human Right\n\n13.2.1 Article 12 of the Universal Declaration of Human Rights\n\nNo one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks. - UN General Assembly, 1948.\n\nThe idea of a right to privacy is not a new one. Article 12 of the Universal Declaration of Human Rights, written in 1948, states that everyone has the right to privacy in their personal and family life, home and correspondence (this includes communication via the post, telephone, and email).\nThis means that everyone has the right to keep their personal information, private and protected from being disclosed to others without their consent. This right to privacy is essential for protecting an individual’s autonomy, dignity, and freedom.\nThe Universal Declaration of Human Rights it is often considered as a benchmark for human rights and many countries have incorporated its principles into their own laws and regulations. This means that in many countries the right to privacy is legally protected and people have the right to take action if their privacy is violated.\nThis means we have to take particular care in our work as data scientists when handling any information personal information, whether that is at the individual level or in aggregate."
  },
  {
    "objectID": "501-ethics-privacy.html#data-privacy-and-the-european-union",
    "href": "501-ethics-privacy.html#data-privacy-and-the-european-union",
    "title": "13  Privacy",
    "section": "\n13.3 Data Privacy and the European Union",
    "text": "13.3 Data Privacy and the European Union\n\n13.3.1 General Date Protection Regulation (2018)\n\n‘Consent’ of the data subject means any freely given, specific, informed and unambiguous indication of the data subject’s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her; - GDPR Article 4\n\nA more recent set of regulations relating to the use of personal data is the General Data Protection Regulation (GDPR). This is a comprehensive data privacy regulation that went into effect on May 2018 within the European Union. The purpose of the GDPR is to give individuals more control over their personal information and how it’s used, and to unify data protection laws across the EU.\nThe GDPR is an extensive legal document that lays down strict rules for how companies and organizations must handle the personal data of EU citizens, regardless of where that data is stored or processed.\nSome key provisions of the GDPR include the requirement to obtain explicit, informed and active consent from individuals before the collection and processing of their personal data. This is precisely why banner notifications about cookies on websites became ubiquitous,\nGDPR also establishes the right for individuals to request access to, or deletion of, their personal data. Furthermore, it states that in the event of a data breach (where unauthorised access or use of personal data occurs) the data holder must inform the affected individuals and relevant authorities within 72 hours."
  },
  {
    "objectID": "501-ethics-privacy.html#privacy-key-terms",
    "href": "501-ethics-privacy.html#privacy-key-terms",
    "title": "13  Privacy",
    "section": "\n13.4 Privacy: Key Terms",
    "text": "13.4 Privacy: Key Terms\nMeasuring privacy within a dataset is a complex task that involves assessing the degree to which personal information is protected from unauthorized access, use, or disclosure. There are many ways of measuring and increasing the degree of privacy within a data set. To understand these, and the literature on data privacy, it helps to be familiar with a few key terms.\n\nPseudonmisation: processing data so that it does not relate to an identifiable person.\n\nA data entry is pseudonymised when it has been processed in a way that it does not relate to an identifiable person. The key word here is identifiable. Replacing your name with your CID would be considered a form of pseudonymisation, because based on that information alone you cannot be identified.\n\nRe-identification: relating a pseudonymised data entry to an identifiable person.\n\nRe-identification is the act of relating a pseudonymised data entry to an identifiable person. Re-identification makes something about that person known that wasn’t known beforehand, perhaps by using processing techniques or cross-referencing against external information. In the previous grading scenario, re-identification occurs after marking so that your grades can be returned to you.\n\nAnonymisation: A pseudonmisation method that precludes re-identification.\n\nAnonymisation is a word that in casual usage is often conflated with our previous definition of pseudonymisation. In the technical sense, anonymisation is any form of pseudonymisation that precludes the possibility of re-identification.\nWe don’t want to anonymise test scores, or we would not be able to map them back to individual students. However, if grades were to be published online then we would want to ensure that this is only done in an anonymised format."
  },
  {
    "objectID": "501-ethics-privacy.html#measuring-privacy",
    "href": "501-ethics-privacy.html#measuring-privacy",
    "title": "13  Privacy",
    "section": "\n13.5 Measuring Privacy",
    "text": "13.5 Measuring Privacy\n\n13.5.1 Pseudo-identifiers and \\(k\\)-anonymity\n\nPseudo-identifiers: Attributes that can also be observed in public data. For example, someone’s name, job title, zip code, or email.\n\n\nFor the set of quasi-identifiers \\(A_1, \\ldots ,A_p\\), a table is \\(k\\)-anonymous if each possible value assignment to these variables \\((a_1, . . . , a_n)\\) is observed for either 0 or at least \\(k\\) individuals.\n\n\\(k\\)-anonymity is a technique that ensures each combination of attributes within a table, is shared by at least k records within that dataset. This ensures that each individual’s data can’t be distinguished from at least \\(k-1\\) other individuals.\nThis provides some rudimentary level of privacy, where \\(k\\) corresponds to the size of smallest equivalence class of pseudo-identifiers within the data. Therefore larger values of \\(k\\) correspond to greater levels of privacy.\nTo see this more concretely, let’s take a look at an example.\n\n13.5.2 \\(k\\)-anonymity example\nIn this example we have a dataset recording visits to a sexual health clinic. We wish to establish the \\(k\\)-anonymity of this data set, where the diagnosed condition should be kept private. To do help with this, the drug use status has been removed for all patients and only partial information is available about their postcode and age.\n\n\n\nPost Code\nAge\nDrug Use\nCondition\n\n\n\n1\nOX1****\n<20\n*\nHerpes\n\n\n2\nOX1****\n<20\n*\nHerpes\n\n\n3\nOX2****\n>=30\n*\nChlamydia\n\n\n4\nOX2****\n>=30\n*\nHerpes\n\n\n5\nOX1****\n<20\n*\nGonorrhoea\n\n\n6\nOX2****\n>=30\n*\nGonorrhoea\n\n\n7\nOX1****\n<20\n*\nGonorrhoea\n\n\n8\nLA1****\n2*\n*\nChlamydia\n\n\n9\nLA1****\n2*\n*\nChlamydia\n\n\n10\nOX2****\n>=30\n*\nGonorrhoea\n\n\n11\nLA1****\n2*\n*\nChlamydia\n\n\n12\nLA1****\n2*\n*\nChlamydia\n\n\n\nBy grouping observations by each distinct combination of pseudo-identifiers we can establish the equivalence classes within the data.\n\n\n\n\n\n\n\n\n\n\n\nPost Code\nAge\nDrug Use\nCondition\nEquivalence Class\n\n\n\n1\nOX1****\n<20\n*\nHerpes\n1\n\n\n2\nOX1****\n<20\n*\nHerpes\n1\n\n\n3\nOX2****\n>=30\n*\nChlamydia\n2\n\n\n4\nOX2****\n>=30\n*\nHerpes\n2\n\n\n5\nOX1****\n<20\n*\nGonorrhoea\n1\n\n\n6\nOX2****\n>=30\n*\nGonorrhoea\n2\n\n\n7\nOX1****\n<20\n*\nGonorrhoea\n1\n\n\n8\nLA1****\n2*\n*\nChlamydia\n3\n\n\n9\nLA1****\n2*\n*\nChlamydia\n3\n\n\n10\nOX2****\n>=30\n*\nGonorrhoea\n2\n\n\n11\nLA1****\n2*\n*\nChlamydia\n3\n\n\n12\nLA1****\n2*\n*\nChlamydia\n3\n\n\n\nHere we have three distinct equivalence classes, each with four observations. Therefore the smallest equivalence class is also of size four and this data set is 4-anonymous.\nWhile we can easily identify the equivalence classes in this small dataset, doing so in large datasets is a non-trivial task."
  },
  {
    "objectID": "501-ethics-privacy.html#improving-privacy",
    "href": "501-ethics-privacy.html#improving-privacy",
    "title": "13  Privacy",
    "section": "\n13.6 Improving Privacy",
    "text": "13.6 Improving Privacy\nThere are three main ways that you can improve the level of privacy within your data, and we have seen examples of two of these already.\nRedaction may be applied to individual or to an attribute, leading to a whole row or column being censored. This is quite an extreme approach: it can lead to a large amount of information being removed from the data set. However, sometimes redacting a full row is necessary; for example when that row contains identifying information like a person’s name or national insurance number. An additional concern when redacting rows from your data is that it will artificially alter the distribution of your sample, making it unrepresentative of the population values.\nAggregation or coarsening is a second approach where the level of anonymity can be increased by binning continuous variables into discrete ranges or by combining categories within a variable that already takes discrete values. The idea here is to reduce the number of equivalence classes within the quasi-identifiers so that the level of k-anonymity is increased.\nA similar approach is to corrupt or obfuscate the observed data by adding noise to the observations, or permuting some portion of them. The aim is to retain overall patterns but ensure individual recorded values no longer correspond to an individual in the raw data set. The difficulty here is in setting the type and amount of noise to be added to the data to grant sufficient privacy without removing all information from the dataset.\nThis trade-off between information loss and privacy is a common theme throughout all of these methods."
  },
  {
    "objectID": "501-ethics-privacy.html#breaking-k-anonymity",
    "href": "501-ethics-privacy.html#breaking-k-anonymity",
    "title": "13  Privacy",
    "section": "\n13.7 Breaking \\(k\\)-anonymity",
    "text": "13.7 Breaking \\(k\\)-anonymity\n\\(k\\)-anonymity ensures that there are at least \\(k-1\\) other people with your particular combination of pseudo-identifiers. What it does not do is ensure that there is any variation within a particular group. The dataset on sexual health we just saw was 4-anonymous, but if we know a person how attended the clinic was from a Lancashire (LA) postcode (and in their 20s) then we know for certain that they have Chlamydia. An alternative privacy measure called \\(l\\)-diversity tries to address this issue.\n\n\n\nPost Code\nAge\nDrug Use\nCondition\n\n\n\n8\nLA1****\n2*\n*\nChlamydia\n\n\n9\nLA1****\n2*\n*\nChlamydia\n\n\n11\nLA1****\n2*\n*\nChlamydia\n\n\n12\nLA1****\n2*\n*\nChlamydia\n\n\n\nA second problem with \\(k\\)-anonymity is that this type of privacy measure is focused entirely on the data available within this dataset. It does not take into account data that might be available elsewhere or might become publicly available in the future. An external data-linkage attack can cross-reference this table against other information to reduce the size of equivalence classes and reveal personal information."
  },
  {
    "objectID": "501-ethics-privacy.html#cautionary-tales",
    "href": "501-ethics-privacy.html#cautionary-tales",
    "title": "13  Privacy",
    "section": "\n13.8 Cautionary tales",
    "text": "13.8 Cautionary tales\n\n13.8.1 Massachussets Medical Data\nMedical research is often slow because it is very difficult to share medical records while maintaining patients’ privacy. In the 1990s a government agency in Massachusetts wanted to improve this by releasing a dataset summarising the hospital visits made by all state employees. They were understandably quite careful about this, making this information available only to academic researchers and redacted all information like names, addresses and security numbers. They did include the the patient’s date of birth, zip code, and sex - this information was deemed sufficiently general while allowing difference in healthcare provision to be investigated.\n\n\n\nLatanya Sweeney Speaking in New York, 2017. Image CC-4.0 from Parker Higgins.\n\n\n\nLatanya Sweeney is now a pre-eminent researcher in the field of data privacy. In the 1990s she was studying for a PhD and MIT and wanted to demonstrate the potential risks of de-anonymising this sort of data. To demonstrate her point she chose to focus on the public records of Massachusetts’ governor, William Weld. For a small fee, Sweeney was able to obtain the voter registration records for the area in which the governor lived. By cross-referencing the two datasets Sweeney was able to uniquely identify the governors medical history and send them to him in the post. This was particularly embarrassing for Weld. since he had previously given a statement reassuring the public that this data release would not compromise the privacy of public servants.\nThis interaction between Sweeney and the Governor of Massachusetts was significant because it highlighted the potential privacy risks associated with the release of publicly available information. It demonstrated that even when data is stripped of names and other identifying information, it can still be possible to re-identify individuals and potentially access sensitive information. The problem here only grows with the dimension of the dataset - the more characteristics that are measures the greater the chance of one person having a unique combination of those.\n\n13.8.2 Neflix Competition\n\n\n\nNetflix logo 2001-2014. Public domain image.\n\n\n\nIn 2006 Netflix announced a public competition to improve the performance of their recommender system with a pros of 1 million USD to the first person or group to improve its performance by at least 10%. For this competition over 100 million ratings from 480,000 users were made public. Each entry contained a pseudonymised user ID, a film id, the rating out of 5 stars and the date that rating was given.\n\n\nUser ID\nFilm ID\nRating\nDate\n\n\n\n000001\n548782\n5\n2001-01-01\n\n\n000001\n549325\n1\n2001-01-01\n\n\n…\n…\n…\n…\n\n\n\nAlthough the Netflix team had pseudonymised the data (and taken other steps like adding noise to the observations), two researchers at the University of Texas were able to successfully re-identify a 96% of individuals within the data. They did this by cross reference the competition dataset against reviews on the openly available internet movie database (IMDb), working on the supposition that users would rate films on both services at approximately the same time - the 96% figure uses a margin of 3 days.\nThe researchers went further, showing that if we were to alter the data to achieve even a modest 2-anonymity then almost all of the useful information would be removed from competition data set.\nThis example should show how difficult it can be to ensure individual privacy in the face of unknown external data sources. It might seem like a trivial example compared to medical records but the media that you consume, and in particular how you rate that media, cam reveal you religious beliefs, your political stance or your sexual orientation. These are protected characteristics that you might not want to broadcast freely.\nIt might not be important if you, or even the average Netflix user, if that information becomes public. What is important is whether any user would find this privacy breach objectionable and potentially come to harm because of it."
  },
  {
    "objectID": "501-ethics-privacy.html#wrapping-up",
    "href": "501-ethics-privacy.html#wrapping-up",
    "title": "13  Privacy",
    "section": "\n13.9 Wrapping Up",
    "text": "13.9 Wrapping Up\n\n\nPrivacy is a fundamental concern.\nPrivacy is hard to measure and hard to ensure.\nAlso a model issue, since models are trained on data.\nNo universal answers, but an exciting area of ongoing research.\n\n\nAlthough we have only scratched the surface of privacy in data science, we will have to wrap this video up here.\nWe have seen that privacy should be a fundamental concern when working with any from of human-related data. This is chiefly because we aren’t in a position to determine what types of privacy breach might significantly and negatively impact the lives of the people we hold data about.\nWe have seen through one example metric that privacy can be both difficult to measure and even more difficult to preserve. This is not only an issue when releasing data into the world, but also when publishing models trained on this data. Approaches analogous to those used by Latanya Sweeney’s can be used be used by bad-actors to identify with high precision whether a given individual was included within the data that was used to train a published model.\nThere are no universal answers to the question of privacy in data science, this is what makes it an exciting area of ongoing research. It is also for this reason that a lot of Sweeney’s work was published only after overcoming great resistance: it exposed systematic vulnerabilities for which there are currently few reliable solutions."
  },
  {
    "objectID": "502-ethics-fairness.html",
    "href": "502-ethics-fairness.html",
    "title": "14  Fairness",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "502-ethics-fairness.html#fairness-and-the-data-revolution",
    "href": "502-ethics-fairness.html#fairness-and-the-data-revolution",
    "title": "14  Fairness",
    "section": "\n14.1 Fairness and the Data Revolution",
    "text": "14.1 Fairness and the Data Revolution\n\n\n\n\nBefore the 1990s, large datasets were typically only collected to understand huge, complex systems. These systems might be the weather, public infrastructure (e.g. hospitals, roads or train networks), the stock market or even populations of people.\nCollecting high quality data on these systems was immensely expensive but paid dividends by allowing us to describe the expected behaviour of these systems at an aggregate level. Using this sort of information, we can’t make journeys or healthcare better at an individual level but we can make changes to try and make these experiences better on average.\nThings changed with the widespread adoption of the internet in the mid-1990s and the subsequent surge in data collection, sharing and processing. Suddenly, we as individuals shifted from being just one part of these huge processes to being a complex process worth of modelling all on our own.\n\n\n\n\nFigure 14.1: Volume of data created, captured, copied, and consumed worldwide from 2010 to 2022.\n\n\n\n\nIt was at this point that focus shifted toward making individual, personalised predictions for specific people, based on the vast amounts of data that we generate as we go about our daily lives.\nThis shift from aggregate to individual behaviour creates the opportunity not only for these predictions to systematically harm groups of people, as they always could, but also to acutely harm individuals."
  },
  {
    "objectID": "502-ethics-fairness.html#you-are-your-data",
    "href": "502-ethics-fairness.html#you-are-your-data",
    "title": "14  Fairness",
    "section": "\n14.2 You are Your Data",
    "text": "14.2 You are Your Data\n\n\n\nThe blunt truth is that, as far as a data science model is concerned, you are nothing more than a point in a high-dimensional predictor space.\n\n\n\n\n\n\n\n\nThe model might use your location in that space to group you with other points that are in some sense “nearby”. Alternatively, the model might estimate some information about you that it currently doesn’t have, based on what it knows about those surrounding points. These other points also represent unique humans with rich and fascinating lives - but the model doesn’t care about that, it is just there to group some points or predict some values.\n\n\n\n\n\n\n\n\nThe idea of fairness comes into data science when we begin to ask ourselves which predictors we should provide the model with when carrying out these tasks. We aren’t asking this from a model selection stand-point. We are asking what are morally permissible predictors, not what leads to a significant improvement in model fit."
  },
  {
    "objectID": "502-ethics-fairness.html#forbidden-predictors",
    "href": "502-ethics-fairness.html#forbidden-predictors",
    "title": "14  Fairness",
    "section": "\n14.3 Forbidden Predictors",
    "text": "14.3 Forbidden Predictors\nThe argument about what features of a human being can be used to make decisions about them started well before the 1990s. The most contentious of these arguments centre around the inclusion of characteristics that are either immutable or not easily changed. Some of these characteristics including race, gender, age or religion receive legal protections. These protected attributes are often forbidden to be used in important decisions, such as whether a bank loan is accepted.\nThis natural lead us to ask what classifies as an important decision?\n\nProtected Characteristics under the Equality Act (2010)\n\n\n\n\nage\ngender reassignment\nmarriage / civil partnership\npregnancy / parental leave\ndisability\n\n\n\nrace including colour, nationality, ethnic or national origin\nreligion or belief\nsex\nsexual orientation\n\n\n\n\nWe also need to be careful if these protected attributes actually have strong predictive power and would improve our predictions (potentially to the benefit of the groups that are being protected by these regulations). Just because a protected attribute isn’t used directly within a model that doesn’t mean the model will not discriminate according to that attribute.\nIf we have multiple predictors within a model, then withholding a protected attribute does not make the model ignorant of that attribute. If you have access to someone’s browsing history, where they live and some of their recent purchases you can probably make a fairly accurate profile of that person, including many of these supposedly protected attributes. In the same way, a model can use or combine attributes that are not legally protected to create a new variable that acts as an accurate proxy for the protected characteristic.\nAnd why wouldn’t our model do this? When using a standard loss function we have literally asked it to get the best possible predictive performance. If a protected attribute has predictive power then the model is likely to approximate it using the predictors that are available to it.\nBefore we see how to handle this concern, let’s step back and consider how we can quantify and measure fairness in a model."
  },
  {
    "objectID": "502-ethics-fairness.html#measuring-fairness",
    "href": "502-ethics-fairness.html#measuring-fairness",
    "title": "14  Fairness",
    "section": "\n14.4 Measuring Fairness",
    "text": "14.4 Measuring Fairness\nConverting the concept of fairness into a mathematical statement is a very difficult task. This is partly because moving from natural language to precise formalism is hard, but it’s also because the term fairness means different things to different people in different contexts. Because of this, there are many complementary definitions of fairness that all try to capture some intuitive notion of a fair model. However, these measures all capture different facets of this complicated concept. Despite this, these measures vary to such an extent they can’t all be satisfied simultaneously.\nI’ll introduce four such measures shortly, focusing in on the case of binary outcomes where a “positive” response of 1 corresponds to an event that would be considered favourably when taken in context. For example this might be a loan that will be successfully repaid or that a person released on bail will not re-offend.\n\nBinary outcome \\(Y \\in \\{0,1\\}\\).\n\nWe’ll consider the simple case where a binary prediction is made in each instance, and where we want our predictions to be fair across the \\(k\\) distinct levels of some protected attribute \\(A\\).\n\nBinary Prediction \\(\\hat Y \\in \\{0,1\\}\\).\nProtected attribute \\(A\\) takes values in \\(\\mathcal{A} = \\{a_1, \\ldots, a_k\\}\\).\n\n\n14.4.1 Demographic Parity\nThe first, and potentially most obvious fairness definition is that of demographic parity. Here a model is deemed fair if, across all subgroups of the protected attribute, the probability of predicting a successful outcome is equal.\n\n\nThe probability of predicting a ‘positive’ outcome is the same for all groups.\n\n\n\\[\\mathbb{P}(\\hat Y = 1 | A = a_i) = \\mathbb{P}( \\hat Y = 1 | A = a_j), \\  \\text{ for all }\\  i,j \\in \\mathcal{A}.\\]\nAn obvious shortcoming demographic parity is that it does not allow us to account for the fact that a positive outcome might not be equally likely in each of these subgroups. In this way demographic parity is analogous to treating people equally, rather than equitably.\n\n14.4.2 Equal Opportunity\nEquality of opportunity addresses this shortcoming by conditioning on a truly positive outcome. Equality of opportunity states that of those who are “worthy” of a loan (in some sense), all subgroups of the protected characteristic should be treated equally.\n\n\nAmong those who have a true ‘positive’ outcome, the probability of predicting a ‘positive’ outcome is the same for all groups.\n\n\n\\[\\mathbb{P}(\\hat Y = 1 | A = a_i, Y =1) = \\mathbb{P}( \\hat Y = 1 | A = a_j, Y=1), \\  \\text{ for all }\\  i,j \\in \\mathcal{A}.\\]\n\n14.4.3 Equal Odds\nOf course, you have encountered two-way tables, type-I and type-II errors. Equally important as granting loans to people who will repay them is to deny loans to those who cannot afford them.\nA model satisfying the equal odds condition can identify true positives and false negatives equally well across all sub-groups of the protected characteristic.\n\nAmong those who have a true ‘positive’ outcome, the probability of predicting a ‘positive’ outcome is the same for all groups.\nAND\nAmong those who have a true ‘negative’ outcome, the probability of predicting a ‘negative’ outcome is the same for all groups.\n\n\n\\[\\mathbb{P}(\\hat Y = y | A = a_i, Y =y) = \\mathbb{P}( \\hat Y = y | A = a_j, Y=y), \\ \\text{ for all } \\ y \\in \\{0,1\\} \\ \\text{ and } \\  i,j \\in \\mathcal{A}.\\]\n\n14.4.4 Predictive Parity\nAll of the measures we have considered so far consider the probability of a prediction given the true credit-worthiness of an applicant. Predictive Parity reverses the order of conditioning (as compared to equal opportunity).\nIt ensures that among those predicted to have a successful outcome, the probability of a truly successful outcome should be the same for all subgroups of the protected characteristic. This ensures that, in our financial example, the bank is spreading its risk exposure equally across all subgroups; each subgroup should have an approximately equal proportion of approved loans being successfully repaid.\n\nThe probability of a true ‘positive’ outcome for people who were predicted a ‘positive’ outcome is equal across groups.\n\n\n\\[\\mathbb{P}(Y = 1 | \\hat Y = 1, A = a_i) = \\mathbb{P}(Y_1 = 1 | \\hat Y = 1, A = a_j) \\ \\text{ for all } \\  i,j \\in \\mathcal{A}.\\]\nWe can play devil’s advocate here and say that this might not be appropriate if there is a genuine difference in the probability of successful repayment between groups."
  },
  {
    "objectID": "502-ethics-fairness.html#metric-madness",
    "href": "502-ethics-fairness.html#metric-madness",
    "title": "14  Fairness",
    "section": "\n14.5 Metric Madness",
    "text": "14.5 Metric Madness\nEven with this very simple binary classification problem that there are many ways we can interpret the term fairness. Which, if any, of these will be appropriate is going to be highly context dependent.\nAn issue with many of these metrics, including some of those introduced, is that they require knowledge of the true outcome. This means that these metrics can only be evaluated retrospectively: if we knew this information to begin with then we wouldn’t need a model to decide who get a loan. On top of this, it means that we only ever get information about the loans that are granted - we don’t have access to the counter factual outcome of whether a loan that was not granted would have been repaid.\nAn additional problem is that evaluating these fairness metrics requires us to know which protected sub-group each individual belongs to. This is clearly a problem: to evaluate the fairness of our loan applications we need to know sensitive information about the applicants, who would - very reasonably - be unwilling to provide that information because it legally cannot be used to inform the decision making process. For this reason, an independent third-party is often required to assess fairness by collating data from the applicants and the bank.\nA third complication here is that these definitions deal in strict equalities. In any given sample, these are almost surely not going to be satisfied even if the metric is truly satisfied. A formal statistical test should be used to assess whether these differences are consistent with a truly fair model, however the more common approach is for regulators to set some acceptable tolerance on the discrepancy in metric values between sub-groups.\nFinally, it is worth noting that all of these problems arise for a simple binary classifier but most models are far more complicated than this. Even working with these conditional probability statements requires careful attention, but things get much trickier when the response or sensitive attribute are continuous valued or when other, non-sensitive predictors are also included in the model."
  },
  {
    "objectID": "502-ethics-fairness.html#modelling-fairly",
    "href": "502-ethics-fairness.html#modelling-fairly",
    "title": "14  Fairness",
    "section": "\n14.6 Modelling Fairly",
    "text": "14.6 Modelling Fairly\n\n14.6.1 Fairness Aware Loss Functions\nNow we have some methods to detect and quantify the fairness of our models, how do we incorporate that into the model fitting process?\n\n\n\nWe now have multiple objectives: to predict our outcome as accurately as possible while also treating people as fairly as possible (by which ever fairness metric or combination of metrics we care to consider). Unfortunately, these things are generally in competition with each other. There is no one best model but rather a family of best models, from which we have to pick a single model to implement.\nCan resolve this issue by linearisation, create our loss function as a linear weighted sum of the two component objectives. This simplifies the problem mathematically, but actually just shift the problem rather than resolving it. Up to scaling constant, each combination of weights corresponds to a unique point on the Pareto frontier, so we have just translated our problem from picking a point on the frontier to picking a pair of weights.\nTo do actually resolve this issue we need to define our relative preference between fairness and predictive power. When I say “our preference”, what I actually mean that of the company or organisation for whom we are doing an analysis - not our own personal view. Eliciting this preference and communicating the idea of an optimal frontier can be tricky. One solution is to present a small set of possible models, which represent a range of preferences between the two competing objectives, and ask the stakeholder to choose between these.\n\n14.6.2 Other Approaches\n\n\nRe-weighting or resampling to better represent minority groups.\nForgetting factor to reduce impact of bias in historical data.\nMeta-modelling to intervene in harmful feedback loops.\n\n\nWhenever we treating all predictions equally and our loss function optimises purely for predictive performance, good predictions for minority groups will never be prioritised. One strategy to correct for this is to either re-weighting or re-sample each observation so that minority groups are given greater importance within the loss function.\nA lot of the problems of fairness that we see are because our models are replicating what happens or used to happen in reality. In some cases, this is being better addressed now and a model can be made more fair by down-weighting training data as it ages. This allows our model to more quickly adapt to changes in the system it is trying to represent.\nIn other cases the use of historically biased data to train models that are put into production has lead to a feedback loop that makes more recent data even more unjust. One example of this, we can consider racial disparity in the interest rates offered on mortgages. Suppose that one racial group of applicants was in the past slightly more likely to default on loans, perhaps due to historical pay inequity. This means that models would likely suggest higher interest loans to this group, in an attempt to offset the bank’s exposure to the risk of non-repayment.\nThis not only reduces the number of loans that will be granted to that racial group but it also makes the loans that are granted more difficult to repay and more likely to be defaulted on. This in turn leads to another increase in the offered interest rate, driving down the number of loans approved and pushing up the chance of non-repayment even further.\nThe decisions made using this model are impacting its future training data and creating a harmful and self-reinforcing feedback loop. Historical down weighting will do nothing to address this sort of issue, which requires active intervention.\nA meta-modelling approach is possible type of intervention. Here post-hoc methods used to estimate the biases within a fitted model and these estimates are used to explicitly correct for historical biases, before the model is used to make predictions or decisions."
  },
  {
    "objectID": "502-ethics-fairness.html#wrapping-up",
    "href": "502-ethics-fairness.html#wrapping-up",
    "title": "14  Fairness",
    "section": "\n14.7 Wrapping Up",
    "text": "14.7 Wrapping Up\nThat’s a good point for us to wrap up this introduction to fairness in data science.\nWe have seen that optimising for predictive accuracy alone can lead to unjust models. We also raised concerns about protected characteristics being included in models, whether that is directly as a predictor or via a collection of other predictors that well approximate them.\nWe have seen that there are a multitude of measures to assess the fairness of our models. We can combine these with standard metrics for goodness-of-fit to create custom loss functions which represent our preference between fairness and predictive performance.\nAs with privacy, there are no universal answers when it comes to measuring and implementing fair data science methodology. This is still a relatively new and rapidly evolving field of data science."
  },
  {
    "objectID": "503-ethics-conduct.html",
    "href": "503-ethics-conduct.html",
    "title": "15  Codes of Conduct",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "503-ethics-conduct.html#data-science-miracle-cure-and-sexiest-job",
    "href": "503-ethics-conduct.html#data-science-miracle-cure-and-sexiest-job",
    "title": "15  Codes of Conduct",
    "section": "\n15.1 Data Science: Miracle Cure and Sexiest Job",
    "text": "15.1 Data Science: Miracle Cure and Sexiest Job\n\n\n\nTitle of Harvard Business Review article: Is data scientist still the sexiest job of the 21st Century?\n\n\n\nIt has been more than 10 years since it was proclaimed that data science was the sexiest job of the century. Current turbulence in the technology sector may have you questioning the veracity of that claim, but it still rings true if we take a less myopic view of where data science is used.\nData science and machine learning are being used more extensively than ever across fields including medicine, healthcare and sociology. Data science is also applied to understand our environment, study ecological systems and inform our strategies for their preservation. In addition to this, data science is used widely across the private sector, where it informs business strategies and financial services, alongside the public sector where it influences governance and policy development."
  },
  {
    "objectID": "503-ethics-conduct.html#what-could-go-wrong",
    "href": "503-ethics-conduct.html#what-could-go-wrong",
    "title": "15  Codes of Conduct",
    "section": "\n15.2 What could go wrong?",
    "text": "15.2 What could go wrong?\nWhile data science methods can be wonderful, they are not infallible. As the number of use cases increases, we should also expect that the number of misuses or high-profile failures to increase too.\nThese two news articles highlight just how badly this can all go.\n\n\n\nBBC news article: facial recognition fails on race, government study says.\n\n\n\nThe first article shows how a combination of non-representative training data and lack of consideration on the part of developers led to a facial recognition system that was much less accurate for people with darker skin. This is absolutely unacceptable, particularly given the widespread use of similar systems in important security applications such as biometric ID for online banking or automatic passport gates.\nA second example shows the risk of catastrophic failures in self-driving cars. We know that misclassification and missed edge cases are inevitable and that in the case of self-driving cars these errors can cause serious and even fatal accidents. We might then ask ourselves if these errors are acceptable if they lead to fewer accidents or fatalities than would be caused by human drivers.\n\n\n\nBBC news article: Uber’s self-driving operator charged over fatal crash.\n\n\n\nAnother important point to establish is where liability falls in such cases: should it be with the operator of the self-driving vehicle, the vendor who solid it or the data scientist who wrote the script controlling the car’s actions? If this behaviour was determined by training data for test-drives, should that driver share in the liability.\nThese are all important questions that we haven’t necessarily thought to ask before deploying data science solutions into the world."
  },
  {
    "objectID": "503-ethics-conduct.html#thats-not-my-type-of-data-science",
    "href": "503-ethics-conduct.html#thats-not-my-type-of-data-science",
    "title": "15  Codes of Conduct",
    "section": "\n15.3 That’s not my type of data science …",
    "text": "15.3 That’s not my type of data science …\nYou might be thinking at this doesn’t effect you because you don’t work in image analysis or on self-driving cars, but related issues come up in more standard applications of data science.\nConsider a retailer implementing targeted promotions. They might combine previous shopping habits and recent product searches to target their offers.\n\n\n\n\n\nFigure 15.1: Predicting pregnancy from search and purchase history.\n\n\n\n\n\nAround 7 months ago, one customer stopped regularly buying contraceptives and alcohol and started buying supplements for vitamin D and folic acid. Based on similar behaviour by lots of other customers, a recommender model might now expect a good way to increase sales would be to send that customer offers for nappies, baby food and talcum powder.\nThis could be seriously upsetting if that customer also happened to be experiencing fertility issues or had been pregnant but had an abortion or miscarriage. It is important that human-overrides are included in such systems like this so that the customer could contact the retailer and prevent this sort of advertising from continuing to happen for months or years to come."
  },
  {
    "objectID": "503-ethics-conduct.html#technological-adoption-relies-on-public-trust",
    "href": "503-ethics-conduct.html#technological-adoption-relies-on-public-trust",
    "title": "15  Codes of Conduct",
    "section": "\n15.4 Technological Adoption Relies on Public Trust",
    "text": "15.4 Technological Adoption Relies on Public Trust\n\n\n\n\n\nThirteen people were killed and 145 injured during the Interstate 35W bridge collapse in 2007.\n\n\n\n\n\nData science is a new discipline, which means that some of the issues we have mentioned are new and happening for the first time. However, many of these issues are not unique to data science and that means we can learn from other disciplines that have already invested a lot of time and energy into those problems.\nAcross many other professions it’s standard to follow a professional code of practice or to have a code of ethics that applies to everyone working in that space. Usually, the professions that have this in place are also those who are legally liable in some way for the outcomes and consequences of their work.\nThe most obvious example of this might be doctors and other medical professionals, but this holds equally true in many jobs from law to engineering, where practitioner’s work can impact the freedom or safety of other people. When thinking critically about the practical and ethical implications of our work as data scientists, we shouldn’t start from scratch but should learn as much as possible from these other fields.\nOne profession in particular that we can learn from is medicine. Around the world, doctors agree to uphold the privacy of their patients by maintaining medical confidentiality. The also agree to only act in what they believe to be the best interests of the patient and to do them no harm. These are both admirable principles that can be tracked directly onto our work in data science.\nGoogle’s original internal code encapsulated a more extreme version of this non-maleficence principle, starting with the phrase ‘Don’t be evil’ (thought this was later rephrased to ‘You can make money without being evil’)."
  },
  {
    "objectID": "503-ethics-conduct.html#a-hippocratic-oath-for-data-scientists",
    "href": "503-ethics-conduct.html#a-hippocratic-oath-for-data-scientists",
    "title": "15  Codes of Conduct",
    "section": "\n15.5 A Hippocratic Oath for Data Scientists",
    "text": "15.5 A Hippocratic Oath for Data Scientists\n\n\n\n\nIn Weapons of Math Destruction (2016), Cathy O’Neil was among the first authors to make a call for a Hippocratic oath for data scientists\n\n\n\n\nWe’ve seen examples of both the broad and the acute harms that can befall individuals because of data science. With these in mind, it seems reasonable to expect that data scientists should have to be aware of the negative impacts of their work and be required to mitigate these wherever possible.\nThis is the argument made by Cathy O’Neil in her book Weapons of Math Destruction: data scientists, and those working with mathematical models more generally, need an equivalent of the Hippocratic oath that’s used in medicine. To understand why this is necessary, we have to understand why it is not the case already.\nDoing the right thing, in data science as in life,is neither obvious nor easy.\nIn the most naive sense, negative consequences can come about as a result of a lack of understanding or because of unanticipated consequences. This lack of understanding might relate to the model itself (which might not be explainable) or about the setting in which the model is being applied (which might require domain specific expertise that the data scientist lacks).\nAlternatively, this ignorance can be about the groups of people who are most at risk of harm from data science models. Data science methods tend to model expected behaviour and echo biases from within the training data. This means that minority groups or groups that have been historically disadvantaged are most at risk of further harm. This same minority status or historical disadvantage means that these same groups have low representation within data science teams. This increases the chance that data science teams are ignorant of or ignore the breadth and depth of the damage they might be causing.\nA second way that data science can lead to harm is when incentives are not properly aligned. As an example we consider a credit scoring application where a data scientist is trying to include fairness measures in addition to optimising predictive accuracy for loan repayments. If business incentives are purely based around immediate increases in profit, then this will likely be very difficult to get put into production. There is a misalignment between the priorities of the business and the ethics of the data scientist.\nAs we have seen,some errors are inevitable and sometimes the best we can do is to balance an inherent trade-off between conflicting priorities or different sources of harm. In these cases it is vitally important that we highlight this trade-off explicitly and offer decision makers a range of solutions that they may choose according to their own priorities.\nDoing the right thing is neither obvious or easy:\n\nLack of understanding,\nUnanticipated consequences\nIncentive structures,\nInherent trade-offs."
  },
  {
    "objectID": "503-ethics-conduct.html#codes-of-conduct",
    "href": "503-ethics-conduct.html#codes-of-conduct",
    "title": "15  Codes of Conduct",
    "section": "\n15.6 Codes of Conduct",
    "text": "15.6 Codes of Conduct\nMembership of a professional body can offer some of the ethical benefits of a Hippocratic oath, because these memberships often require agreement to abide by a code of conduct.\nThis provides individual data scientists with accountability for their actions in a broader sense than through their direct accountability to their employer. This can be helpful as a motivator to at a high quality and in line with ethical guidelines.\nIt can also be useful as a form of external support, to convince organisations who employ data scientists that ethical considerations are an important aspect of our work. There is also the opportunity for peer-to-peer support, so that best practices can be established and improved in a unified way across industries, rather than in isolation within individual organisations. This might be through informal networking and knowledge sharing or through formal training organised by that professional body.\nData science is still a developing field and there currently is no professional body specifically dedicated to data science. Instead, data scientists might join one or more related professional bodies, such as the Royal Statistical Society, the Operational Research Society, or the Society for Industrial and Applied Mathematics."
  },
  {
    "objectID": "503-ethics-conduct.html#wrapping-up",
    "href": "503-ethics-conduct.html#wrapping-up",
    "title": "15  Codes of Conduct",
    "section": "\n15.7 Wrapping Up",
    "text": "15.7 Wrapping Up\nData Science is still maturing, both as a field of study and as a career path. In combination with the widespread adoption of data science in recent years, this means that as a discipline we’re now navigating many new modes of failure and ethical issues. However, not all of these issues are entirely novel. There is an awful lot that we can learn, borrow or adapt from more established fields such as medicine, engineering or physics.\nIt is important that we remain alert to the dangers of our work: both the liability it opens us up to and the harm we might be causing to others, potentially without being aware of either of these things. By joining professional bodies and upholding their codes of conduct we can push back against bad practices and reward good ones."
  },
  {
    "objectID": "510-ethics-checklist.html",
    "href": "510-ethics-checklist.html",
    "title": "Checklist",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes."
  },
  {
    "objectID": "510-ethics-checklist.html#videos-chapters",
    "href": "510-ethics-checklist.html#videos-chapters",
    "title": "Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nPrivacy (19 min) [slides]\nFairness (20 min) [slides]\nCodes of Conduct (14 min) [slides]"
  },
  {
    "objectID": "510-ethics-checklist.html#reading",
    "href": "510-ethics-checklist.html#reading",
    "title": "Checklist",
    "section": "Reading",
    "text": "Reading\nUse the Data Science Ethics section of the reading list to support and guide your exploration of this week’s topics. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "510-ethics-checklist.html#activities",
    "href": "510-ethics-checklist.html#activities",
    "title": "Checklist",
    "section": "Activities",
    "text": "Activities\nThis week has fewer activities, so that you may look over the second assessment before the end of the course.\nCore:\n\nRead Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification by Joy Buolamwini and Timnit Gebru (2018). Proceedings of the 1st Conference on Fairness, Accountability and Transparency.\nFind an example case study or method relating to ethical data science that has not been covered in the lectures. Share what you find by writing a short summary of the case study or method on the discussion forum.\nSkim over the Professional Guidelines listed in the reference materials for this week, in preparation for the live session.\n\nBonus\n\nAnswer the multiple choice questions on Buolamwini and Timnit Gebru (2018)."
  },
  {
    "objectID": "510-ethics-checklist.html#live-session",
    "href": "510-ethics-checklist.html#live-session",
    "title": "Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session this week we will begin with a few minutes of Q & A about the assessments. We will then break into groups to discuss and compare the professional guidelines on ethical data science.\nFinally, if time allows we will round up the session with an activity on randomised response survey designs."
  },
  {
    "objectID": "600-reading-list.html#workflows-reading",
    "href": "600-reading-list.html#workflows-reading",
    "title": "Appendix A — Reading List",
    "section": "\nA.1 Effective Data Science Workflows",
    "text": "A.1 Effective Data Science Workflows\nCore Materials\n\nThe Tidyverse R Style Guide by Hadley Wickham.\n\n\n\nWilson, et al (2017). Good Enough Practices in Scientific Computing. PLOS Computational Biology.\n\nReference Materials\n\nR For Data Science Chapters 2, 6 and 8 by Hadley Wickham and Garrett Grolemund. Chapters covering R workflow basics, a scripting and project based workflow.\nDocumentation for the {here} package\nR Packages Book (Second Edition) by Hadley Wickham and Jenny Bryan.\n\nMaterials of Interest\n\n\nSTAT545, Part 1 by Jennifer Bryan and The STAT 545 TAs\n\n\n\nWhat they forgot to teach you about R, Chapters 2-4 by Jennifer Bryan and Jim Hester.\n\n\n\nBroman et al (2017). Recommendations to Funding Agencies for Supporting Reproducible Research. American Statistical Association.\n\n\nAdvanced R by Hadley Wickham Section introductions on functional and object oriented approaches to programming.\nAtlassian Article on Agile Project Management\n\n\n\nThe Pragmatic Programmer, 20th Anniversary Edition Edition by David Thomas and Andrew Hunt. The section on DRY coding and a few others are freely available.\n\n\nEfficient R programming by Colin Gillespie and Robin Lovelace. Chapter 5 considers Efficient Input/Output is relevant to this week. Chapter 4 on Efficient Workflows links nicely with last week’s topics.\nTowards A Principled Bayesian Workflow by Michael Betancourt.\n\n\n\nHappy Git and GitHub for the useR by Jennifer Bryan"
  },
  {
    "objectID": "600-reading-list.html#data-reading",
    "href": "600-reading-list.html#data-reading",
    "title": "Appendix A — Reading List",
    "section": "\nA.2 Aquiring and Sharing Data",
    "text": "A.2 Aquiring and Sharing Data\nCore Materials\n\nR for Data Science Chapters 9 - 12 by Hadley Wickham. These chapters introduce tibbles as a data structure, how to import data into R and how to wrangle that data into tidy format.\nEfficient R programming by Colin Gillespie and Robin Lovelace. Chapter 5 considers Efficient Input/Output is relevant to this week.\nWickham (2014). Tidy Data. Journal of Statistical Software. The paper that brought tidy data to the mainstream.\nReference Materials\n\nThe {readr} documentation\nThe {data.table} documentation and vignette\nThe {rvest} documentation\nThe {tidyr} documentation\nMDN Web Docs on HTML and CSS\nMaterials of Interest\n\n\nIntroduction to APIs by Brian Cooksey\n\nR for Data Science (Second Edition) Chapters within the Import section.\n\nThis covers importing data from spreadsheets, databases, using Apache Arrow and importing hierarchical data as well as web scraping."
  },
  {
    "objectID": "600-reading-list.html#edav-reading",
    "href": "600-reading-list.html#edav-reading",
    "title": "Appendix A — Reading List",
    "section": "\nA.3 Data Exploration and Visualisation",
    "text": "A.3 Data Exploration and Visualisation\nCore Materials\n\n\nExploratory Data Analysis with R by Roger Peng.\n\nChapters 3 and 4 are core reading, respectively introducing data frame manipulation with {dplyr} and an example workflow for exploratory data analysis. Other chapters may be useful as references.\n\n\nFlexible Imputation of Missing Data by Stef van Buuren. Sections 1.1-1.4 give a thorough introduction to missing data problems.\nReferene Materials\n\nA ggplot2 Tutorial for Beautiful Plotting in R https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/) by Cédric Scherer.\nThe {dplyr} documentation\nRStudio Data Transformation Cheat Sheet\nR for Data Science (First Edition) Chapters on Data Transformations, Exploratory Data Analysis and Relational Data.\nEquivalent sections in R for Data Science Second Edition\nMaterials of Interest\n\nWickham, H. (2010). A Layered Grammar of Graphics. Journal of Computational and Graphical Statistics.\nBetter Data Visualisations by Jonathan Schwabish\n\n\n\nData Visualization: A Practical Introduction by Kieran Healy"
  },
  {
    "objectID": "600-reading-list.html#production-reading",
    "href": "600-reading-list.html#production-reading",
    "title": "Appendix A — Reading List",
    "section": "\nA.4 Preparing for Production",
    "text": "A.4 Preparing for Production\nCore Materials\n\nThe Ethical Algorithm M Kearns and A Roth (Chapter 4)\nRibeiro et al (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier.\nReference Materials\n\nThe Docker Curriculum by Prakhar Srivastav.\nLIME package documentation on CRAN.\nInterpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar.\nDocumentation for apply(), map() and pmap()\nAdvanced R (Second Edition) by Hadley Wickham. Chapter 23 on measuring performance and Chapter 24 on improving performance.\nMaterials of Interest\n\nThe ASA Statement on \\(p\\)-values: Context, Process and Purpose\nThe Garden of Forking Paths: Why multiple comparisons can be a problem, even when there is no “Fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. A Gelman and E loken (2013)\nUnderstanding LIME tutorial by T Pedersen and M Benesty.\nAdvanced R (Second Edition) by Hadley Wickham. Chapter 25 on writing R code in C++."
  },
  {
    "objectID": "600-reading-list.html#data-science-ethics",
    "href": "600-reading-list.html#data-science-ethics",
    "title": "Appendix A — Reading List",
    "section": "\nA.5 Data Science Ethics",
    "text": "A.5 Data Science Ethics\nCore Materials\n\nThe Ethical Algorithm M Kearns and A Roth. Chapters 1 and 2 on Algorithmic Privacy and Algortihmic Fairness.\nGender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification by Joy Buolamwini and Timnit Gebru (2018). Proceedings of the 1st Conference on Fairness, Accountability and Transparency.\nRobust De-anonymization of Large Sparse Datasets by Arvind Narayanan and Vitaly Shmatikov (2008). IEEE Symposium on Security and Privacy.\nReference Materials\n\nFairness and machine learning Limitations and Opportunities by Solon Barocas, Moritz Hardt and Arvind Narayanan.\n\nProfessional Guidleines on Data Ethics from:\n\nThe American Mathematical Society\nThe European Union\nUK Government\nRoyal Statistical Society\nDutch Government\n\n\nMaterials of Interest\n\n\nAlgorithmic Fairness (2020). Pre-print of review paper by Dana Pessach and Erez Shmueli."
  }
]