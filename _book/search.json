[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Effective Data Science",
    "section": "",
    "text": "About this Course\nModel building and evaluation are are necessary but not sufficient skills for the effective practice of data science. In this module you will develop the technical and personal skills that are required to work successfully as a data scientist within an organisation.\nDuring this module you will critically explore how to:\nThis interdisciplinary course will draw from fields including statistics, computing, management science and data ethics. Each topic will be investigated through a selection of lecture videos, conference presentations and academic papers, hands-on lab exercises, along with readings on industry best-practices from recognised professional bodies."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Effective Data Science",
    "section": "Schedule",
    "text": "Schedule\nThese notes are intended for students on the course MATH70076: Data Science in the academic year 2023/24.\nAs the course is scheduled to take place over five weeks, the suggested schedule is:\n\n1st week: effective data science workflows;\n2nd week: acquiring and sharing data;\n3rd week: exploratory data analysis and visualisation;\n4th week: preparing for production;\n5th week: ethics and context of data science.\n\nA pdf version of these notes may be downloaded here. Please be aware that these are very rough and will be updated less frequently than the course webpage."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Effective Data Science",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this module students should be able to:\n\nIndependently scope and manage a data science project;\nSource data from the internet through web scraping and APIs;\nClean, explore and visualise data, justifying and documenting the decisions made;\nEvaluate the need for (and implement) approaches that are explainable, reproducible and scalable;\nAppraise the ethical implications of a data science projects, particularly the risks of compromising privacy or fairness and the potential to cause harm."
  },
  {
    "objectID": "index.html#allocation-of-study-hours",
    "href": "index.html#allocation-of-study-hours",
    "title": "Effective Data Science",
    "section": "Allocation of Study Hours",
    "text": "Allocation of Study Hours\nLectures: 10 Hours (2 hours per week)\nGroup Teaching: 5 Hours (1 hour per week)\nLab / Practical: 10 hours (2 hours per week)\nIndependent Study: 100 hours (15 hours per week + 30 hours coursework)\nDrop-In Sessions: Each week there will be a 1-hour optional drop-in session to address any questions about the course or material. This is where you can get support from the course lecturer or GTA on the topics covered each week, individually or in small groups.\nOffice Hours: Additionally, there will be an office hour each week. This is a weekly opportunity for 1-1 discussion with the course lecturer to address any individual questions, concerns or problems that you might have. These meetings can be in person or on Teams and can be academic (relating to course content or progress) or pastoral (relating to student well-being) in nature. To book a 1-1 meeting please use the link on the course blackboard page."
  },
  {
    "objectID": "index.html#assessment-structure",
    "href": "index.html#assessment-structure",
    "title": "Effective Data Science",
    "section": "Assessment Structure",
    "text": "Assessment Structure\nThe course will be assessed entirely by coursework, reflecting the practical and pragmatic nature of the course material.\nCoursework 1 (30%): To be completed during the fourth week of the course.\nCoursework 2 (70%): To be released in the last week of the course and submitted following the examination period in Summer term."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Effective Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese notes were created by Dr Zak Varty. They were inspired by a previous lecture series by Dr Purvasha Chakravarti at Imperial College London and draw from many resource that were made available by the R community."
  },
  {
    "objectID": "100-workflows-introduction.html",
    "href": "100-workflows-introduction.html",
    "title": "Effective Workflows",
    "section": "",
    "text": "Note\n\n\n\nEffective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nAs a data scientist you will never work alone.\nWithin a single project a data scientist is likely that you will interact with a range of other people, including but not limited to: one or more project managers, stakeholders and subject matter experts. These experts might come from a single specialism or form a multidisciplinary team, depending on the type of work that you are doing.\nTo get your project put into use and working at scale you will likely have to collaborate with data engineers. You will also work closely with other data scientists, to review one another’s work or to collaborate on larger projects.\nFamiliarity with the skills, processes and practices that make for collaboration is instrumental to being a successful as a data scientist. The aim for this part of the course is to provide you with a structure on how you organise and perform your work, so that you can be a good collaborator to current colleges and your future self.\nThis is going to require a bit more effort upfront, but the benefits will compound over time. You will get more done by wasting less time staring quizzically at messy folders of indecipherable code. You will also gain a reputation of someone who is good to work with. This promotes better professional relationships and greater levels of trust, which can in turn lead to working on more exciting and impactful projects."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#what-are-we-trying-to-do",
    "href": "101-workflows-organising-your-work.html#what-are-we-trying-to-do",
    "title": "1  Organising your work",
    "section": "1.1 What are we trying to do?",
    "text": "1.1 What are we trying to do?\nFirst, let’s consider why we want to provide our data science projects with some sense of structure and organization.\nAs a data scientist you’ll never work alone. Within a single project you’ll interact with a whole range of other people. This might be a project manager, one or more business stakeholders or a variety of subject matter experts. These experts might be trained as sociologists, chemists, or civil servants depending on the exact type of data science work that you’re doing.\nTo then get your project put into use and working at scale you’ll have to collaborate with data engineers. You’ll also likely work closely with other data scientists. For smaller projects this might be to act as reviewers for one another’s work. For larger projects working collaboratively will allow you to tackle larger challenges. These are the sorts of project that wouldn’t be feasible alone, because of the inherent limitations on the time and skill of any one individual person.\nEven if you work in a small organization, where you’re the only data scientist, then adopting a way of working that’s focused on collaborating will pay dividends over time. This is because when you inevitably return to the project that you’re working on in several weeks or months or years into the future you’ll have forgotten almost everything of what you did the first time around. You’ll also have forgotten why you made the decisions that you did and what other potential options there were that you didn’t take.\nThis is exactly like working with a current colleague who has shoddy or poor working practices. Nobody wants to be that colleague to somebody else, let alone to their future self. Even when working alone, treating your future self as a current collaborator (and one that you want to get along well with) makes you a kind colleague and a pleasure to work with.\nThe aim of this week is to provide you with a guiding structure on how you organize and perform your work. None of this is going to be particularly difficult or onerous. However it will require a bit more effort up front and daily discipline. Like with flossing, the daily effort required is not large but the benefits will compound over time.\nYou’ll get more done by wasting less time staring quizzically at a mess of folders and indecipherable code. You’ll also get a reputation as someone who’s well organized and good to work with. This promotes better professional relationships and greater levels of trust within your team. These can then, in turn, tead to you working on more exciting and more impactful projects in the future."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#an-r-focused-approach",
    "href": "101-workflows-organising-your-work.html#an-r-focused-approach",
    "title": "1  Organising your work",
    "section": "1.2 An R Focused Approach",
    "text": "1.2 An R Focused Approach\nThe structures and workflows that are recommend here and throughout the rest of this module are focused strongly on a workflow that predominantly uses R, markdown and LaTeX.\nSimilar techniques, code and software can achieve the same results that I show you here when coding in Python or C, or when writing up projects in Quarto or some other markup language. Similarly, different organizations have their own variations on these best practices that we’ll go through together. Often organisations will have extensive guidance on these topics.\nThe important thing is that once you understand what good habits are and have build them in one programming language or business, then transferring these skills to a new setting is largely a matter of learning some new vocabulary or slightly different syntax.\nWith that said, let’s get going!"
  },
  {
    "objectID": "101-workflows-organising-your-work.html#one-project-one-directory",
    "href": "101-workflows-organising-your-work.html#one-project-one-directory",
    "title": "1  Organising your work",
    "section": "1.3 One Project = One Directory",
    "text": "1.3 One Project = One Directory\nIf there’s one thing you should take away from this chapter, it’s this one Golden Rule:\n\nEvery individual project you work on as a data scientist should be in a single, self-contained directory or folder.\n\nThis is worth repeating. Every single project that you work on should be self-contained and live in a single directory. An analogy here might be having a separate ring-binder folder for each of your modules on a degree program.\n\n\n\n\n\n\n\n\n\nThis one golden rule is deceptively simple.\nThe first issue here is that it requires a predetermined scope of what is and what isn’t going to be covered by this particular project. This seems straightforward but at the outset of the project you often do not know exactly where your project will go, or how it will link to other pieces of work within your organization.\nThe second issue is the second law of Thermodynamics, which applies equally well to project management as it does to the heatdeath of the universe. It takes continual external effort to prevent the contents of this one folder from becoming chaotic and disordered over time.\nThat being said, having a single directory has several benefits which more than justify this additional work."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#properties-of-a-well-orgainsed-project",
    "href": "101-workflows-organising-your-work.html#properties-of-a-well-orgainsed-project",
    "title": "1  Organising your work",
    "section": "1.4 Properties of a Well-Orgainsed Project",
    "text": "1.4 Properties of a Well-Orgainsed Project\nWhat are the properties that we would like this single, well-organized project to have? Ideally, we’d like to organize our projects so that I have the following properties:\n\nPortable\nVersion Control Friendly\nReproducible\nIDE friendly.\n\nDon’t worry if you haven’t heard of some of these terms already. We’re going to look at each of them in a little bit of detail.\n\n1.4.1 Portability\nA project is said to be portable if it can be easily moved without breaking.\n\n\n\n\n\n\n\n\n\nThis might be a small move, like relocating the directory to a different location on your own computer. It might also mean a moderate move, say to another machine if yours dies just before a big deadline. Alternatively, it might be a large shift - to be uses by another person who is using a different operating system.\nFrom this thought experiment you can see that there’s a full spectrum of how portable a project may or may not need to be.\n\n\n1.4.2 Version Control Friendly\nA project under Version Control has all changes tracked either manually or automatically. This means that snapshots of the project are taken regularly as it gradually develops and evolves over time. Having these snapshots as many, incremental changes are made to the project allow it to be rolled back to a specific previous state if something goes wrong.\nA version controlled pattern of working helps to avoid the horrendous state that we have all found ourselves in - renaming final_version.doc to final_final_version.doc and so on.\nBy organising your workflow around incremental changes helps you to acknowledge that no work is ever finally complete. There will always be small changes that need to be done in the future.\n\n\n1.4.3 Reproducibility\n\nA study is reproducible if you can take the original data and the computer code used to analyze the data and recreate all of the numerical findings from the study. \nBroman et al (2017). “Recommendations to Funding Agencies for Supporting Reproducible Research”\n\nIn their paper, Broman et al define reproducibility as a project where you can take the original data and code used to perform the analysis and using these we create all of the numerical findings of the study.\nThis definition leads naturally to several follow-up questions.\nWho exactly is you in this definition? Does it specifically mean yourself in the future or should someone else with access to all that data and code be able to recreate your findings too? Also, should this reproducibility be limited to just the numerical results? Or should they also be able to create the associated figures, reports and press releases?\nAnother important question is when this project needs to be reproduced. Will it be in a few weeks time or in 10 years time? Do you need to protect your project from changes in dependencies, like new versions of packages or modules? How about different versions of R or Python? Taking this time scale out even further, what about different operating systems and hardware?\nIt’s unlikely you’d consider someone handing you a floppy disk of code that only runs on Windows XP to be acceptably reproducible. Sure, you could probably find a way to get it to work, but that would be an awful lot of effort on your end.\nThat’s perhaps a bit of an extreme example, but it emphasizes the importance of clearly defining the level of reproducibility that you’re aiming for within every project you work on. This example also highlights the amount of work that can be required to reproduce an analysis, especially after quite some time. It is important to explicitly think about how we dividing that effort between ourselves as the original developer and the person trying to reproduce the analysis in the future.\n\n\n1.4.4 IDE Friendly\nOur final desirable property is that we’d like our projects to play nicely with integrated development environments.\nWhen you’re coding document and writing your data science projects it’d be possible for you to work entirely in either a plain text editor or typing code directly at the command line. While these approaches to a data science workflow have the benefit of simplicity, they also expect a great deal from you as a data scientist.\nThese workflows expect that you should type everything perfectly accurately every time, that you recall the names and argument orders of every function you use, and that you are constantly aware of the current state of all objects within your working environment.\nIntegrated Development Environments (IDEs) are applications that help to reduce this burden, helping make you a more effective programmer and data scientist. IDEs offer tools like code completion and highlighting to make your code easier to read and to write. They offer tools for debugging, to fix where things are going wrong, and they also offer environment panes so that you don’t have to hold everything in your head all at once. Many IDEs also often have templating facilities. These let you save and reuse snippets of code so that you can avoid typing out repetitive, boilerplate code and introducing errors in the process.\nEven if you haven’t heard of IDEs before, you’ve likely already used one. Some common examples might be RStudio for R-users, PyCharm for python users, or Visual Studio as a more language agnostic coding environment.\nWhichever of these we use, we’d like our project to play nicely with them. This lets us reap their benefits while keeping our project portable, version controlled, and reproducible for someone working with a different set-up."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#project-structure",
    "href": "101-workflows-organising-your-work.html#project-structure",
    "title": "1  Organising your work",
    "section": "1.5 Project Structure",
    "text": "1.5 Project Structure\nI’ve given a pretty exhaustive argument for why having a single directory for each project is a good idea. Let’s now take a look inside that directory and define a common starting layout for the content of all of your projects.\nHaving this sort of project directory template will mean that you’ll always know where to find what you’re looking for and other members of your team will too. Again, before we start I’ll reiterate that we’re taking an opinionated approach here and providing a sensible starting point for organizing many projects.\nEvery project is going to be slightly different and some might require slight alterations to what I suggest here. Indeed, even if you start as I suggest then you might have to adapt your project structure as it develops and grows. I think it’s helpful to consider yourself as a tailor when making these changes. I’m providing you with a one size fits all design, that’s great for lots of projects but perfect for none of them. It’s your job to alter and refine this design for each individual case.\nOne final caveat before we get started: companies and businesses will many times have a house style how to write and organize your code or projects. If that’s the case, then follow the style guide that your business or company uses. The most important thing here is to be consistent at both an individual level and across the entire data science team. It’s this consistency that reaps the benefits.\nOkay, so imagine now that you’ve been assigned a shiny new project and have created a single directory in which to house that project. Here we’ve, quite imaginatively, called that directory exciting-new-project. What do we populate this folder with?\n\n\n\n\n\n\n\n\n\nIn the rest of this video, I’ll define the house-style for organizing the root directory of your data science projects in this module.\n\n\n\n\n\n\n\n\n\nWithin the project directory there will be some subdirectories, which you can tell a folders in this file structure because they have a forward slash following their names. There will also be some files directly in the root directory. One of these is called readme.md and the another called either makefile or make.r. We’re going to explore each of these files and directories in turn.\n\n1.5.1 README.md\n\n\n\n\n\n\n\n\n\nLet’s begin with the readme file. This gives a brief introduction to your project and gives information on what the project aims to do. The readme file should describe how to get started using the project and how to contribute to its development.\nThe readme is written either in a plain text format so readme.txt or in markdown format readme.md. The benefit of using markdown is that it allows some light formatting such as sections headers and lists using plain text characters. Here you can see me doing that by using hashes to mark out first and second level headers and using bullet points for a unnumbered list. Whichever format you use, the readme file for your project is always stored in the root directory and is typically named in all uppercase letters.\nThe readme file should be the first thing that someone who’s new to your project reads. By placing the readme in the root directory and capitalising the file name you are increase the visibility of this file and increase the chances of this actually happening.\nAn additional benefit to keeping the readme in the root directory of your project is that code hosting services like GitHub, GitLab or BitBucket will display the contents of that readme file next to the contents of your project. Those services will also nicely format any markdown that you use for you in your readme file.\nWhen writing the readme, it can be useful to imaginge that you are writing this for a new, junior team member. The readme file should let them get started with the project and make some simple contributions after reading only that file. It might also link out to more detailed project documentation that will help the new team member toward a more advanced understanding or complex contribution.\n\n\n1.5.2 Inside the README\nlet’s take a quick aside to see in more detail what should be covered within a readme file.\nA readme we should include the name of the project, which should be self-explanatory (so nothing like my generic choice of exciting-new-project). The readme should also give the project status, which is just a couple of sentences to say whether your project is still under development, the version oft the current release or, on the other end of the project life-cycle, if the project is being deprecated or closed.\nFollowing this, we should also include a description of your project. This will state the purpose of your work and to provide, or link to, any additional context or references that visitors aren’t assumed to be familiar with.\nIf your project involves code or depends on other packages then you should give some instruction on how to install those dependencies and run your code. This might just be text but it could also include things like screenshots, code snippets, gifs or a video of the whole process.\nIt’s also a good practice to include some simple examples of how to use the code within your project an the expected results, so that new users can confirm that everything is working on their local instance. Keep the examples as simple and minimal as you can so that new users\nFor longer or more complicated examples that aren’t necessary in this short introductory document you can add links to those in the readme and explain them in detail elsewhere.\nThere should ideally be a short description of how people can report issues with the project and also how people can get started in resolving those issues or extend the project in some way.\nThat leads me on to one point that I’ve forgotten to list here. There there should be a section listing the authors of the work and the license in which under which it’s distributed. This is to give credit to all the people who’ve contributed to your project and the license file then says how other people may use your work. The license declares how other may use your project and whether they have to give direct attribution to your work in any modifications that they use.\n\n\n1.5.3 data\n\n\n\n\n\n\n\n\n\nMoving back to our project structure, next we have the data directory.\nThe data directory will have two subdirectories one called raw and one called derived. All data that is not generate as part of your project is stored in the raw subdirectory. To ensure that a project is reproducible, data in the Raw folder should never be edited or modified.\n\n\n\n\n\n\n\n\n\nIn this example we’ve got two different data types: an Excel spreadsheet the XLS file and a JSON file. These files are exacty as we received them from our project stakeholder.\nThe text file metadata.txt is a plain text file explaining the contents and interpretation of each of the raw data sets. This metadata should include descriptions of all the measured variables, the units that are recorded in, the date the file was created or acquired, and the source from which it was obtained.\nThe raw data likely isn’t going to be in a form that’s amenable to analyzing straight away. To get the data into a more pleasant form to work, it will require some data manipulation and cleaning. Any manipulation or cleaning that is applied should be well documented and the resulting cleaned files saved within the derived data directory.\n\n\n\n\n\n\n\n\n\nIn our exciting new project, we can see the clean versions of the previous data sets which are ready for modelling. There’s also a third file in this folder. This is data that we’ve acquired for ourselves through web scraping, using a script within the project.\n\n\n1.5.4 src\n\n\n\n\n\n\n\n\n\nThe src or source directory contains all the source code for your project. This will typically be the functions that you’ve written to make the analysis or modelling code more accessible.\nHere we’ve saved each function in its own R script and, in this project, we’ve used subdirectories to organise these by their use case. We’ve got two functions used in data cleaning: the first replaces NA values with a given value, the second replaces these by the mean of all non-missing values.\nWe also have three helper functions: the first two calculate rolling mean and the geometric mean of a given vector, the third is a function that scrapes the web data we saw in the derived data subdirectory.\n\n\n1.5.5 tests\n\n\n\n\n\n\n\n\n\nMoving on then to the tests directory. The structure of this directory mirrors that of the source directory. Each function file has its own counterpart file of tests.\nThese test files provide example sets of inputs and the expected outputs for each function. The test files are used to check edge cases of a function or to assure yourself that you haven’t broken anything while fixing some small bug or adding new capabilities to that function.\n\n\n1.5.6 analyses\n\n\n\n\n\n\n\n\n\nThe analyses directory contains what you probably think of as the bulk of your data science work. It’s going to have one subdirectory for each major analysis that’s performed within your project and within each of these there might be a series of steps that we collect into separate scripts.\nThe activity performed at each step is made clear by the name of each script, as is the order in which we’re going to perform these steps. Here we can see the scripts used for the 2021 annual report. First is a script used to take the raw monthly receipts and produce the cleaned version of the same data set that we saw earlier. This is followed by a trend analysis of this cleaned data set.\nSimilarly for the spending review we have a data cleaning step, followed by some forecast modelling and finally the production of some diagnostic plots to compare these forecasts.\n\n\n1.5.7 outputs\n\n\n\n\n\n\n\n\n\nThe outputs directory has again one subdirectory for each meta-analysis within the project. These are then further organized by the output type whether that be some data, a figure, or a table.\nDepending on the nature of your project, you might want to use a modified subdirectory structure here. For example, if you’re doing several numerical experiments then you might want to arrange your outputs by experiment, rather than by output type.\n\n\n1.5.8 reports\n\n\n\n\n\n\n\n\n\nThe reports directory is then where everything comes together. This is where the written documents that form the final deliverables of your project are created. If these final documents are written in LaTeX or markdown, both the source and the compiled documents can be found within this directory.\nWhen including content in this report, for example figures, I’d recommend against making copies of those figure files within the reports directory. If you do that, then you’ll have to manually update the files every time you modify them. Instead you can use relative file paths to include these figures. Relative file paths specify how to get to the image, starting from your TeX document and moving up and down through the levels of your project directory.\nIf you’re not using markdown or LaTeX to write your reports, but instead use an online platform like overleaf as a latex editor or Google docs to write collaboratively then links to them in the reports directory using additional readme files. Make sure you set the read and write permissions for those links appropriately, too.\nWhen using these online writing systems, you’ll have to manually upload and update your plots whenever you modify any of your earlier analysis. That’s one of the drawbacks of these online tools that has to be traded off against their ease of use.\nIn our exciting new project, here we can see that the annual report is written in a markdown format, which is compiled to both HTML and PDF. The spendiing review is written in LaTeX and we only have the source for it, we don’t have the compiled pdf version of the document.\n\n\n1.5.9 make file\n\n\n\n\n\n\n\n\n\nThe final element of our template project structure is a make file. We aren’t going to cover how to read or write make files in this course. Instead, I’ll give you a brief description of what they are and what it is supposed to do.\nAt a high level, the make file is just a text file. What makes it special is what it contains. Similar to a shell or a bash script, make file contains code that could be run at the command line. This code will create or update each element of your project.\nThe make file defines shorthand commands for the full lines of code that create each element of your project. The make file also records the order in which these operations have to happen, and which of these steps are dependent on one another. This means that if one step part of your project is updated then any changes will be propagated through your entire project. This is done in quite a clever way so the only part of your projects that are re-run are those that need to be updated.\nWe’re omitting make files from this course not because they’re fiendishly difficult to write or read, but rather because they require a reasonable foundation in working at the command line to be understood. What I suggest you do instead throughout this course is to create your own R or markdown file called make. This file will define the intended running order and dependencies of your project and if it is an R file, it might also automate some parts of your analysis."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#wrapping-up",
    "href": "101-workflows-organising-your-work.html#wrapping-up",
    "title": "1  Organising your work",
    "section": "1.6 Wrapping up",
    "text": "1.6 Wrapping up\nWrapping up then, that’s everything for this chapter.\nI’ve introduced a project structure that will serve you well as a baseline for the vast majority of projects in data science.\n\n\n\n\n\n\n\n\n\nIn your own work, remember that the key here is standardisation. Working consistently across projects, a company or a group is more important than sticking rigidly to the particular structure that I have defined here.\nThere are two notable exceptions where you probably don’t want to use this project structure. That’s when you’re building an app or you’re building a package. These require specific organisation of the files within your project directory. We’ll explore the project structure used for package development during the live session this week."
  },
  {
    "objectID": "101-workflows-organising-your-work.html#session-information",
    "href": "101-workflows-organising-your-work.html#session-information",
    "title": "1  Organising your work",
    "section": "1.7 Session Information",
    "text": "1.7 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), digest(v.0.6.33), jsonlite(v.1.8.7), evaluate(v.0.21), rlang(v.1.1.1), cli(v.3.6.1), renv(v.0.16.0), rstudioapi(v.0.15.0), rmarkdown(v.2.23), tools(v.4.2.2), pander(v.0.6.5), htmlwidgets(v.1.6.2), xfun(v.0.39), yaml(v.2.3.7), fastmap(v.1.1.1), compiler(v.4.2.2), htmltools(v.0.5.5) and knitr(v.1.43)"
  },
  {
    "objectID": "102-workflows-naming-files.html#introduction",
    "href": "102-workflows-naming-files.html#introduction",
    "title": "2  Naming Files",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\nPhil Karlton, Netscape Developer\n\nWhen working on a data science project we can in principle name directories, files, functions and other objects whatever we like. In reality though, using an ad-hoc system of naming is likely to cause confusion, headaches and mistakes. We obviously want to avoid all of those things, in the spirit of being kind to our current colleges and also to our future selves.\nComing up with good names is an art form. Like most art, naming things is an activity that you get better at with practice. Another similarity is that the best naming systems don’t come from giving data scientists free reign over their naming system. Like all art, the best approaches to naming things give you strong guidelines and boundaries within which to express your creativity and skill.\nIn this lecture we’ll explore what these boundaries and what we want them to achieve for us. The content of this lecture is based largely around a talk of the same name given by Jennifer Bryan and the tidyverse style guide, which forms the basis of Google’s style guide for R programming."
  },
  {
    "objectID": "102-workflows-naming-files.html#naming-files",
    "href": "102-workflows-naming-files.html#naming-files",
    "title": "2  Naming Files",
    "section": "2.2 Naming Files",
    "text": "2.2 Naming Files\nWe’ll be begin by focusing in on what we call our files. That is, we’ll first focus on the part of the file name that comes before the dot. In the second part of this video, we’ll then cycle back around to discuss file extensions.\n\n2.2.1 What do we want from our file names?\nBefore we dive into naming files, we should first consider what we want from the file names that we choose. There are three key properties that that we would like to satisfy.\n\nMachine Readable\nHuman Readable\nOrder Friendly\n\nThee first desirable property is for file names to be easily readable by computers, the second is for the file names to be easily readable by humans and finally the file names should take advantage of the default ordering imposed on our files.\nThis set of current file names is sorely lacking across all of these properties:\nabstract.docx\nEffective Data Science's module guide 2022.docx \nfig 12.png\nRplot7.png\n1711.05189.pdf\nHR Protocols 2015 FINAL (Nov 2015).pdf\nWe want to provide naming conventions to move us toward the better file names listed below.\n2015-10-22_human-resources-protocols.pdf\n2022_effective-data-science-module-guide.docx\n2022_RSS-conference-abstract.docx \nfig12_earthquake-timeseries.png \nfig07_earthquake-location-map.png\nogata_1984_spacetime-clustering.pdf\nLet’s take a few minutes to examine what exactly we mean by each of these properties.\n\n\n2.2.2 Machine Readable\nWhat do we mean by machine readable file names?\n\nEasy to compute on by deliberate use of delimiters:\n\nunderscores_separate_metadata, hyphens-separate-words.\n\nPlay nicely with regular expressions and globbing:\n\navoid spaces, punctuation, accents, cases;\nrm Rplot*.png\n\n\nMachine readable names are useful when:\n\nmanaging files: ordering, finding, moving, deleting:\nextracting information directly from file names,\nworking programmatically with file names and regex.\n\nWhen we are operating on a large number of files it is useful to be able to work with them programmatically.\nOne example of where this might be useful is when downloading assessments for marking. This might require me to unzip a large number of zip files, copying the pdf report from each unzipped folder into a single directory and all of the R scripts from each unzipped folder into another directory. The marked scripts and code then need to be paired back up in folders named by student, and re-zipped ready to be returned.\nThis is monotonously dull and might work for ~50 students but not for ~5000. Working programmatically with files is the way to get this job done efficiently. This requires the file names to play nicely with the way that computers interpret file names, which they regard as a string of characters.\nIt is often helpful to have some meta-data included in the file name, for example the student’s id number and the assessment title. We will use an underscore to separate elements of meta-data within the file name and a hyphen to separate sub-elements of meta-data, for example words within the assessment title.\nRegular expressions and globbing are two ideas from string manipulation that you may not have met, but which will inform our naming conventions. Regular expressions allow you to search for strings (in our case file names) that match a particular pattern. Regular expressions can do really complicated searches but become gnarly when you have to worry about special characters like spaces, punctuation, accents and cases, so these should be avoided in file names.\nA special type of regular expression is called globbing where a star is used to replace any number of subsequent characters in a file name, so that here we can delete all png images that begin with Rplot using a single line of code. Globbing becomes particular powerful when you use a consistent structure to create your file names.\nAs in the assessment marking example, having machine readable file names is particularly useful when managing files, such as ordering, finding, moving or deleting them. Another example of this is when your analysis requires you to load a large number of individual data files.\nMachine readable file names are also useful for extracting meta-information from files without having to open them in memory. This is particularly useful when the files might be too large to load into memory, or you only want to load data from a certain year.\nThe final benefit we list here is the scalability, reduction in drudgery and lowered risk for human error when operating on a very large number of files.\n\n\n2.2.3 Order Friendly\nThe next property we will focus on also links to how computers operate. We’d like our file names to exploit the default orderings used by computers. This means starting file names with character strings or metadata that allow us order our files in some meaningful way.\n\n2.2.3.1 Running Order\nOne example of this is where there’s some logical order in which your code should be executed, as in the example analysis below.\ndiagnositc-plots.R\ndownload.R\nruntime-comparison.R\n...\nmodel-evaluation.R\nwrangle.R\nAdding numbers to the start of these file names can make the intended ordering immediately obvious.\n00_download.R\n01_wrangle.R\n02_model.R\n...\n09_model-evaluation.R\n10_model-comparison-plots.R\nStarting single digit numbers with a leading 0 is a very good idea here to prevent script 1 being sorted in with the tens, script 2 in with the twenties and so on. If you might have over 100 files, for example when saving the output from many simulations, use two or more zeros to maintain this nice ordering.\n\n\n2.2.3.2 Date Order\nA second example of orderable file names is when the file has a date associated with it. This might be a version of a report or the date on which some data were recorded, cleaned or updated.\n2015-10-22_human-resources-protocols.pdf\n...\n2022-effective-data-science-module-guide.docx\nWhen using dates, in file names or elsewhere, you should conform to the ISO standard date format.\n\nISO 8601 sets an international standard format for dates: YYYY-MM-DD.\n\nThis format uses four numbers for the year, followed by two numbers for the month and two numbers of the day of the month. This structure mirrors a nested file structure moving from least to most specific. It also avoids confusion over the ordering of the date elements. Without using the ISO standard a date like 04-05-22 might be interpreted as the fourth of May 2022, the fifth of April 2022, or the twenty-second of May 2004.\n\n\n\n2.2.4 Human Readable\nThe final property we would like our file names to have is human readability. This requires the names of our files to be meaningful, informative and easily read by real people.\nThe first two of these are handled by including appropriate metadata in the file name. The ease with which these are read by real people is determined by the length of the file name and by how that name is formatted.\nThere are lots of formatting options with fun names like camelCase, PascalCase, and snake_case.\n   easilyReadByRealPeople (camelCase)\n   EasilyReadByRealPeople (PascalCase)\n   easily_read_by_real_people (snake_case)\n   easily-read-by-real-people (skewer-case)\nThere is weak evidence to suggest that snake case and skewer case are most the readable. We’ll use a mixture of these two, using snake case between metadata items and skewer case within them. This has a slight cost to legibility, in a trade-off against making computing on these file names easier.\nThe final aspect that you have control over is the length of the name. Having short, evocative and useful file names is not easy and is a skill in itself. For some hints and tips you might want to look into tips for writing URL slugs. These are last part of a web address that are intended to improve accessibility by being immediately and intuitively meaningful to any user.\n\n\n2.2.5 Naming Files - Summary\n\nFile names should be meaningful, informative and scripts end in .r\nStick to letters, numbers underscores (_) and hyphens (-).\nPay attention to capitalisation file.r \\(\\neq\\) File.r on all operating systems.\nShow order with left-padded numbers or ISO dates."
  },
  {
    "objectID": "102-workflows-naming-files.html#file-extensions-and-where-you-work",
    "href": "102-workflows-naming-files.html#file-extensions-and-where-you-work",
    "title": "2  Naming Files",
    "section": "2.3 File Extensions and Where You Work",
    "text": "2.3 File Extensions and Where You Work\nSo far we have focused entirely on what comes before the dot, that is the file name.Equally, if not more, important is what comes after the dot, the file extension.\nexample-script.r\nexample-script.py\n\nproject-writeup.doc\nproject-writeup.tex\nThe file extension describes how information is stored in that file and determines the software that can use, view or run that file.\nYou likely already use file extensions to distinguish between code scripts, written documents, images, and notebook files. We’ll now explore the benefits and drawbacks of various file types with respect to several important features.\n\n2.3.1 Open Source vs Proprietary File Types\nThe first feature we’ll consider is whether the file type is open source, and can be used by anyone without charge, or if specialist software must be paid for in order to interact with those files.\n\n\n\n\n\n\n\n\n\nIn the figure above, each column represents a different class of file, moving left to right we have example file types for tabular data, list-like data and text documents. File types closer to the top are open source while those lower down rely on proprietary software, which may or may not require payment.\nTo make sure that our work is accessible to as many people as possible we should favour the open source options like csv files over Google sheets or excel, JSON files over Matlab data files, and tex or markdown over a word or Google doc.\nThis usually has a benefit in terms of project longevity and scalability. The open source file types are often somewhat simpler in structure, making them more robust to changes over time less memory intensive.\nTo see this, let’s take a look inside some data files.\n\n\n2.3.2 Inside Data Files\n\n2.3.2.1 Inside a CSV file\nCSV or comma separated value files are used to store tabular data.\nIn tabular data, each row of the data represents one record and each column represents a data value.\nA csv encodes this by having each record on a separate line and using commas to separate values with that record. You can see this by opening a csv file in a text editor such as notepad.\nThe raw data stores line breaks using \\n and indicates new rows by \\r. These backslashed indicae that these are escape characters with special meanings, and should not be literally interpreted as the letters n and r.\n\n#&gt; [1] \"Name,Number\\r\\nA,1\\r\\nB,2\\r\\nC,3\"\n\nWhen viewed in a text editor, the example file would look something like this.\nName,Number \nA,1\nB,2\nC,3\n\n\n2.3.2.2 Inside a TSV file\nTSV or tab separated value files are also used to store tabular data.\nLike in a csv each record is given on a new line but in a tsv tabs rather than commas are used to separate values with each record. This can also be seen by opening a tsv file in a text editor such as notepad.\n\n#&gt; [1] \"Name\\tNumber\\r\\nA\\t1\\r\\nB\\t2\\r\\nC\\t3\"\n\nName    Number \nA   1\nB   2\nC   3\nOne thing to note is that tabs are a separate character and are not just multiple spaces. In plain text these can be impossible to tell apart, so most text editors have an option to display tabs differently from repeated spaces, though this is usually not enabled by default.\n\n\n2.3.2.3 Inside an Excel file\nWhen you open an excel file in a text editor, you will immediately see that this is not a human interpretable file format.\n504b 0304 1400 0600 0800 0000 2100 62ee\n9d68 5e01 0000 9004 0000 1300 0802 5b43\n6f6e 7465 6e74 5f54 7970 6573 5d2e 786d\n6c20 a204 0228 a000 0200 0000 0000 0000\n0000 0000 0000 0000 0000 0000 0000 0000\n.... .... .... .... .... .... .... ....\n0000 0000 0000 0000 ac92 4d4f c330 0c86\nef48 fc87 c8f7 d5dd 9010 424b 7741 48bb\n2154 7e80 49dc 0fb5 8da3 241b ddbf 271c\n1054 1a83 0347 7fbd 7efc cadb dd3c 8dea\n.... .... .... .... .... .... .... ....\nEach entry here is a four digit hexadecimal number and there are a lot more of them than we have entries in our small table.\nThis is because excel files can carry a lot of additional information that a csv or tsv are not able to, for example cell formatting or having multiple tables (called sheets by excel) stored within a single file.\nThis means that excel files take up much more memory because they are carrying a lot more information than is strictly contained within the data itself.\n\n\n2.3.2.4 Indise a JSON file\nJSON, or Java Script Object Notation, files are an open source format for list-like data. Each record is represented by a collection of key:value pairs. In our example table each entry has two fields, one corresponding to the Name key and one corresponding to the Number key.\n[{\n    \"Name\": \"A\",\n    \"Number\": \"1\"\n}, {\n    \"Name\": \"B\",\n    \"Number\": \"2\"\n}, {\n    \"Name\": \"C\",\n    \"Number\": \"3\"\n}]\nThis list-like structure allows non-tabular data to be stored by using a property called nesting: the value taken by a key can be a single value, a vector of values or another list-like object.\nThis ability to create nested data structures has lead to this data format being used widely in a range of applications that require data transfer.\n\n\n2.3.2.5 Inside an XML file\nXML files are another open source format for list-like data, where each record is represented by a collection of key:value pairs.\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;root&gt;\n  &lt;row&gt;\n    &lt;Name&gt;A&lt;/Name&gt;\n    &lt;Number&gt;1&lt;/Number&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;Name&gt;B&lt;/Name&gt;\n    &lt;Number&gt;2&lt;/Number&gt;\n  &lt;/row&gt;\n  &lt;row&gt;\n    &lt;Name&gt;C&lt;/Name&gt;\n    &lt;Number&gt;3&lt;/Number&gt;\n  &lt;/row&gt;\n&lt;/root&gt;\nThe difference from a JSON file is mainly in how those records are formatted within the file. In a JSON file this is designed to look like objects in the Java Script programming language and in XML the formatting is done to look like html, the markup language used to write websites.\n\n\n\n2.3.3 A Note on Notebooks\n\nThere are two and a half notebook formats that you are likely to use: .rmd, .ipynb or alternatively .qmd.\nR markdown documents .rmd are plain text files, so are very human friendly.\nJuPyteR notebooks have multi-language support but are not so human friendly (JSON in disguise).\nQuarto documents offer the best of both worlds and more extensive language support. Not yet as established as a format.\n\nIn addition to the files you read and write, the files that you code in will largely determine your workflow.\nThere are three main options for the way that you code: first is typing it directly at the command line, second is using a text editor or IDE to write scripts and third is writing a notebook that mixes code, text and output together in a single document.\nWe’ll compare these methods of working soon, but first let’s do a quick review of what notebooks are available to you and why you might want to use them.\nAs a data scientist, there are two and a half notebook formats that you’re likely to have met before. The first two are Rmarkdown files for those working predominantly in R and interactive Python or jupyter notebooks for those working predominantly in Python. The final half format are quarto markdown documents, which are relatively new and extend the functionality of Rmarkdown files to provide multi-language support.\nThe main benefit of R markdown documents is that they’re plain text files, so they’re very human friendly and work very well with version control software like git. JuPyteR notebooks have the benefit of supporting code written in Julia, Python or R, but are not so human friendly - under the hood these documents are JSON files that should not be edited directly (because a misplaced bracket will break them!).\nQuarto documents offer the best of both worlds, with plain text formatting and even more extensive language support than jupyter notebooks. Quarto is a recent extension of Rmarkdown, which is rapidly becoming popular in the data science community. Quarto also allows you to create a wider range of documents, including websites, these course notes and the associated slides.\nEach format has its benefits and drawbacks depending on the context in which they are used and all have some shared benefits and limitations by nature of them all being notebook documents.\n\n\n2.3.4 File Extensions and Where You Code\n\n\n\nProperty\nNotebook\nScript\nCommand Line\n\n\n\n\nreproducible\n~\n✓\nX\n\n\nreadable\n~\n✓\n~\n\n\nself-documenting\n✓\nX\nX\n\n\nin production\nX\n✓\n~\n\n\nordering / automation\n~\n✓\n~\n\n\n\nThe main benefit of notebook documents is that they are self-documenting, in that they can mix the documentation, code and report all into a single document. Notebooks also provide a level of interactivity when coding that is not possible when working directly at the command line or using a text editor to write scripts. This limitation is easily overcome by using an integrated development environment when scripting, rather than a plain text editor.\nWriting code in .r files is not self-documenting but this separation of code, documentation and outputs has many other benefits. Firstly, the resulting scripts provide a reproducible and automatable workflow, unlike one-off lines of code being run at the command line. Secondly, using an IDE to write these provides you with syntax highlighting and code linting features to help you write readable and accurate code. Finally, the separation of code from documentation and output allows your work to be more easily or even directly put into production.\nIn this course we will advocate for a scripting-first approach to data science, though notebooks and command line work definitely have their place.\nNotebooks are great as teaching and rapid development tools but have strong limitations with being put into production. Conversely, coding directly at the command line is perfect for simple one-time tasks but it leaves no trace of your workflow and leads to an analysis that cannot be easily replicated in the future.\n\n\n2.3.5 Summary\nFinally, let’s wrap things up by summarising what we have learned about naming files.\nBefore the dot we want to pick file names that machine readable, human friendly and play nicely with the default orderings provided to us.\n\nName files so that they are:\n\nMachine Readable,\nHuman Readable,\nOrder Friendly.\n\n\nAfter the dot, we want to pick file types that are widely accessible, easily read by humans and allow for our entire analysis to be reproduced.\n\nUse document types that are:\n\nWidely accessible,\nEasy to read and reproduce,\nAppropriate for the task at hand.\n\n\nAbove all we want to name our files and pick our file types to best match with the team we are working in and the task that is at hand."
  },
  {
    "objectID": "102-workflows-naming-files.html#session-information",
    "href": "102-workflows-naming-files.html#session-information",
    "title": "2  Naming Files",
    "section": "2.4 Session Information",
    "text": "2.4 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: readr(v.2.1.4)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), magrittr(v.2.0.3), hms(v.1.1.3), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), fansi(v.1.0.4), tools(v.4.2.2), xfun(v.0.39), utf8(v.1.2.3), cli(v.3.6.1), htmltools(v.0.5.5), yaml(v.2.3.7), digest(v.0.6.33), tibble(v.3.2.1), lifecycle(v.1.0.3), tzdb(v.0.4.0), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), pander(v.0.6.5), compiler(v.4.2.2), pillar(v.1.9.0), jsonlite(v.1.8.7), renv(v.0.16.0) and pkgconfig(v.2.0.3)"
  },
  {
    "objectID": "103-workflows-organising-your-code.html#introduction",
    "href": "103-workflows-organising-your-code.html#introduction",
    "title": "3  Code",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nWe have already described how we might organise an effective data science project at the directory and file level. In this chapter we will delve one step deeper and consider how we can structure our work within those files. In particular, we’ll focus on code files here.\nWe’ll start by comparing the two main approaches to structuring our code, namely functional programming and object oriented programming. We’ll then see how we should order code within our scripts and conventions on how to name the functions and objects that we work with in our code.\nRounding up this chapter, we’ll summarise the main points from the R style guide that we will be following in this course and highlight some useful packages for writing effective code."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#functional-programming",
    "href": "103-workflows-organising-your-code.html#functional-programming",
    "title": "3  Code",
    "section": "3.2 Functional Programming",
    "text": "3.2 Functional Programming\nA functional programming style has two major properties:\n\nObject immutability,\nComplex programs written using function composition.\n\nThis first point here states that the original data or objects should never be modified or altered by the code we write. We have met this idea before when making new, cleaner versions of our raw data but taking care to leave the original messy data intact. Object immutability is the exact same idea but in a code context rather than data context.\nSecondly, in functional programming, complex problems are solved by decomposing them into a series of smaller problems. A separate, self-contained function is then written to solve each sub-problem. Each individual function is, in itself, simple and easy to understand. This makes these small functions easy to test and easy to reuse in many places. Code complexity is then built up by composing these functions in various ways.\nIt can be difficult to get into this way of thinking, but people with mathematical training often find it quite natural. This is because mathematicians have many years of experience in working with function compositions in the abstract, mathematical sense.\n\\[y = g(x) = f_3 \\circ f_2 \\circ f_1(x).\\]\n\n3.2.1 The Pipe Operator\nOne issue with functional programming is that lots of nested functions means that there are also lots of nested brackets. These start to get tricky to keep track of when you have upwards of 3 functions being composed. This reading difficulty is only exacerbated if your functions have additional arguments on top of the original inputs.\n\nlog(exp(cos(sin(pi))))\n#&gt; [1] 1\n\nThe pipe operator %&gt;% from the {magrittr} package helps with this issue. It works exactly like function composition: it takes the whatever is on the left (whether that is an existing object or the output of a function) and passes it to the following function call as the first argument of that function.\n\nlibrary(magrittr)\npi %&gt;% \n  sin() %&gt;% \n  cos() %&gt;% \n  exp() %&gt;% \n  log()\n#&gt; [1] 1\n\n\niris %&gt;% \n  head(n = 3)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n\nThe pipe operator is often referred to as “syntactic sugar”. This is because it doesn’t add anything to your code in itself, but rather it makes your code so much more palatable to read.\nIn R versions 4.1 and greater, there’s a built-in version of this pipe operator,|&gt;. This is written using the vertical bar symbol followed by a greater than sign. To type the vertical bar, you can usually find it found above backslash on the keyboard. (Just to cause confusion, the vertical bar symbol is also called the pipe symbol and performs a similar operation in general programming contexts.)\n\nlibrary(magrittr)\npi |&gt; \n  sin() |&gt; \n  cos() |&gt; \n  exp() |&gt; \n  log()\n#&gt; [1] 1\n\n\niris |&gt; \n  head(n = 3)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n\nThe base R pipe usually behaves in the same way as the pipe from magrittr, but there are a few cases where they differ. For reasons of back-compatibility and consistency we’ll stick to the {magrittr} pipe in this course.\n\n\n3.2.2 When not to pipe\n\n\n\nPipes are designed to put focus on the the actions you are performing rather than the object that you are preforming those operations on. This means that there are two cases where you should almost certainly not use a pipe.\nThe first of these is when you need to manipulate more than one object at a time. Using secondary objects as reference points (but leaving them unchanged) is of course perfectly fine, but pipes should be used when applying a sequence of steps to create a new, modified version of one primary object.\nSecondly, just because you can chain together many actions into a single pipeline, that doesn’t mean that you necessarily should. Very long sequences of piped operations are easier to read than nested functions, however they still burden the reader with the same cognitive load on their short term memory. Be kind and create meaningful, intermediate objects with informative names. This will help the reader to more easily understand the logic within your code."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#object-oriented-programming",
    "href": "103-workflows-organising-your-code.html#object-oriented-programming",
    "title": "3  Code",
    "section": "3.3 Object Oriented Programming",
    "text": "3.3 Object Oriented Programming\nThe main alternative to functional programming is object oriented programming.\n\nSolve problems by using lots of simple objects\nR has 3 OOP systems: S3, S4 and R6.\nObjects belong to a class, have methods and fields.\nExample: agent based simulation of beehive.\n\n\n3.3.1 OOP Philosophy\nIn functional programming, we solve complicated problems by using lots of simple functions. In object oriented programming we solve complicated problems using lots of simple objects. Which of these programming approaches is best will depend on the particular type of problem that you are trying to solve.\nFunctional programming is excellent for most types of data science work. Object oriented comes into its own when your problem has many small components interacting with one another. This makes it great for things like designing agent-based simulations, which I’ll come back to in a moment.\nIn R there are three different systems for doing object oriented programming (called S3, S4, and R6), so things can get a bit complicated. We won’t go into detail about them here, but I’ll give you an overview of the main ideas.\nThis approach to programming might be useful for you in the future, for example if you want to extend base R functions to work with new types of input, and to have user-friendly displays. In that case (Advanced R)[https://adv-r.hadley.nz/] by Hadley Wickham is an excellent reference text.\nIn OOP, each object belongs to a class and has a set of methods associated with it. The class defines what an object is and methods describe what that object can do. On top of that, each object has class-specific attributes or data fields. These fields are shared by all objects in a class but the values that they take give information about that specific object.\n\n\n3.3.2 OOP Example\n\n\n\nThis is is all sounding very abstract. Let’s consider writing some object oriented code to simulate a beehive. Each object will be a bee, and each bee is an instance of one of three bee classes: it might be a queen, a worker or a drone for example. Different bee classes have different methods associated with them, which describe what the bee can do, for example all bees would have 6 methods that let them move up, down, left, right, forward and backward within the hive. An additional “reproduce” method might only be defined for queen bees and a pollinate method might only be defined for workers. Each instance of a bee has its own fields, which give data about that specific bee. All bees have x, y and z coordinate fields giving their location within the hive. The queen class might have an additional field for their number of offspring and the workers might have an additional field for how much pollen they are carrying.\nAs the simulation progresses, methods are applied to each object altering their fields and potentially creating or destroying objects. This is very different from the preservation mindset of functional programming, but hopefully you can see that it is a very natural approach to many types of problem."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#structuring-r-script-headers",
    "href": "103-workflows-organising-your-code.html#structuring-r-script-headers",
    "title": "3  Code",
    "section": "3.4 Structuring R Script Headers",
    "text": "3.4 Structuring R Script Headers\n\nTL;DR\n\nStart script with a comment of 1-2 sentences explaining what it &gt; does.\nsetwd() and rm(ls()) are the devil’s work.\n“Session” &gt; “Restart R” or Keyboard shortcut: crtl/cmd + shift &gt; + 0\nPolite to gather all library() and source() calls.\nRude to mess with other people’s set up using &gt; install.packages().\nPortable scripts use paths relative to the root directory of the project.\n\n\nFirst things first, let’s discuss what should be at the top of your R scripts.\nIt is almost always a good idea to start your file with a few commented out sentences describing the purpose of the script and, if you work in a large team, perhaps who contact with any questions about this script. (There is more on comments coming up soon, don’t worry!)\nIt is also good practise to move all library() and source() calls to the top of your script. These indicate the packages and helper function that are dependencies of your script; it’s useful to know what you need to have installed before trying to run any code.\nThat segues nicely to the next point, which is never to hard code package installations. It is extremely bad practise and very rude to do so because then your script might alter another person’s R installation. If you don’t know already, this is precisely the difference between an install.packages() and library() call: install.packages() will download the code for that package to the users computer, while library() takes that downloaded code and makes it available in the current R session. To avoid messing with anyone’s R installation, you should always type install.package() commands directly in the console and then place the corresponding library() calls within your scripts.\nNext, it is likely that you, or someone close to you, will commit the felony of starting every script by setting the working directory and clearing R’s global environment. This is very bad practice, it’s indicative of a workflow that’s not project based and it’s problematic for at least two reasons. Firstly, the path you set will likely not work on anyone else’s computer. Secondly, clearing the environment like this may look like it gets you back to fresh, new R session but all of your previously loaded packages will still be loaded and lurking in the background.\nInstead, to achieve your original aim of starting a new R session, go to the menu and select the “Session” drop down then select “Restart R”. Alternatively, you can use keyboard shortcuts to do the same. This is “crtl + shift + 0” on Windows and “cmd + shift + 0” on a mac. The fact that a keyboard shortcut exists for this should quite strongly hint that, in a reproducible and project oriented workflow, you should be restarting R quite often in an average working day. This is the scripting equivalent of “clear all output and rerun all” in a notebook.\nFinally, let’s circle back to the point I made earlier about setting the working directory. The reason that this will not work is because you are likely giving file paths that are specific to your computer, your operating system and your file organisation system. The chances of someone else having all of these the same are practically zero."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#portable-file-paths-with-here",
    "href": "103-workflows-organising-your-code.html#portable-file-paths-with-here",
    "title": "3  Code",
    "section": "3.5 Portable File paths with {here}",
    "text": "3.5 Portable File paths with {here}\n\n# Bad - breaks if project moved\nsource(\"zaks-mbp/Desktop/exciting-new-project/src/helper_functions/rolling_mean.R\")\n\n# Better - breaks if Windows\nsource(\"../../src/helper_functions/rolling_mean.R\")\n\n# Best - but use here:here() to check root directory correctly identified\nsource(here::here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n# For more info on the here package:\nvignette(\"here\")\n\nTo fix the problem of person- and computer-specific file paths you can have two options.\nThe first is to use relative file paths. In this you assume that each R script is being run in its current location and my moving up and down through the levels of your project directory you point to the file that you need.\nThis is good in that it solves the problem of paths breaking because you move the project to a different location on your own laptop. However, it does not fully solve the portability problem because you might move your file to a different location within the same project. It also does not solve the problem that windows uses MacOS and linux use forward slashes in file paths with widows uses backslashes.\nTo resolve these final two issues I recommend using the here() function from the {here} package. This package looks for a .Rproj or .git file to identify the root directory of your project and creates file paths relative to the root of your project, that are suitable for the operating system the code is being run on.\nIt really is quite marvellous. For more information on how to use the here package, explore its chapter in R - What They Forgot, R for Data Science or this project oriented workflow blog post."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#code-body",
    "href": "103-workflows-organising-your-code.html#code-body",
    "title": "3  Code",
    "section": "3.6 Code Body",
    "text": "3.6 Code Body\nMoving on now, we will go from the head to the body of the code. Having well named and organised code will facilitate both reading and understanding. Comments and sectioning do the rest of this work.\nThis section is designed as an introduction to the tidyverse style guide and not as a replacement to it. ### Comments\n\n# This is an example script showing good use of comments and sectioning \n\nlibrary(here)\nsource(here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n#===============================================================================  &lt;- 80 characters max for readability\n# Major Section on Comments ----\n#===============================================================================\n\n#-------------------------------------------------------------------------------\n##  Minor Section on inline comments ---- \n#-------------------------------------------------------------------------------\nx &lt;- 1:10 # this is an inline comment\n\n#-------------------------------------------------------------------------------\n##  Minor Section on full line comments ---- \n#-------------------------------------------------------------------------------\nrolling_mean(x)\n# This is an full line comment\n\nComments may be either short in-line comments at the end of a line or full lines dedicated to comments. To create either type of comment in R, simply type hash followed by one space. The rest of that line will not be evaluated and will function as a comment. If multi-line comments are needed simply start multiple lines with a hash and a space.\nThe purpose of these comments is to explain the why of what you are doing, not the what. If you are explaining what you are doing in most of your comments then you perhaps need to consider writing more informative function names, something we will return to in the general advice section.\nComments can also be used to add structure to your code, buy using commented lines of hyphens and equal signs to chunk your files into minor and major sections.\nMarkdown-like section titles can be added to these section and subsection headers. Many IDEs, such as RStudio, will interpret these as a table of contents for you, so that you can more easily navigate your code.\n\n3.6.1 Objects are Nouns\n\nObject names should use only lowercase letters, numbers, and _.\nUse underscores (_) to separate words within a name. (snake_case)\nUse nouns, preferring singular over plural names.\n\n\n# Good\nday_one\nday_1\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1\n\nWhen creating and naming objects a strong guideline in that objects should be named using short but meaningful nouns. Names should not include any special characters and should use underscores to separate words within the object name.\nThis is similar to our file naming guide, but note that hyphens can’t be used in object names because this conflicts with the subtraction operator.\nWhen naming objects, as far as possible use singular nouns. The main reason for this is that the plurisation rules in English are complex and will eventually trip up either you or a user of your code.\n\n\n3.6.2 Functions are Verbs\n\nFunction names should use only lower-case letters, numbers, and _.\nUse underscores (_) to separate words within a name. (snake_case)\nSuggest imperative mood, as in a recipe.\nBreak long functions over multiple lines. 4 vs 2 spaces.\n\n\n# Good\nadd_row()\npermute()\n\n# Bad\nrow_adder()\npermutation()\n\nlong_function_name &lt;- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\nThe guidelines for naming functions are broadly similar, with the advice that functions should be verbs rather than nouns.\nFunctions should be named in the imperative mood, like in a recipe. This is again for consistency; having function names in a range of moods and tenses leads to coding nightmares.\nAs with object names you should aim to give your functions and their arguments short, evocative names. For functions with many arguments or a long name, you might not be able to fit the function definition on a single line. In this case you can should place each argument on its own double indented line and the function body on a single indented line.\n\n\n3.6.3 Casing Consistantly\nAs we have mentioned already, we have many options for separating words within names:\n\nCamelCase\npascalCase\nsnakecase\nunderscore_separated ❤️\nhyphen-separated\npoint.separated 💀\n\nFor people used to working in Python it is tempting to use point separation in function names, in the spirit of methods from object oriented programming. Indeed, some base R functions even use this convention.\nHowever, the reason that we advise against it is because it is already used for methods in some of R’s inbuilt OOP functionality. We will use underscore separation in our work.\n\n\n3.6.4 Style Guide Summary\n\nUse comments to structure your code\nObjects = Nouns\nFunctions = Verbs\nUse snake case and consistant grammar"
  },
  {
    "objectID": "103-workflows-organising-your-code.html#further-tips-for-friendly-coding",
    "href": "103-workflows-organising-your-code.html#further-tips-for-friendly-coding",
    "title": "3  Code",
    "section": "3.7 Further Tips for Friendly Coding",
    "text": "3.7 Further Tips for Friendly Coding\nIn addition to naming conventions the style guide gives lots of other guidance on writing code in a way that is kind to future readers of that code.\nI’m not going to go repeat all of that guidance here, but the motivation for all of these can be boiled down into the following points.\n\nWrite your code to be easily understood by humans.\nUse informative names, typing is cheap.\n\n\n# Bad\nfor (i in dmt) {\n  print(i)\n}\n\n# Good\nfor (temperature in daily_max_temperature) {\n  print(temperature)\n}\n\n\nDivide your work into logical stages, human memory is expensive.\n\nWhen writing your code, keep that future reader in mind. This means using names that are informative and reasonably short, it also means adding white space, comments and formatting to aid comprehension. Adding this sort of structure to your code also helps to reduce the cognitive burden that you are placing on the human reading your code.\nInformative names are more important than short names. This is particularly true when using flow controls, which are things like for loops and while loops. Which of these for loops would you like to encounter when approaching a deadline or urgently fixing a bug? Almost surely the second one, where context is immediately clear.\nA computer doesn’t care if you call a variable by only a single letter, by a random key smash (like aksnbioawb) or by an informative name. A computer also doesn’t care if you include no white space your code - the script will still run. However, doing these things are friendly practices that can help yourself when debugging and your co-workers when collaborating."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#reduce-reuse-recycle",
    "href": "103-workflows-organising-your-code.html#reduce-reuse-recycle",
    "title": "3  Code",
    "section": "3.8 Reduce, Reuse, Recycle",
    "text": "3.8 Reduce, Reuse, Recycle\nIn this final section, we’ll look at how you can make your workflow more efficient by reducing the amount of code you write, as well as reusing and recycling code that you’ve already written.\n\n3.8.1 DRY Coding\nThis idea of making your workflow more efficient by reducing, reusing and recycling your code is summarised by the DRY acronym: don’t repeat yourself.\nThis can be boiled down to three main points:\n\n\nif you do something twice in a single script, then write a function to do that thing;\nif you want to use your function elsewhere within your project, then save it in a separate script;\nIf you want to use your function across projects, then add it to a package.\n\n\nOf course, like with scoping projects in the first place, this requires some level of clairvoyance: you have to be able to look into the future and see whether you’ll use a function in another script or project. This is difficult, bordering on impossible. So in practice, this is done retrospectively - you find a second script or project that needs a function then pull it out its own separate file or include it in a package.\nAs a rule of thumb, if you are having to consider whether or not to make the function more widely available then you should do it. It takes much less effort to do this work now, while it’s fresh in your mind, than to have to re-familiarise yourself with the code in several years time.\nLet’s now look at how to implement those sub-bullet points: “when you write a function, document it” and “when you write a function, test it”.\n\n\n3.8.2 Rememer how to use your own code\nWhen you come to use a function written by somebody else, you likely have to refer to their documentation to teach or to remind yourself of things like what the expected inputs are and how exactly the method is implemented.\nWhen writing your own functions you should create documentation that fills the same need. Even if the function is just for personal use, over time you’ll forget exactly how it works.\n\nWhen you write a function, document it.\n\nBut what should that documentation contain?\n\nInputs\nOutputs\nExample use cases\nAuthor (if not obvious or working in a team)\n\nYour documentation should describe the inputs and outputs of your function, some simple example uses. If you are working in a large team, the documentation should also indicate who wrote the function and who’s responsible for maintaining it over time.\n\n\n3.8.3 {roxygen2} for documentation\nIn the same way that we used the {here} package to simplify our file path problems, we’ll use the {roxygen2} package to simplify our testing workflow.\nThe {roxygen2} package gives us an easily insert-able temple for documenting our functions. This means we don’t have to waste our time and energy typing out and remembering boilerplate code. It also puts our documentation in a format that allows us to get hints and auto-completion for our own functions, just like the functions we use from packages that are written by other people.\nTo use Roxygen, you only need to install it once - it doesn’t need to be loaded with a library call at the top of your script. After you’ve done this, and with your cursor inside a function definition, you can then insert skeleton code to document that function in one of two ways: you can either use the Rstudio menu or the keyboard short cut for your operating system.\n\ninstall.packages(\"roxygen2\")\nWith cursor inside function: Code &gt; Insert Roxygen Skeleton\nKeyboard shortcut: cmd + option + shift + r or crtl + option + shift + r\nFill out relevant fields\n\n\n\n3.8.4 An {roxygen2} example\nBelow, we’ve got an example of an Roxygen skeleton to document a function that calculates the geometric mean of a vector. Here, the hash followed by an apostrophe is a special type of comment. It indicates that this is function documentation rather than just a regular comment.\n\n#' Title\n#'\n#' @param x \n#' @param remove_NA \n#'\n#' @return\n#' @export\n#'\n#' @examples\ngeometric_mean &lt;- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n\nWe’ll fill in all of the fields in this skeleton apart from export, which we’ll remove. If we put this function in a R package, then the export field makes it available to users of that package. Since this is just a standalone function we won’t need the export field, though keeping it wouldn’t actually cause us any problems either.\n\n#' Calculate the geometric mean of a numeric vector\n#'\n#' @param x numeric vector\n#' @param remove_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. \n#'\n#' @return the geometric mean of the values in `x`, a numeric scalar value. \n#'\n#' @examples\n#' geometric_mean(x = 1:10)\n#' geometric_mean(x = c(1:10, NA), remove_NA = TRUE)\n#' \ngeometric_mean &lt;- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n\nOnce we have filled in the skeleton documentation it might look something like this. We have described what the function does, what the expected inputs are and what the user can expect as an output. We’ve also given an few simple examples of how the function can be used.\nFor more on Roxygen, see the package documentation or the chapter of R packages on function documentation.\n\n\n3.8.5 Checking Your Code\n\nIf you write a function, test it.\n\nTesting code has two main purposes:\n\nTo warn or prevent user misuse (e.g. strange inputs),\nTo catch edge cases.\n\nOn top of explaining how our functions should work, we really ought to check that they do work. This is the job of unit testing.\nWhenever you write a function you should test that it works as you intended it to. Additionally, you should test that your function is robust to being misused by the user. Depending on the context, this might be accidental or malicious misuse. Finally, you should check that the function behaves properly for strange, but still valid, inputs. These are known as edge cases.\nTesting can be a bit of a brutal process, you’ve just created a beautiful function and now you’re job is to do your best to break it!\n\n\n3.8.6 An Informal Testing Workflow\n\nWrite a function\nExperiment with the function in the console, try to break it\nFix the break and repeat.\n\nProblems: Time consuming and not reproducible.\nAn informal approach to testing your code might be to first write a function and then play around with it in the console to check that it behaves well when you give it obvious inputs, edge cases and deliberately wrong inputs. Each time you manage to break the function, you edit it to fix the problem and then start the process all over again.\nThis is testing the code, but only informally. There’s no record of how you have tried to break your code already. The problem with this approach is that when you return to this code to add a new feature, you’ll probably have forgotten at least one of the informal tests you ran the first time around. This goes against our efforts towards reproducibility and automation. It also makes it very easy to break code that used to work just fine.\n\n\n3.8.7 A Formal Testing Workflow\nWe can formalise this testing workflow by writing our tests in their own R script and saving them for future reference. Remember from the first lecture that these should be saved in the tests/ directory, the structure of which should mirror that of the src/ directory for your project. All of the tests for one function should live in a single file, which is named after that function.\nOne way of writing these tests is to use lots of if statements. The {testthat} can do some of that syntactic heavy lifting for us. It has lots of helpful functions to test that the output of your function is what you expect.\n\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1, NA), remove_NA = FALSE),\n  expected = NA)\n\n# Error: geometric_mean(x = c(1, NA), remove_NA = FALSE) not equal to NA.\n# Types not compatible: double is not logical\n\nIn this example, we have an error because our function returns a logical NA rather than a double NA. Yes, R really does have different types of NA for different types of missing data, it usually just handles these nicely in the background for you.\nThis subtle difference is probably not something that you would have spotted on your own, until it caused you trouble much further down the line. This rigorous approach is one of the benefits of of using the {testthat} functions.\nTo fix this test we change out expected output to NA_real_.\nWe’ll revisit the {testthat} package in the live session this week, when we will learn how to use it to test functions within our own packages."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#summary",
    "href": "103-workflows-organising-your-code.html#summary",
    "title": "3  Code",
    "section": "3.9 Summary",
    "text": "3.9 Summary\n\nFunctional and Object Oriented Programming\nStructuring your scripts\nStyling your code\nReduce, reuse, recycle\nDocumenting and testing\n\nLet’s wrap up by summarising what we have learned in this chapter.\nWe started out with a discussion on the differences between functional and object oriented programming. While R is capable of both, data science work tends to have more of a functional flavour to it.\nWe’ve then described how to structure your scripts and style your code to make it as human-friendly and easy to debug as possible.\nFinally, we discussed how to write DRY code that is well documented and tested."
  },
  {
    "objectID": "103-workflows-organising-your-code.html#session-information",
    "href": "103-workflows-organising-your-code.html#session-information",
    "title": "3  Code",
    "section": "3.10 Session Information",
    "text": "3.10 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: magrittr(v.2.0.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), pkgload(v.1.3.2.1), timechange(v.0.2.0), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), stringr(v.1.5.0), tools(v.4.2.2), xfun(v.0.39), cli(v.3.6.1), htmltools(v.0.5.5), rprojroot(v.2.0.3), yaml(v.2.3.7), digest(v.0.6.33), assertthat(v.0.2.1), lifecycle(v.1.0.3), crayon(v.1.5.2), brio(v.1.1.3), purrr(v.1.0.1), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), testthat(v.3.1.10), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), emo(v.0.0.0.9000), stringi(v.1.7.12), pander(v.0.6.5), compiler(v.4.2.2), desc(v.1.4.2), generics(v.0.1.3), jsonlite(v.1.8.7), lubridate(v.1.9.2) and renv(v.0.16.0)"
  },
  {
    "objectID": "110-workflows-checklist.html#videos-chapters",
    "href": "110-workflows-checklist.html#videos-chapters",
    "title": "Workflows Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nOrganising your work (30 min) [slides]\nNaming Files (20 min) [slides]\nOrganising your code (27 min) [slides]"
  },
  {
    "objectID": "110-workflows-checklist.html#reading",
    "href": "110-workflows-checklist.html#reading",
    "title": "Workflows Checklist",
    "section": "Reading",
    "text": "Reading\nUse the workflows section of the reading list to support and guide your exploration of this week’s materials. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "110-workflows-checklist.html#tasks",
    "href": "110-workflows-checklist.html#tasks",
    "title": "Workflows Checklist",
    "section": "Tasks",
    "text": "Tasks\nCore:\n\nFind 3 data science projects on Github and explore how they organise their work. Write a post on the EdStem forum that links to all three, and in a couple of paragraphs describe the content and structure of one project.\nCreate your own project directory (or directories) for this course and its assignments.\nWrite two of your own R functions. The first should calculate the geometric mean of a numeric vector. The second should calculate the rolling arithmetic mean of a numeric vector.\n\nBonus:\n\nRe-factor an old project to match the project organisation and coding guides for this course. This might be a small research project, class notes or a collection of homework assignments. Use an R-based project if possible. If you only have python projects, then either translate these to R or apply the PEP8 style guide. Take care to select a suitably sized project so that this is a meaningful exercise but does not take more than a few hours.\nIf you are able to do so, host your re-factored project publicly and share it with the rest of the class on the EdStem Discussion forum."
  },
  {
    "objectID": "110-workflows-checklist.html#live-session",
    "href": "110-workflows-checklist.html#live-session",
    "title": "Workflows Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then create a minimal R package to organise and test the functions you have written.\nPlease come to the live session prepared to discuss the following points:\n\nDid you make the assignment projects as subdirectories or as their stand alone projects? Why?\nWhat were some terms that you had not met before during the readings? How did you find their meanings?\nWhat did you have to consider when writing your rolling mean function?"
  },
  {
    "objectID": "200-data-introduction.html",
    "href": "200-data-introduction.html",
    "title": "Acquiring and Sharing Data",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\nNote\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nData can be difficult to acquire and gnarly when you get it.\nThe raw material that you work with as a data scientist is, unsurprisingly, data. In this part of the course we will focus on the different ways in which data can be stored, distributed and obtained.\nBeing able to obtain and read a dataset is often a surprisingly large hurdle in getting a new data science project off the ground. The skill of being able to source and read data from many locations is usually sanitised during a statistics programme: you’re given a ready-to-go, cleaned CSV file and all focus is placed on modelling. This week aims to remedy that by equipping you with the skills to acquire and manage your own data.\nWe will begin this week by explore different file types. This dictates what type of information you can store, who can access that information and how they read that it into R. We will then turn our attention to the case when data are not given to you directly. We will learn how to obtain data from a raw webpage and how to request data that via a service known as an API."
  },
  {
    "objectID": "201-data-tabular.html#loading-tabular-data",
    "href": "201-data-tabular.html#loading-tabular-data",
    "title": "4  Tabular Data",
    "section": "4.1 Loading Tabular Data",
    "text": "4.1 Loading Tabular Data\n\n\n\nRecall that simpler, open source formats improve accessibility and reproducibility. We will begin by reading in three open data formats for tabular data.\n\nrandom-data.csv\nrandom-data.tsv\nrandom-data.txt\n\nEach of these data sets contains 26 observations of 4 variables:\n\nid, a Roman letter identifier;\ngaussian, standard normal random variates;\ngamma, gamma(1,1) random variates;\nuniform, uniform(0,1) random variates.\n\n\n4.1.1 Base R\n\nrandom_df &lt;- read.csv(file = 'random-data.csv')\nprint(random_df)\n#&gt;    id    gaussian      gamma    uniform\n#&gt; 1   a -1.20706575 0.98899970 0.22484576\n#&gt; 2   b  0.27742924 0.03813386 0.08498474\n#&gt; 3   c  1.08444118 1.09462335 0.63729826\n#&gt; 4   d -2.34569770 1.49301101 0.43101637\n#&gt; 5   e  0.42912469 5.40361248 0.07271609\n#&gt; 6   f  0.50605589 1.72386539 0.80240202\n#&gt; 7   g -0.57473996 1.95357133 0.32527830\n#&gt; 8   h -0.54663186 0.07807803 0.75728904\n#&gt; 9   i -0.56445200 0.21198194 0.58427152\n#&gt; 10  j -0.89003783 0.20803673 0.70883941\n#&gt; 11  k -0.47719270 2.08607862 0.42697577\n#&gt; 12  l -0.99838644 0.49463708 0.34357270\n#&gt; 13  m -0.77625389 0.77171305 0.75911999\n#&gt; 14  n  0.06445882 0.37216648 0.42403021\n#&gt; 15  o  0.95949406 1.88207991 0.56088725\n#&gt; 16  p -0.11028549 0.76622568 0.11613577\n#&gt; 17  q -0.51100951 0.50488585 0.30302180\n#&gt; 18  r -0.91119542 0.22979791 0.47880269\n#&gt; 19  s -0.83717168 0.75637275 0.34483055\n#&gt; 20  t  2.41583518 0.62435969 0.60071414\n#&gt; 21  u  0.13408822 0.64638373 0.07608332\n#&gt; 22  v -0.49068590 0.11247545 0.95599261\n#&gt; 23  w -0.44054787 0.11924307 0.02220682\n#&gt; 24  x  0.45958944 4.91805535 0.84171063\n#&gt; 25  y -0.69372025 0.60282666 0.63244245\n#&gt; 26  z -1.44820491 0.64446571 0.31009417\n\nOutput is a data.frame object. (List of vectors with some nice methods)\n\n\n4.1.2 {readr}\n\nrandom_tbl &lt;- readr::read_csv(file = 'random-data.csv')\n#&gt; Rows: 26 Columns: 4\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (1): id\n#&gt; dbl (3): gaussian, gamma, uniform\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nprint(random_tbl)\n#&gt; # A tibble: 26 × 4\n#&gt;   id    gaussian  gamma uniform\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 a       -1.21  0.989   0.225 \n#&gt; 2 b        0.277 0.0381  0.0850\n#&gt; 3 c        1.08  1.09    0.637 \n#&gt; 4 d       -2.35  1.49    0.431 \n#&gt; 5 e        0.429 5.40    0.0727\n#&gt; 6 f        0.506 1.72    0.802 \n#&gt; # ℹ 20 more rows\n\nOutput is a tibble object. (List of vectors with some nicer methods)\n\n4.1.2.1 Benefits of readr::read_csv()\n\nIncreased speed (approx. 10x) and progress bar.\nStrings are not coerced to factors. No more stringsAsFactors = FALSE\nNo row names and nice column names.\nReproducibility bonus: does not depend on operating system.\n\n\n\n\n4.1.3 WTF: Tibbles\n\n4.1.3.1 Printing\n\nDefault to first 10 rows and as many columns as will comfortably fit on your screen.\nCan adjust this behaviour in the print call:\n\n\n# print first three rows and all columns\nprint(random_tbl, n = 3, width = Inf)\n#&gt; # A tibble: 26 × 4\n#&gt;   id    gaussian  gamma uniform\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 a       -1.21  0.989   0.225 \n#&gt; 2 b        0.277 0.0381  0.0850\n#&gt; 3 c        1.08  1.09    0.637 \n#&gt; # ℹ 23 more rows\n\nBonus: Colour formatting in IDE and each column tells you it’s type.\n\n\n4.1.3.2 Subsetting\nSubsetting tibbles will always return another tibble.\n\n# Row Subsetting\nrandom_tbl[1, ] # returns tibble\nrandom_df[1, ]  # returns data.frame\n\n# Column Subsetting\nrandom_tbl[ , 1]      # returns tibble\nrandom_df[ , 1]       # returns vector\n\n# Combined Subsetting\nrandom_tbl[1, 1]      # returns 1x1 tibble\nrandom_df[1, 1]       # returns single value\n\n\nThis helps to avoids edge cases associated with working on data frames.\n\n\n\n4.1.4 Other {readr} functions\nSee {readr} documentation, there are lots of useful additional arguments that can help you when reading messy data.\nFunctions for reading and writing other types of tabular data work analogously.\n\n4.1.4.1 Reading Tabular Data\n\nlibrary(readr)\nread_tsv(\"random-data.tsv\")\nread_delim(\"random-data.txt\", delim = \" \")\n\n\n\n4.1.4.2 Writing Tabular Data\n\nwrite_csv(random_tbl, \"random-data-2.csv\")\nwrite_tsv(random_tbl, \"random-data-2.tsv\")\nwrite_delim(random_tbl, \"random-data-2.tsv\", delim = \" \")\n\n\n\n\n4.1.5 Need for Speed\nSome times you have to load lots of large data sets, in which case a 10x speed-up might not be sufficient.\nIf each data set still fits inside RAM, then check out data.table::fread() which is optimised for speed. (Alternatives exist for optimal memory usage and data too large for working memory, but not covered here.)\nNote: While it can be much faster, the resulting data.table object lacks the consistancy properties of a tibble so be sure to check for edge cases, where the returned value is not what you might expect."
  },
  {
    "objectID": "201-data-tabular.html#tidy-data",
    "href": "201-data-tabular.html#tidy-data",
    "title": "4  Tabular Data",
    "section": "4.2 Tidy Data",
    "text": "4.2 Tidy Data\n\n4.2.1 Wide vs. Tall Data\n\n4.2.1.1 Wide Data\n\nFirst column has unique entries\nEasier for humans to read and compute on\nHarder for machines to compute on\n\n\n\n4.2.1.2 Tall Data\n\nFirst column has repeating entries\nHarder for humans to read and compute on\nEasier for machines to compute on\n\n\n\n4.2.1.3 Examples\nExample 1 (Wide)\n\n\n\nPerson \nAge \nWeight \nHeight \n\n\n\n\nBob\n32\n168\n180\n\n\nAlice\n24\n150\n175\n\n\nSteve\n64\n144\n165\n\n\n\nExample 1 (Tall)\n\n\n\nPerson \nVariable \nValue \n\n\n\n\nBob\nAge\n32\n\n\nBob\nWeight\n168\n\n\nBob\nHeight\n180\n\n\nAlice\nAge\n24\n\n\nAlice\nWeight\n150\n\n\nAlice\nHeight\n175\n\n\nSteve\nAge\n64\n\n\nSteve\nWeight\n144\n\n\nSteve\nHeight\n165\n\n\n\n[Source: Wikipedia - Wide and narrow data]\nExample 2 (Wide)\n\n\n\nTeam\nPoints\nAssists\nRebounds\n\n\n\n\nA\n88\n12\n22\n\n\nB\n91\n17\n28\n\n\nC\n99\n24\n30\n\n\nD\n94\n28\n31\n\n\n\nExample 2 (Tall)\n\n\n\nTeam\nVariable\nValue\n\n\n\n\nA\nPoints\n88\n\n\nA\nAssists\n12\n\n\nA\nRebounds\n22\n\n\nB\nPoints\n91\n\n\nB\nAssists\n17\n\n\nB\nRebounds\n28\n\n\nC\nPoints\n99\n\n\nC\nAssists\n24\n\n\nC\nRebounds\n30\n\n\nD\nPoints\n94\n\n\nD\nAssists\n28\n\n\nD\nRebounds\n31\n\n\n\n[Source: Statology - Long vs wide data]\n\n\n4.2.1.4 Pivoting Wider and Longer\n\nError control at input and analysis is format-dependent.\nSwitching between long and wide formats useful to control errors.\nEasy with the {tidyr} package functions\n\n\ntidyr::pivot_longer()\ntidyr::pivot_wider()\n\n\n\n\n4.2.2 Tidy What?\n\n\n\n[Image: R4DS - Chapter 12]\n\n\nTidy Data is an opinionated way to store tabular data.\nImage Source: Chapter 12 of R for Data Science.\n\nEach column corresponds to a exactly one measured variable\nEach row corresponds to exactly one observational unit\nEach cell contains exactly one value.\n\nBenefits of tidy data\n\nConsistent data format: Reduces cognitive load and allows specialised tools (functions) to efficiently work with tabular data.\nVectorisation: Keeping variables as columns allows for very efficient data manipulation. (this goes back to data frames and tibbles being lists of vectors)\n\n\n\n4.2.3 Example - Tidy Longer\nConsider trying to plot these data as time series. The year variable is trapped in the column names!\n\n#&gt; # A tibble: 3 × 3\n#&gt;   country     `1999` `2000`\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan    745   2666\n#&gt; 2 Brazil       37737  80488\n#&gt; 3 China       212258 213766\n\nTo tidy this data, we need to pivot_longer(). We will turn the column names into a new year variable and retaining cell contents as a new variable called cases.\n\nlibrary(magrittr)\n\ncountries %&gt;% \n  tidyr::pivot_longer(cols = c(`1999`,`2000`), names_to = \"year\", values_to = \"cases\")\n#&gt; # A tibble: 6 × 3\n#&gt;   country     year   cases\n#&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan 1999     745\n#&gt; 2 Afghanistan 2000    2666\n#&gt; 3 Brazil      1999   37737\n#&gt; 4 Brazil      2000   80488\n#&gt; 5 China       1999  212258\n#&gt; 6 China       2000  213766\n\nMuch better!\n\n\n4.2.4 Example - Tidy Wider\nThere are other times where we might have to widen our data to tidy it.\nThis example is not tidy. Why not?\n\n\n\nTeam\nVariable\nValue\n\n\n\n\nA\nPoints\n88\n\n\nA\nAssists\n12\n\n\nA\nRebounds\n22\n\n\nB\nPoints\n91\n\n\nB\nAssists\n17\n\n\nB\nRebounds\n28\n\n\nC\nPoints\n99\n\n\nC\nAssists\n24\n\n\nC\nRebounds\n30\n\n\nD\nPoints\n94\n\n\nD\nAssists\n28\n\n\nD\nRebounds\n31\n\n\n\nThe observational unit here is a team. However, each variable should be a stored in a separate column, with cells containing their values.\nTo tidy this data we first generate it as a tibble. We use the tribble() function, which allows us to create a tibble row-wise rather than column-wise.\nWe can then tidy it by creating new columns for each value of the current Variable column and taking the values for these from the current Value column.\n\ntournament %&gt;% \n  tidyr::pivot_wider(\n    id_cols = \"Team\", \n    names_from = \"Variable\",\n    values_from = \"Value\")\n#&gt; # A tibble: 4 × 4\n#&gt;   Team  Points Assists Rebounds\n#&gt;   &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 A         88      12       22\n#&gt; 2 B         91      17       28\n#&gt; 3 C         99      24       30\n#&gt; 4 D         94      28       31\n\n\n\n4.2.5 Other helpful functions\nThe pivot_*() family of functions resolve issues with rows (too many observations per row or rows per observation).\n\nThere are similar helper functions to solve column issues:\n\nMultiple variables per column: tidyr::separate(),\nMultiple columns per variable: tidyr::unite().\n\n\n\n4.2.6 Missing Data\nIn tidy data, every cell contains a value. Including cells with missing values.\n\nMissing values are coded as NA (generic) or a type-specific NA, such as NA_character_.\nThe {readr} family of read_*() function have good defaults and helpful na argument.\nExplicitly code NA values when collecting data, avoid ambiguity: ” “, -999 or worst of all 0.\nMore on missing values in EDA videos…"
  },
  {
    "objectID": "201-data-tabular.html#wrapping-up",
    "href": "201-data-tabular.html#wrapping-up",
    "title": "4  Tabular Data",
    "section": "4.3 Wrapping Up",
    "text": "4.3 Wrapping Up\n\nReading in tabular data by a range of methods\nIntroduced the tibble and tidy data (+ tidy not always best)\nTools for tidying messy tabular data"
  },
  {
    "objectID": "201-data-tabular.html#session-information",
    "href": "201-data-tabular.html#session-information",
    "title": "4  Tabular Data",
    "section": "4.4 Session Information",
    "text": "4.4 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: magrittr(v.2.0.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), pillar(v.1.9.0), compiler(v.4.2.2), tools(v.4.2.2), digest(v.0.6.33), bit(v.4.0.5), jsonlite(v.1.8.7), evaluate(v.0.21), lifecycle(v.1.0.3), tibble(v.3.2.1), pkgconfig(v.2.0.3), rlang(v.1.1.1), cli(v.3.6.1), rstudioapi(v.0.15.0), yaml(v.2.3.7), parallel(v.4.2.2), xfun(v.0.39), fastmap(v.1.1.1), withr(v.2.5.0), dplyr(v.1.1.2), knitr(v.1.43), generics(v.0.1.3), vctrs(v.0.6.3), htmlwidgets(v.1.6.2), hms(v.1.1.3), bit64(v.4.0.5), tidyselect(v.1.2.0), glue(v.1.6.2), R6(v.2.5.1), fansi(v.1.0.4), vroom(v.1.6.3), rmarkdown(v.2.23), pander(v.0.6.5), readr(v.2.1.4), tzdb(v.0.4.0), tidyr(v.1.3.0), purrr(v.1.0.1), htmltools(v.0.5.5), renv(v.0.16.0), utf8(v.1.2.3) and crayon(v.1.5.2)"
  },
  {
    "objectID": "202-data-webscraping.html#scraping-webpage-data-using-rvest",
    "href": "202-data-webscraping.html#scraping-webpage-data-using-rvest",
    "title": "5  Web Scraping",
    "section": "5.1 Scraping webpage data using {rvest}",
    "text": "5.1 Scraping webpage data using {rvest}\n\n\n\nYou can’t always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.\nI’m not suggesting you will need to do this manually, but you will likely need to get data from the internet that’s been made publicly or privately available to you.\nThis might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.\nIn this chapter we will cover the basics of scraping webpages, following the vignette for the {rvest} package."
  },
  {
    "objectID": "202-data-webscraping.html#what-is-a-webpage",
    "href": "202-data-webscraping.html#what-is-a-webpage",
    "title": "5  Web Scraping",
    "section": "5.2 What is a webpage?",
    "text": "5.2 What is a webpage?\nBefore we can even hope to get data from a webpage, we first need to understand what a webpage is.\nWebpages are written in a similar way to LaTeX: the content and styling of webpages are handled separately and are coded using plain text files.\nIn fact, websites go one step further than LaTeX. The content and styling of websites are written in different files and in different languages. HTML (HyperText Markup Language) is used to write the content and then CSS (Cascading Style Sheets) are used to control the appearance of that content when it’s displayed to the user."
  },
  {
    "objectID": "202-data-webscraping.html#html",
    "href": "202-data-webscraping.html#html",
    "title": "5  Web Scraping",
    "section": "5.3 HTML",
    "text": "5.3 HTML\nA basic HTML page with no styling applied might look something like this:\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A level 1 heading&lt;/h1&gt;\n  &lt;p&gt;Hello World!&lt;/p&gt;\n  &lt;p&gt;Here is some plain text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\n\n5.3.1 HTML elements\n\nJust like XML data files, HTML has a hierarchical structure. This structure is crafted using HTML elements. Each HTML element is made up of of a start tag, optional attributes, an end tag.\nWe can see each of these in the first level header, where &lt;h1&gt; is the opening tag, id='first' is an additional attribute and &lt;/h1&gt; is the closing tag. Everything between the opening and closing tag are the contents of that element. There are also some special elements that consist of only a single tag and its optional attributes. An example of this is the &lt;img&gt; tag.\nSince &lt; and &gt; are used for start and end tags, you can’t write them directly in an HTML document. Instead, you have to use escape characters. This sounds fancy, but it’s just an alternative way to write characters that serve some special function within a language.\nYou can write greater than &gt; and less than as &lt;. You might notice that those escapes use an ampersand (&). This means that if you want a literal ampersand on your webpage, you have to escape too using &amp;.\nThere are a wide range of possible HTML tags and escapes. We’ll cover the most common tags in this lecture and you don’t need to worry about escapes too much because {rvest} will automatically handle them for you.\n\n\n5.3.2 Important HTML Elements\nIn all, there are in excess of 100 HTML elements. The most important ones for you to know about are:\n\nThe &lt;html&gt; element, that must enclose every HTML page. The &lt;html&gt; element must have two child elements within it. The &lt;head&gt; element contains metadata about the document, like the page title that is shown in the browser tab and the CSS style sheet that should be applied. The &lt;body&gt; element then contains all of the content that you see in the browser.\nBlock elements are used to give structure to the page. These are elements like headings, sub-headings and so on from &lt;h1&gt; all the way down to &lt;h6&gt;. This category also contains paragraph elements &lt;p&gt;, ordered lists &lt;ol&gt; unordered lists &lt;ul&gt;.\nFinally, inline tags like &lt;b&gt; for bold, &lt;i&gt; for italics, and &lt;a&gt; for hyperlinks are used to format text inside block elements.\n\nWhen you come across a tag that you’ve never seen before, you can find out what it does with just a little bit of googling. A good resource here is the MDN Web Docs which are produced by Mozilla, the company that makes the Firefox web browser. The W3schools website is another great resource for web development and coding resources more generally."
  },
  {
    "objectID": "202-data-webscraping.html#html-attributes",
    "href": "202-data-webscraping.html#html-attributes",
    "title": "5  Web Scraping",
    "section": "5.4 HTML Attributes",
    "text": "5.4 HTML Attributes\nWe’ve seen one example of a header with an additional attribute. More generally, all tags can have named attributes. These attributes are contained within the opening tag and look something like:\n&lt;tag attribute1='value1' attribute2='value2'&gt;element contents&lt;/tag&gt;\nTwo of the most important attributes are id and class. These attributes are used in conjunction with the CSS file to control the visual appearance of the page. These are often very useful to identify the elements that you are interested in when scraping data off a page."
  },
  {
    "objectID": "202-data-webscraping.html#css-selectors",
    "href": "202-data-webscraping.html#css-selectors",
    "title": "5  Web Scraping",
    "section": "5.5 CSS Selectors",
    "text": "5.5 CSS Selectors\nThe Cascading Style Sheet is used to describe how your HTML content will be displayed. To do this, CSS has it’s own system for selecting elements of a webpage, called CSS selectors.\nCSS selectors define patterns for locating the HTML elements that a particular style should be applied to. A happy side-effect of this is that they can sometimes be very useful for scraping, because they provide a concise way of describing which elements you want to extract.\nCSS Selectors can work on the level of an element type, a class, or a tag and these can be used in a nested (or cascading) way.\n\nThe p selector will select all paragraph &lt;p&gt; elements.\nThe .title selector will select all elements with class “title”.\nThe p.special selector will select all&lt;p&gt; elements with class “special”.\nThe #title selector will select the element with the id attribute “title”.\n\nWhen you want to select a single element id attributes are particularly useful because that must be unique within a html document. Unfortunately, this is only helpful if the developer added an id attribute to the element(s) you want to scrape!\nIf you want to learn more CSS selectors I recommend starting with the fun CSS diner tutorial to build a base of knowledge and then using the W3schools resources as a reference to explore more webpages in the wild."
  },
  {
    "objectID": "202-data-webscraping.html#which-attributes-and-selectors-do-you-need",
    "href": "202-data-webscraping.html#which-attributes-and-selectors-do-you-need",
    "title": "5  Web Scraping",
    "section": "5.6 Which Attributes and Selectors Do You Need?",
    "text": "5.6 Which Attributes and Selectors Do You Need?\nTo scrape data from a webpage, you first have to identify the tag and attribute combinations that you are interested in gathering.\nTo find your elements of interest, you have three options. These go from hardest to easiest but also from most to least robust.\n\nright click + “inspect page source” (F12)\nright click + “inspect”\nRvest Selector Gadget (very useful but fallible)\n\nInspecting the source of some familiar websites can be a useful way to get your head around these concepts. Beware though that sophisticated webpages can be quite intimidating. A good place to start is with simpler, static websites such as personal websites, rather than the dynamic webpages of online retailers or social media platforms."
  },
  {
    "objectID": "202-data-webscraping.html#reading-html-with-rvest",
    "href": "202-data-webscraping.html#reading-html-with-rvest",
    "title": "5  Web Scraping",
    "section": "5.7 Reading HTML with {rvest}",
    "text": "5.7 Reading HTML with {rvest}\nWith {rvest}, reading a html page can be as simple as loading in tabular data.\n\nhtml &lt;- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\n\nThe class of the resulting object is an xml_document. This type of object is from the low-level package {xml2}, which allows you to read xml files into R.\n\nclass(html)\n#&gt; [1] \"xml_document\" \"xml_node\"\n\nWe can see that this object is split into several components: first is some metadata on the type of document we have scraped, followed by the head and then the body of that html document.\n\nhtml\n#&gt; {html_document}\n#&gt; &lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;\n#&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#&gt; [2] &lt;body class=\"nav-fixed\"&gt;\\n\\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\\n   ...\n\nWe have several possible approaches to extracting information from this document."
  },
  {
    "objectID": "202-data-webscraping.html#extracting-html-elements",
    "href": "202-data-webscraping.html#extracting-html-elements",
    "title": "5  Web Scraping",
    "section": "5.8 Extracting HTML elements",
    "text": "5.8 Extracting HTML elements\nIn {rvest} you can extract a single element with html_element(), or all matching elements with html_elements(). Both functions take a document object and one or more CSS selectors as inputs.\n\nlibrary(rvest)\n\nhtml %&gt;% html_elements(\"h1\")\n#&gt; {xml_nodeset (1)}\n#&gt; [1] &lt;h1&gt;Teaching&lt;/h1&gt;\nhtml %&gt;% html_elements(\"h2\")\n#&gt; {xml_nodeset (2)}\n#&gt; [1] &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\n#&gt; [2] &lt;h2 class=\"anchored\" data-anchor-id=\"course-history\"&gt;Course History&lt;/h2&gt;\nhtml %&gt;% html_elements(\"p\")\n#&gt; {xml_nodeset (7)}\n#&gt; [1] &lt;p&gt;I am fortunate to have had the opportunity to teach in a variety of ...\n#&gt; [2] &lt;p&gt;Developing and teaching a number of modules in statistics, data sci ...\n#&gt; [3] &lt;p&gt;Supervising undergraduate, postgraduate and doctoral research proje ...\n#&gt; [4] &lt;p&gt;Adapting and leading short courses on scientific writing and commun ...\n#&gt; [5] &lt;p&gt;Running workshops and computer labs for undergraduate and postgradu ...\n#&gt; [6] &lt;p&gt;Speaking at univerisity open days and providing one-to-one tuition  ...\n#&gt; [7] &lt;p&gt;I am an associate fellow of the Higher Education Academy, which you ...\n\nYou can also combine and nest these selectors. For example you might want to extract all links that are within paragraphs and all second level headers.\n\nhtml %&gt;% html_elements(\"p a,h2\")\n#&gt; {xml_nodeset (3)}\n#&gt; [1] &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\n#&gt; [2] &lt;a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\" ...\n#&gt; [3] &lt;h2 class=\"anchored\" data-anchor-id=\"course-history\"&gt;Course History&lt;/h2&gt;"
  },
  {
    "objectID": "202-data-webscraping.html#extracting-data-from-html-elements",
    "href": "202-data-webscraping.html#extracting-data-from-html-elements",
    "title": "5  Web Scraping",
    "section": "5.9 Extracting Data From HTML Elements",
    "text": "5.9 Extracting Data From HTML Elements\nNow that we’ve got the elements we care about extracted from the complete document. But how do we get the data we need out of those elements?\nYou’ll usually get the data from either the contents of the HTML element or else from one of it’s attributes. If you’re really lucky, the data you need will already be formatted for you as a HTML table or list.\n\n5.9.1 Extracting text\nThe functions rvest::html_text() and rvest::html_text2() can be used to extract the plain text contents of an HTML element.\n\nhtml %&gt;% \n  html_elements(\"#teaching li\") %&gt;% \n  html_text2()\n#&gt; [1] \"Developing and teaching a number of modules in statistics, data science and data ethics. These were predominantly at the postgradute-level and include courses designed for in-person and remote learning.\"\n#&gt; [2] \"Supervising undergraduate, postgraduate and doctoral research projects.\"                                                                                                                                   \n#&gt; [3] \"Adapting and leading short courses on scientific writing and communication.\"                                                                                                                               \n#&gt; [4] \"Running workshops and computer labs for undergraduate and postgraduate modules.\"                                                                                                                           \n#&gt; [5] \"Speaking at univerisity open days and providing one-to-one tuition to high school students.\"\n\nThe difference between html_text() and html_text2() is in how they handle whitespace. In HTML whitespace and line breaks have very little influence over how the code is interpreted by the computer (this is similar to R but very different from Python). html_text() will extract the text as it is in the raw html, while html_text2() will do its best to extract the text in a way that gives you something similar to what you’d see in the browser.\n\n\n5.9.2 Extracting Attributes\nAttributes are also used to record information that you might like to collect. For example, the destination of links are stored in the href attribute and the source of images is stored in the src attribute.\nAs an example of this, consider trying to extract the twitter link from the icon in the page footer. This is quite tricky to locate in the html source, so I used the Selector Gadget to help find the correct combination of elements.\n\nhtml %&gt;% html_element(\".compact:nth-child(1) .nav-link\")\n#&gt; {html_node}\n#&gt; &lt;a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\"&gt;\n#&gt; [1] &lt;i class=\"bi bi-twitter\" role=\"img\"&gt;\\n&lt;/i&gt;\n\nTo extract the href attribute from the scraped element, we use the rvest::html_attr() function.\n\nhtml %&gt;% \n  html_elements(\".compact:nth-child(1) .nav-link\") %&gt;% \n  html_attr(\"href\")\n#&gt; [1] \"https://www.twitter.com/zakvarty\"\n\nNote: rvest::html_attr() will always return a character string (or list of character strings). If you are extracting an attribute that describes a quantity, such as the width of an image, you’ll need to convert this from a string to your required data type. For example, of the width is measures in pixels you might use as.integer().\n\n\n5.9.3 Extracting tables\nHTML tables are composed in a similar, nested manner to LaTeX tables.\nThere are four main elements to know about that make up an HTML table:\n\n&lt;table&gt;,\n&lt;tr&gt; (table row),\n&lt;th&gt; (table heading),\n&lt;td&gt; (table data).\n\nHere’s our simple example data, formatted as an HTML table:\n\nhtml_2 &lt;- minimal_html(\"\n  &lt;table&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Name&lt;/th&gt;\n      &lt;th&gt;Number&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;A&lt;/td&gt;\n      &lt;td&gt;1&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;B&lt;/td&gt;\n      &lt;td&gt;2&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;C&lt;/td&gt;\n      &lt;td&gt;3&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/table&gt;\n  \")\n\nSince tables are a common way to store data, {rvest} includes a useful function html_table() that converts directly from an HTML table into a tibble.\n\nhtml_2 %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table()\n#&gt; # A tibble: 3 × 2\n#&gt;   Name  Number\n#&gt;   &lt;chr&gt;  &lt;int&gt;\n#&gt; 1 A          1\n#&gt; 2 B          2\n#&gt; 3 C          3\n\nApplying this to our real scraped data we can easily extract the table of taught courses.\n\nhtml %&gt;% \n  html_element(\"table\") %&gt;% \n  html_table()\n#&gt; # A tibble: 31 × 3\n#&gt;   Year      Course                                     Role    \n#&gt;   &lt;chr&gt;     &lt;chr&gt;                                      &lt;chr&gt;   \n#&gt; 1 \"2022-23\" Data Science                               Lecturer\n#&gt; 2 \"\"        Ethics in Data Science I, II and III       Lecturer\n#&gt; 3 \"\"        Data Ethics for Digital Chemistry          Lecturer\n#&gt; 4 \"\"        Y1 research projects: point process models Lecturer\n#&gt; 5 \"2021-22\" Supervised Learning                        Lecturer\n#&gt; 6 \"\"        Ethics in Data Science I                   Lecturer\n#&gt; # ℹ 25 more rows"
  },
  {
    "objectID": "202-data-webscraping.html#tip-for-building-tibbles",
    "href": "202-data-webscraping.html#tip-for-building-tibbles",
    "title": "5  Web Scraping",
    "section": "5.10 Tip for Building Tibbles",
    "text": "5.10 Tip for Building Tibbles\nWhen scraping data from a webpage, your end-goal is typically going to be constructing a data.frame or a tibble.\nIf you are following our description of tidy data, you’ll want each row to correspond some repeated unit on the HTML page. In this case, you should\n\nUse html_elements() to select the elements that contain each observation unit;\nUse html_element() to extract the variables from each of those observations.\n\nTaking this approach guarantees that you’ll get the same number of values for each variable, because html_element() always returns the same number of outputs as inputs. This is vital when you have missing data - when not every observation unit has a value for every variable of interest.\nAs an example, consider this extract of text about the starwars dataset.\n\nstarwars_html &lt;- minimal_html(\"\n  &lt;ul&gt;\n    &lt;li&gt;&lt;b&gt;C-3PO&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;167 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R2-D2&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;96 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;Yoda&lt;/b&gt; weighs &lt;span class='weight'&gt;66 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R4-P17&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  \")\n\nThis is an unordered list where each list item corresponds to one observational unit (one character from the starwars universe). The name of the character is given in bold, the character species is specified in italics and the weight of the character is denoted by the .weight class. However, some characters have only a subset of these variables defined: for example Yoda has no species entry.\nIf we try to extract each element directly, our vectors of variable values are of different lengths. We don’t know where the missing values should be, so we can’t line them back up to make a tibble.\n\nstarwars_html %&gt;% html_elements(\"b\") %&gt;% html_text2()\n#&gt; [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %&gt;% html_elements(\"i\") %&gt;% html_text2()\n#&gt; [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %&gt;% html_elements(\".weight\") %&gt;% html_text2()\n#&gt; [1] \"167 kg\" \"96 kg\"  \"66 kg\"\n\nWhat we should do instead is start by extracting all of the list item elements using html_elements(). Once we have done this, we can then use html_element() to extract each variable for all characters. This will pad with NAs, so that we can collate them into a tibble.\n\nstarwars_characters &lt;- starwars_html %&gt;% html_elements(\"li\")\n\nstarwars_characters %&gt;% html_element(\"b\") %&gt;% html_text2()\n#&gt; [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %&gt;% html_element(\"i\") %&gt;% html_text2()\n#&gt; [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %&gt;% html_element(\".weight\") %&gt;% html_text2()\n#&gt; [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\n\n\ntibble::tibble(\n  name = starwars_characters %&gt;% html_element(\"b\") %&gt;% html_text2(),\n  species = starwars_characters %&gt;% html_element(\"i\") %&gt;% html_text2(),\n  weight = starwars_characters %&gt;% html_element(\".weight\") %&gt;% html_text2()\n)\n#&gt; # A tibble: 4 × 3\n#&gt;   name   species weight\n#&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; \n#&gt; 1 C-3PO  droid   167 kg\n#&gt; 2 R2-D2  droid   96 kg \n#&gt; 3 Yoda   &lt;NA&gt;    66 kg \n#&gt; 4 R4-P17 droid   &lt;NA&gt;"
  },
  {
    "objectID": "202-data-webscraping.html#session-information",
    "href": "202-data-webscraping.html#session-information",
    "title": "5  Web Scraping",
    "section": "5.11 Session Information",
    "text": "5.11 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: rvest(v.1.0.3)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), xml2(v.1.3.5), magrittr(v.2.0.3), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), fansi(v.1.0.4), stringr(v.1.5.0), httr(v.1.4.6), tools(v.4.2.2), xfun(v.0.39), utf8(v.1.2.3), cli(v.3.6.1), selectr(v.0.4-2), htmltools(v.0.5.5), yaml(v.2.3.7), digest(v.0.6.33), tibble(v.3.2.1), lifecycle(v.1.0.3), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), curl(v.5.0.1), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), stringi(v.1.7.12), pander(v.0.6.5), pillar(v.1.9.0), compiler(v.4.2.2), jsonlite(v.1.8.7), renv(v.0.16.0) and pkgconfig(v.2.0.3)"
  },
  {
    "objectID": "203-data-apis.html#aquiring-data-via-an-api",
    "href": "203-data-apis.html#aquiring-data-via-an-api",
    "title": "6  APIs",
    "section": "6.1 Aquiring Data Via an API",
    "text": "6.1 Aquiring Data Via an API\n\n\n\nWe’ve already established that you can’t always rely on tidy, tabular data to land on your desk.\nSometimes you are going to have to go out and gather data for yourself. We have already seen how to scrape information directly from the HTML source of a webpage. But surely there has to be an easer way. Thankfully, there often is!\nIn this chapter we will cover the basics of obtaining data via an API. This material draws together the Introduction to APIs book by Brian Cooksey and the DIY web data section of STAT545 at the University of British Columbia."
  },
  {
    "objectID": "203-data-apis.html#why-do-i-need-to-know-about-apis",
    "href": "203-data-apis.html#why-do-i-need-to-know-about-apis",
    "title": "6  APIs",
    "section": "6.2 Why do I need to know about APIs?",
    "text": "6.2 Why do I need to know about APIs?\n\nAn API, or application programming interface, is a set of rules that allows different software applications to communicate with each other.\n\nAs a data scientist, you will often need to access data that is stored on remote servers or in cloud-based services. APIs provide a convenient way for data scientists to programmatically retrieve this data, without having to manually download data sets or and process them locally on their own computer.\nThis has multiple benefits including automation and standardisation of data sharing.\n\nAutomation: It is much faster for a machine to process a data request than a human. Having a machine handling data requests also scales much better as either the number or the complexity of data requests grows. Additionally, there is a lower risk of introducing human error. For example, a human might accidentally share the wrong data, which can have serious legal repercussions.\nStandardisation: Having a machine process data requests requires the format of these requests and the associated responses to be standardised. This allows data sharing and retrieval to become a reproducible and programmatic aspect of our work."
  },
  {
    "objectID": "203-data-apis.html#what-is-an-api",
    "href": "203-data-apis.html#what-is-an-api",
    "title": "6  APIs",
    "section": "6.3 What is an API?",
    "text": "6.3 What is an API?\nSo then, if APIs are so great, what exactly are they?\nIn human-to-human communication, the set of rules governing acceptable behaviour is known as etiquette. Depending on when or where you live, social etiquette can be rather strict. The rules for computer-to-computer communication take this to a whole new level, because with machines there can be no room left for interpretation.\nThe set of rules governing interactions between computers or programmes is known as a protocol.\nAPIs provide a standard protocol for different programs to interact with one another. This makes it easier for developers to build complex systems by leveraging the functionality of existing services and platforms. The benefits of working in a standardised and modular way apply equally well to sharing data as they do to writing code or organising files.\nThere are two sides to communication and when machines communicate these are known as the server and the client.\n\n\n\nServers can seem intimidating, because unlike your laptop or mobile phone they don’t have their own input and output devices; they have no keyboard, no monitor, and no a mouse. Despite this, servers are just regular computers that are designed to store data and run programmes. Servers don’t have their own input or output devices because they are intended to be used remotely, via another computer. There is no need for a screen or a mouse if the user is miles away. Nothing scary going on here!\nPeople often find clients much less intimidating - they are simply any other computer or application that might contact the sever."
  },
  {
    "objectID": "203-data-apis.html#http",
    "href": "203-data-apis.html#http",
    "title": "6  APIs",
    "section": "6.4 HTTP",
    "text": "6.4 HTTP\nThis leads us one step further down the rabbit-hole. An API is a protocol that defines the rules of how applications communicate with one another. But how does this communication happen?\nHTTP (Hypertext Transfer Protocol) is the dominant mode communication on the World Wide Web. You can see the secure version of HTTP, HTTPS, at the start of most web addresses up at the top of your browser. For example:\nhttps://www.zakvarty.com/blog\nHTTP is the foundation of data communication on the web and is used to transfer files (such as text, images, and videos) between web servers and clients.\n\n\n\nTo understand HTTP communications, I find it helpful to imagine the client and the server as being a customer and a waiter at a restaurant. The client makes some request to the server, which then tries to comply before giving a response. The server might respond to confirm that the request was completed successfully. Alternatively, the server might respond with an error message, which is (hopefully) informative about why the request could not be completed.\nThis request-response model is the basis for HTTP, the communication system used by the majority of APIs."
  },
  {
    "objectID": "203-data-apis.html#http-requests",
    "href": "203-data-apis.html#http-requests",
    "title": "6  APIs",
    "section": "6.5 HTTP Requests",
    "text": "6.5 HTTP Requests\nAn HTTP request consists of:\n\nUniform Resource Locator (URL) [unique identifier for a thing]\nMethod [tells server the type of action requested by client]\nHeaders [meta-information about request, e.g. device type]\nBody [Data the client wants to send to the server]\n\n\n\n\n\n6.5.1 URL\nThe URL in a HTTP request specifies where that request is going to be made, for example http://example.com.\n\n\n6.5.2 Method\nThe action that the client wants to take is indicated by a set of well-defined methods or HTTP verbs. The most common HTTP verbs are GET, POST, PUT, PATCH, and DELETE.\nThe GET verb is used to retrieve a resource from the server, such as a web page or an image. The POST verb is used to send data to the server, such as when submitting a form or uploading a file. The PUT verb is used to replace a resource on the server with a new one, while the PATCH verb is used to update a resource on the server without replacing it entirely. Finally, the DELETE verb is used to delete a resource from the server.\nIn addition to these common HTTP verbs, there are also several less frequently used verbs. These are used for specialized purposes, such as requesting only the headers of a resource, or testing the connectivity between the client and the server.\n\n\n6.5.3 Header\nThe request headers contain meta-information about the request. This is where information about the device type would be included within the request.\n\n\n6.5.4 Body\nFinally, the body of the request contains the data that the client is providing to the server."
  },
  {
    "objectID": "203-data-apis.html#http-responses",
    "href": "203-data-apis.html#http-responses",
    "title": "6  APIs",
    "section": "6.6 HTTP Responses",
    "text": "6.6 HTTP Responses\nWhen the server receives a request it will attempt to fulfil it and then send a response back to the client.\n\n\n\nA response has a similar structure to a request apart from:\n\nresponses do not have a URL,\nresponses do not have a method,\nresponses have a status code.\n\n\n6.6.1 Status Codes\nThe status code is a 3 digit number, each of which has a specific meaning. Some common error codes that you might (already have) come across are:\n\n200: Success,\n404: Page not found (all 400s are errors),\n503: Page down.\n\nIn a data science context, a successful response will return the requested data within the data field. This will most likely be given in JSON or XML format."
  },
  {
    "objectID": "203-data-apis.html#authentication",
    "href": "203-data-apis.html#authentication",
    "title": "6  APIs",
    "section": "6.7 Authentication",
    "text": "6.7 Authentication\nNow that we know how applications communicate, you might ask how we can control who has access to the API and what types of request they can make. This can be done by the server setting appropriate permissions for each client. But then how does the server verify that the client is really who is claims to be?\nAuthentication is a way to ensure that only authorized clients are able to access an API. This is typically done by the server requiring each client to provide some secret information that uniquely identifies them, whenever they make requests to the API. This information allows the API server to validate the authenticity this user before it authorises the request.\n\n6.7.1 Basic Authentication\nThere are various ways to implement API authentication.\nBasic authentication involves each legitimate client having a username and password. An encrypted version of these is included in the Authorization header of the HTTP request. If the hear matches with the server’s records then the request is processed. If not, then a special status code (401) is returned to the client.\nBasic authentication is dangerous because it does not put any restrictions on what a client can do once they are authorised. Additional, individualised restrictions can be added by using an alternative authentication scheme.\n\n\n6.7.2 API Key Authentication\nAn API key is long, random string of letters and numbers that is assigned to each authorised user. An API key is distinct from the user’s password and keys are typically issued by the service that provides an API. Using keys rather than basic authentication allows the API provider to track and limit the usage of their API.\nFor example, a provider may issue a unique API key to each developer or organization that wants to use the API. The provider can then limit access to certain data. They could also limit the number of requests that each key can make in a given time period or prevent access to certain administrative functions, like changing passwords or deleting accounts.\nUnlike Basic Authentication, there is no standard way of a client sharing a key with the server. Depending on the API this might be in the Authorization field of the header, at the end of the URL (http://example.com?api_key=my_secret_key), or within the body of the data."
  },
  {
    "objectID": "203-data-apis.html#api-wrappers",
    "href": "203-data-apis.html#api-wrappers",
    "title": "6  APIs",
    "section": "6.8 API wrappers",
    "text": "6.8 API wrappers\nWe’ve learned a lot about how the internet works. Fortunately, a lot of the time we won’t have to worry about all of that new information other than for debugging purposes.\nIn the best case scenario, a very kind developer has written a “wrapper” function for the API. These wrappers are functions in R that will construct the HTTP request for you. If you are particularly lucky, the API wrapper will also format the response for you, converting it from XML or JSON back into an R object that is ready for immediate use."
  },
  {
    "objectID": "203-data-apis.html#geonames-wrapper",
    "href": "203-data-apis.html#geonames-wrapper",
    "title": "6  APIs",
    "section": "6.9 {geonames} wrapper",
    "text": "6.9 {geonames} wrapper\nrOpenSci has a curated list of many wrappers for accessing scientific data using R. We will focus on the GeoNames API, which gives open access to a geographical database. To access this data, we will use wrapper functions provided by the {geonames} package.\nThe aim here is to illustrate the important steps of getting started with a new API.\n\n6.9.1 Set up\nBefore we can get any data from the GeoNames API, we first need to do a little bit of set up.\n\nInstall and load {geonames} from CRAN\n\n\n#install.packages(\"geonames\")\nlibrary(geonames)\n\n\nCreate a user account for the GeoNames API\n\n\n\n\n\n\n\n\n\n\n\nActivate the account (see activation email)\n\n\n\n\n\n\n\n\n\n\n\nEnable the free web services for your GeoNames account by logging in at this link.\nTell R your credentials for GeoNames.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe could use the following code to tell R our credentials, but we absolutely should not.\n\noptions(geonamesUsername = \"example_username\")\n\nThis would save our username as an environment variable, but it also puts our API credentials directly into the script. If we share the script with others (internally, externally or publicly) we would be sharing our credentials too. Not good!\n\n\n\n\n6.9.2 Keep it Secret, Keep it Safe\nThe solution to this problem is to add our credentials as environment variables in our .Rprofile rather than in this script. The .Rprofile is an R script that is run at the start of every session. It can be created and edited directly, but can also be created and edited from within R.\nTo make/open your .Rprofile use the edit_r_profile() function from the {usethis} package.\n\nlibrary(usethis)\nusethis::edit_r_profile()\n\nWithin this file, add options(geonamesUsername=\"example_username\") on a new line, remembering to replace example_username with your own GeoNames username.\nThe final step is to check this this file ends with a blank line, save it and restart R. Then we are all set to start using {geonames}.\nThis set up procedure is indicative of most API wrappers, but of course the details will vary between each API. This is why good documentation is important!\n\nIf you are using {renv} to track the versions of R and the packages you are using take care. For {renv} to be effective you have to place your project level .Rprofile under version control - this means you might accidentally share your API credentials.\nA workaround for this problem is to create an R file that you source() from within the project level .Rprofile but is included in .gitignore, so that your credentials remain secret.\n\n\n\n6.9.3 Using {geonames}\nGeoNames has a whole host of different geo-datasets that you can explore. As a first example, let’s get all of the geo-tagged wikipedia articles that are within 1km of Imperial College London.\n\nimperial_coords &lt;- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km &lt;- 1\n\nimperial_neighbours &lt;- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\n\nLooking at the structure of imperial_neighbours we can see that it is a data frame with one row per geo-tagged wikipedia article.\n\nstr(imperial_neighbours)\n#&gt; 'data.frame':    204 obs. of  13 variables:\n#&gt;  $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n#&gt;  $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n#&gt;  $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n#&gt;  $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n#&gt;  $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n#&gt;  $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n#&gt;  $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n#&gt;  $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n#&gt;  $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n#&gt;  $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n#&gt;  $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n#&gt;  $ thumbnailImg: chr  NA NA NA NA ...\n#&gt;  $ geoNameId   : chr  NA NA NA NA ...\n\nTo confirm we have the correct location we can inspect the title of the first five neighbours.\n\nimperial_neighbours$title[1:5]\n#&gt; [1] \"Department of Mechanical Engineering, Imperial College London\"             \n#&gt; [2] \"Imperial College Business School\"                                          \n#&gt; [3] \"Exhibition Road\"                                                           \n#&gt; [4] \"Imperial College School of Medicine\"                                       \n#&gt; [5] \"Department of Civil and Environmental Engineering, Imperial College London\"\n\nNothing too surprising here, mainly departments of the college and Exhibition Road, which runs along one side of the campus. These sorts of check are important - I initially forgot the minus in the longitude and was getting results in East London!"
  },
  {
    "objectID": "203-data-apis.html#what-if-there-is-no-wrapper",
    "href": "203-data-apis.html#what-if-there-is-no-wrapper",
    "title": "6  APIs",
    "section": "6.10 What if there is no wrapper?",
    "text": "6.10 What if there is no wrapper?\nIf there is not a wrapper function, we can still access APIs fairly easilty using the {httr} package.\nWe will look at an example using OMDb, which is an open source version of IMDb, to get information about the movie Mean Girls.\nTo use the OMDB API you will once again need to request a free API key, follow a verification link and add your API key to your .Rprofile.\n\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n\nYou can then restart R and safely access your API key from within your R session.\n\n# Load your API key into the current R session\nombd_api_key &lt;- getOption(\"OMDB_API_Key\")\n\nUsing the documentation for the API, requests have URLs of the following form, where terms in angular brackets should be replaced by you.\nhttp://www.omdbapi.com/?t=&lt;TITLE&gt;&y=&lt;YEAR&gt;&plot=&lt;LENGTH&gt;&r=&lt;FORMAT&gt;&apikey=&lt;API_KEY&gt;\nWith a little bit of effort, we can write a function that composes this type of request URL for us. We will using the {glue} package to help us join strings together.\nRunning the example we get:\n\nmean_girls_request &lt;- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\n\nWe can then use the {httr} package to construct our request and store the response we get.\n\n#&gt; [1] 200\n\nThankfully it was a success! If you get a 401 error code here, check that you have clicked the activation link for your API key.\nThe full structure of the response is quite complicated, but we can easily extract the requested data using content()\n\n#&gt; $Title\n#&gt; [1] \"Mean Girls\"\n#&gt; \n#&gt; $Year\n#&gt; [1] \"2004\"\n#&gt; \n#&gt; $Rated\n#&gt; [1] \"PG-13\"\n#&gt; \n#&gt; $Released\n#&gt; [1] \"30 Apr 2004\"\n#&gt; \n#&gt; $Runtime\n#&gt; [1] \"97 min\"\n#&gt; \n#&gt; $Genre\n#&gt; [1] \"Comedy\"\n#&gt; \n#&gt; $Director\n#&gt; [1] \"Mark Waters\"\n#&gt; \n#&gt; $Writer\n#&gt; [1] \"Rosalind Wiseman, Tina Fey\"\n#&gt; \n#&gt; $Actors\n#&gt; [1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n#&gt; \n#&gt; $Plot\n#&gt; [1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n#&gt; \n#&gt; $Language\n#&gt; [1] \"English, German, Vietnamese, Swahili\"\n#&gt; \n#&gt; $Country\n#&gt; [1] \"United States, Canada\"\n#&gt; \n#&gt; $Awards\n#&gt; [1] \"7 wins & 25 nominations\"\n#&gt; \n#&gt; $Poster\n#&gt; [1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n#&gt; \n#&gt; $Ratings\n#&gt; $Ratings[[1]]\n#&gt; $Ratings[[1]]$Source\n#&gt; [1] \"Internet Movie Database\"\n#&gt; \n#&gt; $Ratings[[1]]$Value\n#&gt; [1] \"7.1/10\"\n#&gt; \n#&gt; \n#&gt; $Ratings[[2]]\n#&gt; $Ratings[[2]]$Source\n#&gt; [1] \"Rotten Tomatoes\"\n#&gt; \n#&gt; $Ratings[[2]]$Value\n#&gt; [1] \"84%\"\n#&gt; \n#&gt; \n#&gt; $Ratings[[3]]\n#&gt; $Ratings[[3]]$Source\n#&gt; [1] \"Metacritic\"\n#&gt; \n#&gt; $Ratings[[3]]$Value\n#&gt; [1] \"66/100\"\n#&gt; \n#&gt; \n#&gt; \n#&gt; $Metascore\n#&gt; [1] \"66\"\n#&gt; \n#&gt; $imdbRating\n#&gt; [1] \"7.1\"\n#&gt; \n#&gt; $imdbVotes\n#&gt; [1] \"403,600\"\n#&gt; \n#&gt; $imdbID\n#&gt; [1] \"tt0377092\"\n#&gt; \n#&gt; $Type\n#&gt; [1] \"movie\"\n#&gt; \n#&gt; $DVD\n#&gt; [1] \"21 Sep 2004\"\n#&gt; \n#&gt; $BoxOffice\n#&gt; [1] \"$86,058,055\"\n#&gt; \n#&gt; $Production\n#&gt; [1] \"N/A\"\n#&gt; \n#&gt; $Website\n#&gt; [1] \"N/A\"\n#&gt; \n#&gt; $Response\n#&gt; [1] \"True\""
  },
  {
    "objectID": "203-data-apis.html#wrapping-up",
    "href": "203-data-apis.html#wrapping-up",
    "title": "6  APIs",
    "section": "6.11 Wrapping up",
    "text": "6.11 Wrapping up\nWe have learned a bit more about how the internet works, the benefits of using an API to share data and how to request data from Open APIs.\nWhen obtaining data from the internet it’s vital that you keep your credentials safe, and that don’t do more work than is needed.\n\nKeep your API keys out of your code. Store them in your .Rprofile (and make sure this is not under version control!)\nScraping is always a last resort. Is there an API already?\nWriting your own code to access an API can be more painful than necessary.\nDon’t repeat other people, if a suitable wrapper exists then use it."
  },
  {
    "objectID": "203-data-apis.html#session-information",
    "href": "203-data-apis.html#session-information",
    "title": "6  APIs",
    "section": "6.12 Session Information",
    "text": "6.12 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: geonames(v.0.999)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), digest(v.0.6.33), R6(v.2.5.1), jsonlite(v.1.8.7), evaluate(v.0.21), httr(v.1.4.6), rlang(v.1.1.1), cli(v.3.6.1), curl(v.5.0.1), renv(v.0.16.0), rstudioapi(v.0.15.0), rmarkdown(v.2.23), tools(v.4.2.2), pander(v.0.6.5), glue(v.1.6.2), htmlwidgets(v.1.6.2), xfun(v.0.39), yaml(v.2.3.7), fastmap(v.1.1.1), compiler(v.4.2.2), htmltools(v.0.5.5) and knitr(v.1.43)"
  },
  {
    "objectID": "210-data-checklist.html#videos-chapters",
    "href": "210-data-checklist.html#videos-chapters",
    "title": "Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nTabular Data (27 min) [slides]\nWeb Scraping (22 min) [slides]\nAPIs (25 min) [slides]"
  },
  {
    "objectID": "210-data-checklist.html#reading",
    "href": "210-data-checklist.html#reading",
    "title": "Checklist",
    "section": "Reading",
    "text": "Reading\nUse the Acquiring and Sharing Data section of the reading list to support and guide your exploration of this week’s topics. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "210-data-checklist.html#tasks",
    "href": "210-data-checklist.html#tasks",
    "title": "Checklist",
    "section": "Tasks",
    "text": "Tasks\nCore:\n\nRevisit the Projects that you explored on Github last week. This time look for any data or documentation files.\n\nAre there any file types that are new to you?\nIf so, are there packages or helper function that would let you read this data into R?\nWhy might you not find many data files on Github?\n\nPlay CSS Diner to familiarise yourself with some CSS selectors.\nIdentify 3 APIs that give access to data on topics that interest you. Write a post on the discussion forum describing the APIs and use one of them to load some data into R.\nScraping Book Reviews:\n\nVisit the Amazon page for R for Data Science. Write code to scrape the percentage of customers giving each “star” rating (5⭐, …, 1⭐).\nTurn your code into a function that will return a tibble of the form:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct\nn_reviews\npercent_5_star\npercent_4_star\npercent_3_star\npercent_2_star\npercent_1_star\nurl\n\n\n\n\nexample_name\n1000\n20\n20\n20\n20\n20\nwww.example.com\n\n\n\n\n\n\nGeneralise your function to work for other Amazon products, where the function takes as input a vector of product names and an associated vector of URLs.\nUse your function to compare the reviews of the following three books: R for Data Science, R packages and ggplot2.\n\nBonus:\n\nAdd this function to the R package you made last week, remembering to add tests and documentation."
  },
  {
    "objectID": "210-data-checklist.html#live-session",
    "href": "210-data-checklist.html#live-session",
    "title": "Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then work through some examples of how to read data from non-standard sources.\nPlease come to the live session prepared to discuss the following points:\n\nRoger Peng states that files can be imported and exported using readRDS() and saveRDS() for fast and space efficient data storage. What is the downside to doing so?\nWhat data types have you come across (that we have not discussed already) and in what context are they used?\nWhat do you have to give greater consideration to when scraping data than when using an API?"
  },
  {
    "objectID": "300-edav-intro.html",
    "href": "300-edav-intro.html",
    "title": "(PART) Data Exploration and Visualisation",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\nImportant\n\n\n\nEffective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it.\nIf you would like to contribute to the development of EDS, you may do so at https://github.com/zakvarty/data_science_notes.\n\n\nNow that we have read our raw data into R we can start getting our data science project moving and being to see some initial returns on the time and effort that we have invested so far.\nIn this section we will explore how to wrangle, explore and visualise the data that forms the basis of our projects. These skills are often overlooked by folks coming into data science as being “soft skills” compared to modelling. However, I would argue that this is not the case because each of these tasks requires its own specialist knowledge and tools.\nAdditionally, these task make up the majority of data scientist’s work and are often where we can add the most value to an organisation. At this stage in a project we turn useless, messy data into a form that can be used; we derive initial insights from this data while making minimal assumptions; and we communicate all of this in an accurate and engaging way, to drive decision making both within and outwith the organisation."
  },
  {
    "objectID": "301-edav-wrangling.html#what-is-data-wrangling",
    "href": "301-edav-wrangling.html#what-is-data-wrangling",
    "title": "7  Data Wrangling",
    "section": "7.1 What is Data Wrangling?",
    "text": "7.1 What is Data Wrangling?\n\n\n\nOkay, so you’ve got some data. That’s a great start!\nYou might have had it handed to you by a collaborator, requested it via an API or scraped it from the raw html of a webpage. In the worst case scenario, you’re an actual scientist (not just a data one) and you spent the last several months of your life painstakingly measuring flower petals or car parts. Now we really want to do something useful with that data.\nWe’ve seen already how you can load the data into R and pivot between wider and longer formats, but that probably isn’t enough to satisfy your curiosity. You want to be able to view your data, manipulate and subset it, create new variables from existing ones and cross-reference your dataset with others. All of these are things possible in R and are known under various collective names including data manipulation, data munging and data wrangling.\nI’ve decided to use the term data wrangling here. That’s because data manipulation sounds boring as heck and data munging is both unpleasant to say and makes me imagine we are squelching through some sort of information swamp.\nIn what follows, I’ll give a fly-by tour of tools for data wrangling in R, showing some examples along the way. I’ll focus on some of the most common and useful operations and link out to some more extensive guides for wrangling your data in R, that you can refer back to as you need them."
  },
  {
    "objectID": "301-edav-wrangling.html#example-data-sets",
    "href": "301-edav-wrangling.html#example-data-sets",
    "title": "7  Data Wrangling",
    "section": "7.2 Example Data Sets",
    "text": "7.2 Example Data Sets\n\n\n\n\n\n\n\n\n\n\n\n\nTo demonstrate some standard skills we will use two datasets. The mtcars data comes built into any R installation. The second data set we will look at is the penguins data from {palmerpenguins}.\n\nlibrary(palmerpenguins)\npengins &lt;- palmerpenguins::penguins\ncars &lt;- datasets::mtcars"
  },
  {
    "objectID": "301-edav-wrangling.html#viewing-your-data",
    "href": "301-edav-wrangling.html#viewing-your-data",
    "title": "7  Data Wrangling",
    "section": "7.3 Viewing Your Data",
    "text": "7.3 Viewing Your Data\n\n7.3.1 View()\nThe View() function can be used to create a spreadsheet-like view of your data. In RStudio this will open as a new tab.\nView() will work for any “matrix-like” R object, such as a tibble, data frame, vector or matrix. Note the capital letter - the function is called View(), not view().\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 head()\nFor large data sets, you might not want (or be able to) view it all at once. You can then use head() to view the first few rows. The integer argument n specifies the number of rows you would like to return.\n\nhead(x = pengins, n = 3)\n#&gt; # A tibble: 3 × 8\n#&gt;   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie  Torgers…           39.1          18.7               181        3750\n#&gt; 2 Adelie  Torgers…           39.5          17.4               186        3800\n#&gt; 3 Adelie  Torgers…           40.3          18                 195        3250\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n7.3.3 str()\nAn alternative way to view the a large data set, or one with a complicated format is to examine its structure with str(). This is a useful way to inspect the structure of list-like objects, particularly when they’ve got a nested structure.\n\nstr(penguins)\n#&gt; tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#&gt;  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n#&gt;  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n#&gt;  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n#&gt;  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n#&gt;  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n#&gt;  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\n7.3.4 names()\nFinally, if you just want to access the variable names you can do so with the names() function from base R.\n\nnames(penguins)\n#&gt; [1] \"species\"           \"island\"            \"bill_length_mm\"   \n#&gt; [4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n#&gt; [7] \"sex\"               \"year\"\n\nSimilarly, you can explicitly access the row and column names of a data frame or tibble using colnames() or rownames().\n\ncolnames(cars)\n#&gt;  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n#&gt; [11] \"carb\"\n\n\nrownames(cars)\n#&gt;  [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n#&gt;  [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n#&gt;  [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n#&gt; [10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n#&gt; [13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n#&gt; [16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n#&gt; [19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n#&gt; [22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n#&gt; [25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n#&gt; [28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n#&gt; [31] \"Maserati Bora\"       \"Volvo 142E\"\n\nIn the cars data, the car model are stored as the row names. This doesn’t really jive with our idea of tidy data - we’ll see how to fix that shortly."
  },
  {
    "objectID": "301-edav-wrangling.html#renaming-variables",
    "href": "301-edav-wrangling.html#renaming-variables",
    "title": "7  Data Wrangling",
    "section": "7.4 Renaming Variables",
    "text": "7.4 Renaming Variables\n\n7.4.1 colnames()\nThe function colnames() can be used to set, as well as to retrieve, column names.\n\ncars_renamed &lt;- cars \ncolnames(cars_renamed)[1] &lt;- \"miles_per_gallon\"\ncolnames(cars_renamed)\n#&gt;  [1] \"miles_per_gallon\" \"cyl\"              \"disp\"            \n#&gt;  [4] \"hp\"               \"drat\"             \"wt\"              \n#&gt;  [7] \"qsec\"             \"vs\"               \"am\"              \n#&gt; [10] \"gear\"             \"carb\"\n\n\n\n7.4.2 dplyr::rename()\nWe can also use functions from {dplyr} to rename columns. Let’s alter the second column name.\n\nlibrary(dplyr)\ncars_renamed &lt;- rename(.data = cars_renamed, cylinders = cyl)\ncolnames(cars_renamed)\n#&gt;  [1] \"miles_per_gallon\" \"cylinders\"        \"disp\"            \n#&gt;  [4] \"hp\"               \"drat\"             \"wt\"              \n#&gt;  [7] \"qsec\"             \"vs\"               \"am\"              \n#&gt; [10] \"gear\"             \"carb\"\n\nThis could be done as part of a pipe, if we were making many alterations.\n\ncars_renamed &lt;- cars_renamed %&gt;% \n  rename(displacement = disp) %&gt;% \n  rename(horse_power = hp) %&gt;% \n  rename(rear_axel_ratio = drat)\n\ncolnames(cars_renamed)\n#&gt;  [1] \"miles_per_gallon\" \"cylinders\"        \"displacement\"    \n#&gt;  [4] \"horse_power\"      \"rear_axel_ratio\"  \"wt\"              \n#&gt;  [7] \"qsec\"             \"vs\"               \"am\"              \n#&gt; [10] \"gear\"             \"carb\"\n\nWhen using the dplyr function you have to remember the format new_name = old_name. This matches the format used to create a data frame or tibble, but is the opposite order to the python function of the same name and often catches people out.\nIn the section on creating new variables, we will see an alternative way of doing this by copying the column and then deleting the original."
  },
  {
    "objectID": "301-edav-wrangling.html#subsetting",
    "href": "301-edav-wrangling.html#subsetting",
    "title": "7  Data Wrangling",
    "section": "7.5 Subsetting",
    "text": "7.5 Subsetting\n\n7.5.1 Base R\nIn base R you can extract rows, columns and combinations thereof using index notation.\n\n# First row\npenguins[1, ]\n#&gt; # A tibble: 1 × 8\n#&gt;   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie  Torgers…           39.1          18.7               181        3750\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# First Column \npenguins[ , 1]\n#&gt; # A tibble: 344 × 1\n#&gt;   species\n#&gt;   &lt;fct&gt;  \n#&gt; 1 Adelie \n#&gt; 2 Adelie \n#&gt; 3 Adelie \n#&gt; 4 Adelie \n#&gt; 5 Adelie \n#&gt; 6 Adelie \n#&gt; # ℹ 338 more rows\n\n# Rows 2-3 of columns 1, 2 and 4\npenguins[2:3, c(1, 2, 4)]\n#&gt; # A tibble: 2 × 3\n#&gt;   species island    bill_depth_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie  Torgersen          17.4\n#&gt; 2 Adelie  Torgersen          18\n\nUsing negative indexing you can remove rows or columns\n\n# Drop all but first row\npenguins[-(2:344), ]\n#&gt; # A tibble: 1 × 8\n#&gt;   species island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt; 1 Adelie  Torgers…           39.1          18.7               181        3750\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n# Drop all but first column \npenguins[ , -(2:8)]\n#&gt; # A tibble: 344 × 1\n#&gt;   species\n#&gt;   &lt;fct&gt;  \n#&gt; 1 Adelie \n#&gt; 2 Adelie \n#&gt; 3 Adelie \n#&gt; 4 Adelie \n#&gt; 5 Adelie \n#&gt; 6 Adelie \n#&gt; # ℹ 338 more rows\n\nYou can also select rows or columns by their names. This can be done using the bracket syntax ([ ]) or the dollar syntax ($).\n\npengins[ , \"species\"]\n#&gt; # A tibble: 344 × 1\n#&gt;   species\n#&gt;   &lt;fct&gt;  \n#&gt; 1 Adelie \n#&gt; 2 Adelie \n#&gt; 3 Adelie \n#&gt; 4 Adelie \n#&gt; 5 Adelie \n#&gt; 6 Adelie \n#&gt; # ℹ 338 more rows\npenguins$species\n#&gt;   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt;  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#&gt; [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n#&gt; [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#&gt; [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#&gt; [344] Chinstrap\n#&gt; Levels: Adelie Chinstrap Gentoo\n\nSince penguins is a tibble, these return different types of object. Subsetting a tibble with bracket syntax will return a tibble, while extracting a column using the dollar syntax returns a vector of values.\n\n\n7.5.2 filter() and select()\n{dplyr} has two functions for subsetting, filter() subsets by rows and select() subsets by column.\nIn both functions you list what you would like to retain. Filter and select calls can be piped together to subset based on row and column values.\n\npenguins %&gt;% \n  select(species, island, body_mass_g)\n#&gt; # A tibble: 344 × 3\n#&gt;   species island    body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n#&gt; 1 Adelie  Torgersen        3750\n#&gt; 2 Adelie  Torgersen        3800\n#&gt; 3 Adelie  Torgersen        3250\n#&gt; 4 Adelie  Torgersen          NA\n#&gt; 5 Adelie  Torgersen        3450\n#&gt; 6 Adelie  Torgersen        3650\n#&gt; # ℹ 338 more rows\n\n\npenguins %&gt;% \n  select(species, island, body_mass_g) %&gt;% \n  filter(body_mass_g &gt; 6000)\n#&gt; # A tibble: 2 × 3\n#&gt;   species island body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;\n#&gt; 1 Gentoo  Biscoe        6300\n#&gt; 2 Gentoo  Biscoe        6050\n\nSubsetting rows can be inverted by negating the filter() statement\n\npenguins %&gt;% \n  select(species, island, body_mass_g) %&gt;% \n  filter(!(body_mass_g &gt; 6000))\n#&gt; # A tibble: 340 × 3\n#&gt;   species island    body_mass_g\n#&gt;   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n#&gt; 1 Adelie  Torgersen        3750\n#&gt; 2 Adelie  Torgersen        3800\n#&gt; 3 Adelie  Torgersen        3250\n#&gt; 4 Adelie  Torgersen        3450\n#&gt; 5 Adelie  Torgersen        3650\n#&gt; 6 Adelie  Torgersen        3625\n#&gt; # ℹ 334 more rows\n\nand dropping columns can done by selecting all columns except the one(s) you want to drop.\n\n#&gt; # A tibble: 340 × 1\n#&gt;   body_mass_g\n#&gt;         &lt;int&gt;\n#&gt; 1        3750\n#&gt; 2        3800\n#&gt; 3        3250\n#&gt; 4        3450\n#&gt; 5        3650\n#&gt; 6        3625\n#&gt; # ℹ 334 more rows"
  },
  {
    "objectID": "301-edav-wrangling.html#creating-new-variables",
    "href": "301-edav-wrangling.html#creating-new-variables",
    "title": "7  Data Wrangling",
    "section": "7.6 Creating New Variables",
    "text": "7.6 Creating New Variables\n\n7.6.1 Base R\nWe can create new variables in base R by assigning a vector of the correct length to a new column name.\n\ncars_renamed$weight &lt;- cars_renamed$wt\n\nIf we then drop the original column from the data frame, this gives us an alternative way of renaming columns.\n\ncars_renamed &lt;- cars_renamed[ ,-which(names(cars_renamed) == \"wt\")]\nhead(cars_renamed, n = 5)\n#&gt;                   miles_per_gallon cylinders displacement horse_power\n#&gt; Mazda RX4                     21.0         6          160         110\n#&gt; Mazda RX4 Wag                 21.0         6          160         110\n#&gt; Datsun 710                    22.8         4          108          93\n#&gt; Hornet 4 Drive                21.4         6          258         110\n#&gt; Hornet Sportabout             18.7         8          360         175\n#&gt;                   rear_axel_ratio  qsec vs am gear carb weight\n#&gt; Mazda RX4                    3.90 16.46  0  1    4    4  2.620\n#&gt; Mazda RX4 Wag                3.90 17.02  0  1    4    4  2.875\n#&gt; Datsun 710                   3.85 18.61  1  1    4    1  2.320\n#&gt; Hornet 4 Drive               3.08 19.44  1  0    3    1  3.215\n#&gt; Hornet Sportabout            3.15 17.02  0  0    3    2  3.440\n\nOne thing to be aware of is that this operation does not preserve column ordering.\nGenerally speaking, code that relies on columns being in a specific order is fragile - it breaks easily. If possible, you should try to write your code in another way that’s robust to column reordering. I’ve done that here when removing the wt column by looking up the column index as part of my code, rather than assuming it will always be the fourth column.\n\n\n7.6.2 dplyr::mutate()\nThe function from {dplyr} to create new columns is mutate(). Let’s create another column that has the car’s weight in kilogrammes rather than tonnes.\n\ncars_renamed &lt;- cars_renamed %&gt;% \n  mutate(weight_kg = weight * 1000)\n\ncars_renamed %&gt;% \n  select(miles_per_gallon, cylinders, displacement, weight, weight_kg) %&gt;% \n  head(n = 5)\n#&gt;                   miles_per_gallon cylinders displacement weight weight_kg\n#&gt; Mazda RX4                     21.0         6          160  2.620      2620\n#&gt; Mazda RX4 Wag                 21.0         6          160  2.875      2875\n#&gt; Datsun 710                    22.8         4          108  2.320      2320\n#&gt; Hornet 4 Drive                21.4         6          258  3.215      3215\n#&gt; Hornet Sportabout             18.7         8          360  3.440      3440\n\nYou can also create new columns that are functions of multiple other columns.\n\ncars_renamed &lt;- cars_renamed %&gt;% \n  mutate(cylinder_adjusted_mpg = miles_per_gallon / cylinders)\n\n\n\n7.6.3 rownames_to_column()\nOne useful example of adding an additional row to a data frame is to convert its row names to a column of the data fame.\n\ncars %&gt;% \n  mutate(model = rownames(cars_renamed)) %&gt;% \n  select(mpg, cyl, model) %&gt;% \n  head(n = 5)\n#&gt;                    mpg cyl             model\n#&gt; Mazda RX4         21.0   6         Mazda RX4\n#&gt; Mazda RX4 Wag     21.0   6     Mazda RX4 Wag\n#&gt; Datsun 710        22.8   4        Datsun 710\n#&gt; Hornet 4 Drive    21.4   6    Hornet 4 Drive\n#&gt; Hornet Sportabout 18.7   8 Hornet Sportabout\n\nThere’s a neat function called rownames_to_column() in {tibble} which will add this as the first column and remove the row names all in one step.\n\ncars %&gt;% \n  tibble::rownames_to_column(var = \"model\") %&gt;% \n  head(n = 5)\n#&gt;               model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\n\n7.6.4 rowids_to_column()\nAnother function from {tibble} adds the row id of each observation as a new column. This is often useful when ordering or combining tables.\n\ncars %&gt;% \n  tibble::rowid_to_column(var = \"row_id\") %&gt;% \n  head(n = 5)\n#&gt;   row_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; 3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"
  },
  {
    "objectID": "301-edav-wrangling.html#summaries",
    "href": "301-edav-wrangling.html#summaries",
    "title": "7  Data Wrangling",
    "section": "7.7 Summaries",
    "text": "7.7 Summaries\nThe summarise() function allows you to collapse a data frame into a single row, which using a summary statistic of your choosing.\nWe can calculate the average bill length of all penguins in a single summarise() function call.\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm))\n#&gt; # A tibble: 1 × 1\n#&gt;   average_bill_length_mm\n#&gt;                    &lt;dbl&gt;\n#&gt; 1                     NA\n\nSince we have missing values, we might instead want to calculate the mean of the recorded values.\n\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n#&gt; # A tibble: 1 × 1\n#&gt;   average_bill_length_mm\n#&gt;                    &lt;dbl&gt;\n#&gt; 1                   43.9\n\nWe can also use summarise() to gather multiple summaries in a single data frame.\n\nbill_length_mm_summary &lt;- penguins %&gt;% \n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\nbill_length_mm_summary\n#&gt; # A tibble: 1 × 8\n#&gt;    mean median   min   q_0   q_1   q_2   q_3   q_4\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6\n\nIn all, this isn’t overly exciting. You might, rightly, wonder why you’d want to use these summarise() calls when we could just use the simpler base R calls directly.\nOne benefit is that the summarise calls ensure consistent output. However, the main advantage comes when you want to apply these summaries to distinct subgroups of the data."
  },
  {
    "objectID": "301-edav-wrangling.html#grouped-operations",
    "href": "301-edav-wrangling.html#grouped-operations",
    "title": "7  Data Wrangling",
    "section": "7.8 Grouped Operations",
    "text": "7.8 Grouped Operations\nThe real benefit of summarise() comes from its combination with group_by(). This allows to you calculate the same summary statistics for each level of a factor with only one additional line of code. Here we’re re-calculating the same set of summary statistics we just found for all penguins, but for each individual species.\n\npenguins %&gt;% \n  group_by(species) %&gt;%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#&gt; # A tibble: 3 × 9\n#&gt;   species    mean median   min   q_0   q_1   q_2   q_3   q_4\n#&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  \n#&gt; 2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#&gt; 3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\nYou can group by multiple factors to calculate summaries for each distinct combination of levels within your data set. Here we group by combinations of species and the island to which they belong.\n\npenguin_summary_stats &lt;- penguins %&gt;% \n  group_by(species, island) %&gt;%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#&gt; `summarise()` has grouped output by 'species'. You can override using the\n#&gt; `.groups` argument.\n\npenguin_summary_stats\n#&gt; # A tibble: 5 × 10\n#&gt; # Groups:   species [3]\n#&gt;   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#&gt;   &lt;fct&gt;     &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#&gt; 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#&gt; 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#&gt; 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#&gt; 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\n\n7.8.1 Ungrouping\nBy default, each call to summarise() will undo one level of grouping. This means that our previous result was still grouped by species.\n(We can see this in the tibble output above, and also by examining the structure of the returned data frame. This tells us that this is an S3 object of class “grouped_df”, which inherits its properties from a “tbl_df”, “tbl”, and “data.frame” objects.)\n\nclass(penguin_summary_stats)\n#&gt; [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nSince we have grouped by two variables, R expects us to use two summaries before returning a data frame (or tibble) that is not grouped. One way to satisfy this is to use apply a second summary at the species level of grouping.\n\npenguin_summary_stats %&gt;% \n  summarise_all(mean, na.rm = TRUE)\n#&gt; # A tibble: 3 × 10\n#&gt;   species   island  mean median   min   q_0   q_1   q_2   q_3   q_4\n#&gt;   &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2\n#&gt; 2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#&gt; 3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\nHowever, we won’t always want to do apply another summary. In that case, we can undo the grouping using ungroup(). Remembering to ungroup is a common gotcha and cause of confusion when working with multiple-group summaries.\n\nungroup(penguin_summary_stats)\n#&gt; # A tibble: 5 × 10\n#&gt;   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#&gt;   &lt;fct&gt;     &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#&gt; 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#&gt; 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#&gt; 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#&gt; 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\n\nThere’s an alternative method to achieve the same thing in a single step when using {dplyr} versions 1.0.0 and above. This is to to set the .groups parameter of the summarise() function call, which determines the grouping of the returned data frame.\nThe .groups parameter and can take 4 possible values:\n\n“drop_last”: dropping the last level of grouping (The only option before v1.0.0);\n“drop”: All levels of grouping are dropped;\n“keep”: Same grouping structure as .data;\n“rowwise”: Each row is its own group.\n\nBy default, “drop_last” is used if all the results have 1 row and “keep” is used otherwise."
  },
  {
    "objectID": "301-edav-wrangling.html#reordering-factors",
    "href": "301-edav-wrangling.html#reordering-factors",
    "title": "7  Data Wrangling",
    "section": "7.9 Reordering Factors",
    "text": "7.9 Reordering Factors\nR stored factors as integer values, which it then maps to a set of labels. Only factor levels that appear in your data will be assigned a coded integer value and the mapping between factor levels and integers will depend on the order that the labels appear in your data.\nThis can be annoying, particularly when your factor levels relate to properties that aren’t numerical but do have an inherent ordering to them. In the example below, we have the t-shirt size of twelve people.\n\ntshirts &lt;- tibble::tibble(\n  id = 1:12, \n  size = as.factor(c(\"L\", NA, \"M\", \"S\", \"XS\", \"M\", \"XXL\", \"L\", \"XS\", \"M\", \"L\", \"S\"))\n)\n\nlevels(tshirts$size)\n#&gt; [1] \"L\"   \"M\"   \"S\"   \"XS\"  \"XXL\"\n\nIrritatingly, the sizes aren’t in order and extra large isn’t included because it’s not included in this particular sample. This leads to awkward looking summary tables and plots.\n\ntshirts %&gt;% group_by(size) %&gt;% summarise(count = n())\n#&gt; # A tibble: 6 × 2\n#&gt;   size  count\n#&gt;   &lt;fct&gt; &lt;int&gt;\n#&gt; 1 L         3\n#&gt; 2 M         3\n#&gt; 3 S         2\n#&gt; 4 XS        2\n#&gt; 5 XXL       1\n#&gt; 6 &lt;NA&gt;      1\n\nWe can fix this by creating a new variable with the factors explicitly coded in the correct order. We also need to specify that we should not drop empty groups as part of group_by().\n\n#&gt; # A tibble: 7 × 2\n#&gt;   size_tidy count\n#&gt;   &lt;fct&gt;     &lt;int&gt;\n#&gt; 1 XS            2\n#&gt; 2 S             2\n#&gt; 3 M             3\n#&gt; 4 L             3\n#&gt; 5 XL            0\n#&gt; 6 XXL           1\n#&gt; # ℹ 1 more row"
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-factors",
    "href": "301-edav-wrangling.html#be-aware-factors",
    "title": "7  Data Wrangling",
    "section": "7.10 Be Aware: Factors",
    "text": "7.10 Be Aware: Factors\nAs we have seen a little already, categorical variables can cause issues when wrangling and presenting data in R. All of these problems are solvable using base R techniques but the {forcats} package provides tools for the most common of these problems. This includes functions for changing the order of factor levels or the values with which they are associated.\nSome examples functions from the package include:\n\nfct_reorder(): Reordering a factor by another variable.\nfct_infreq(): Reordering a factor by the frequency of values.\nfct_relevel(): Changing the order of a factor by hand.\nfct_lump(): Collapsing the least/most frequent values of a factor into “other”.\n\nExamples of each of these can be found in the forcats vignette or the factors chapter of R for data science."
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-strings",
    "href": "301-edav-wrangling.html#be-aware-strings",
    "title": "7  Data Wrangling",
    "section": "7.11 Be Aware: Strings",
    "text": "7.11 Be Aware: Strings\nWorking with and analysing text data is a skill unto itself. However, it is useful to be able to do some basic manipulation of character strings programatically.\nBecause R was developed as a statistical programming language, it is well suited to the computational and modelling aspects of working with text data but the base R string manipulation functions can be a bit unwieldy at times.\nThe {stringr} package aims to combat this by providing useful helper functions for a range of text management problems. Even when not analysing text data these can be useful, for example to remove prefixes on a lot of column names.\nSuppose we wanted to keep only the text following an underscore in these column names. We could do that by using a regular expression to extract lower-case or upper-case letters which follow an underscore.\n\nhead(poorly_named_df)\n#&gt; # A tibble: 6 × 11\n#&gt;   observation_id   V1_A    V2_B   V3_C    V4_D   V5_E   V6_F    V7_G   V8_H\n#&gt;            &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1              1 -0.363  0.213  -0.301 -1.34   -1.01  -0.106  0.0222 -0.101\n#&gt; 2              2  0.239 -1.63   -0.339 -0.215   0.327 -0.141 -0.204   1.21 \n#&gt; 3              3  1.36  -1.15    1.02  -0.180   0.158  0.167  0.467   0.723\n#&gt; 4              4 -1.56   1.43   -0.721 -0.441  -2.07   0.689 -1.34   -1.21 \n#&gt; 5              5  1.60  -0.0935 -1.74  -1.01    1.42  -0.427 -1.39    0.162\n#&gt; 6              6 -0.184 -0.759  -0.168 -0.0412  0.680 -0.241  0.198  -0.525\n#&gt; # ℹ 2 more variables: V9_I &lt;dbl&gt;, V10_J &lt;dbl&gt;\n\n\n#&gt;  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n\nAlternatively, can avoid using regular expressions. We can split each column name at the underscore and keep only the second part of each string.\n\n# split column names at underscores and inspect structure of resuting object\nsplit_strings &lt;- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n#&gt; List of 11\n#&gt;  $ : chr [1:2] \"observation\" \"id\"\n#&gt;  $ : chr [1:2] \"V1\" \"A\"\n#&gt;  $ : chr [1:2] \"V2\" \"B\"\n#&gt;  $ : chr [1:2] \"V3\" \"C\"\n#&gt;  $ : chr [1:2] \"V4\" \"D\"\n#&gt;  $ : chr [1:2] \"V5\" \"E\"\n#&gt;  $ : chr [1:2] \"V6\" \"F\"\n#&gt;  $ : chr [1:2] \"V7\" \"G\"\n#&gt;  $ : chr [1:2] \"V8\" \"H\"\n#&gt;  $ : chr [1:2] \"V9\" \"I\"\n#&gt;  $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n#&gt;  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n\nAgain, unless you plan to work extensively with text data, I would recommend that you look up such string manipulations as you need them. The strings section of R for Data Science is a useful starting point."
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-date-times",
    "href": "301-edav-wrangling.html#be-aware-date-times",
    "title": "7  Data Wrangling",
    "section": "7.12 Be Aware: Date-Times",
    "text": "7.12 Be Aware: Date-Times\nRemember all the fuss we made about storing dates in the ISO standard format? That was because dates and times are complicated enough to work with before adding extra ambiguity.\n\\[ \\text{YYYY} - \\text{MM} - \\text{DD}\\] Dates, times and time intervals have to reconcile two factors: the physical orbit of the Earth around the Sun and the social and geopolitical mechanisms that determine how we measure and record the passing of time. This makes the history of date and time records fascinating and can make working with this type of data complicated.\nMoving from larger to smaller time spans: leap years alter the number of days in a year, months are of variable length (with February’s length changing from year to year). If your data are measured in a place that uses daylight saving, then one day a year will be 23 hours long and another will be 25 hours long. To make things worse, the dates and the hour at which the clocks change are not uniform across countries, which might be in distinct time zones that themselves change over time.\nEven at the level of minutes and seconds we aren’t safe - since the Earth’s orbit is gradually slowing down a leap second is added approximately every 21 months. Nor are things any better when looking at longer time scales or across cultures, where we might have to account for different calendars: months are added removed and altered over time, other calendar systems still take different approaches to measuring time and using different units and origin points.\nWith all of these issues you have to be very careful when working with date and time data. Functions to help you with this can be found in the {lubridate} package, with examples in the dates and times chapter of R for data science."
  },
  {
    "objectID": "301-edav-wrangling.html#be-aware-relational-data",
    "href": "301-edav-wrangling.html#be-aware-relational-data",
    "title": "7  Data Wrangling",
    "section": "7.13 Be Aware: Relational Data",
    "text": "7.13 Be Aware: Relational Data\nWhen the data you need are stored across two or more data frames you need to be able to cross-reference those and match up values for observational unit. This sort of data is know as relational data, and is used extensively in data science.\nThe variables you use to match observational units across data frames are known as keys. The primary key belongs to the first table and the foreign key belongs to the secondary table. There are various ways to join these data frames, depending on if you want to retain.\n\n7.13.0.1 Join types\nYou might want to keep only observational units that have key variables values in both data frames, this is known as an inner join.\n\n\n\n\n\nInner join diagram. Source: R for Data Science\n\n\n\n\nYou might instead want to keep all units from the primary table but pad with NAs where there is not a corresponding foreign key in the second table. This results in an (outer) left-join.\n\n\n\n\n\nDiagram for left, right and outer joins. Source: R for Data Science\n\n\n\n\nConversely, you might keep all units from the second table but pad with NAs where there is not a corresponding foreign key in the primary table. This is imaginatively named an (outer) right-join.\nIn the (outer) full join, all observational units from either table are retained and all missing values are padded with NAs.\nThings get more complicated when keys don’t uniquely identify observational units in either one or both of the tables. I’d recommend you start exploring these ideas with the relational data chapter of R for Data Science.\n\n\n7.13.0.2 Why and where to learn more\nWorking with relational data is essential to getting any data science up and running out in the wilds of reality. This is because businesses and companies don’t store all of their data in a huge single csv file. For one this isn’t very efficient, because most cells would be empty. Secondly, it’s not a very secure approach, since you can’t grant partial access to the data. That’s why information is usually stored in many data frames (more generically known as tables) within one or more databases.\nThese data silos are created, maintained, accessed and destroyed using a relational data base management system. These management systems use code to manage and access the stored data, just like we have seen in the dplyr commands above. You might well have heard of the SQL programming language (and its many variants), which is a popular language for data base management and is the inspiration for the dplyr package and verbs.\nIf you’d like to learn more then there are many excellent introductory SQL books and courses, I’d recommend picking one that focuses on data analysis or data science unless you really want to dig into efficient storage and querying of databases."
  },
  {
    "objectID": "301-edav-wrangling.html#wrapping-up",
    "href": "301-edav-wrangling.html#wrapping-up",
    "title": "7  Data Wrangling",
    "section": "7.14 Wrapping up",
    "text": "7.14 Wrapping up\nWe have:\n\nLearned how to wrangle tabular data in R with {dplyr}\nMet the idea of relational data and {dplyr}’s relationship to SQL\nBecome aware of some tricky data types and packages that can help."
  },
  {
    "objectID": "301-edav-wrangling.html#session-information",
    "href": "301-edav-wrangling.html#session-information",
    "title": "7  Data Wrangling",
    "section": "7.15 Session Information",
    "text": "7.15 Session Information\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: dplyr(v.1.1.2) and palmerpenguins(v.0.1.1)\nloaded via a namespace (and not attached): Rcpp(v.1.0.11), rstudioapi(v.0.15.0), knitr(v.1.43), magrittr(v.2.0.3), tidyselect(v.1.2.0), R6(v.2.5.1), rlang(v.1.1.1), fastmap(v.1.1.1), fansi(v.1.0.4), stringr(v.1.5.0), tools(v.4.2.2), xfun(v.0.39), utf8(v.1.2.3), cli(v.3.6.1), withr(v.2.5.0), htmltools(v.0.5.5), yaml(v.2.3.7), digest(v.0.6.33), tibble(v.3.2.1), lifecycle(v.1.0.3), purrr(v.1.0.1), htmlwidgets(v.1.6.2), vctrs(v.0.6.3), glue(v.1.6.2), evaluate(v.0.21), rmarkdown(v.2.23), stringi(v.1.7.12), pander(v.0.6.5), compiler(v.4.2.2), pillar(v.1.9.0), generics(v.0.1.3), jsonlite(v.1.8.7), renv(v.0.16.0) and pkgconfig(v.2.0.3)"
  },
  {
    "objectID": "302-edav-analysis.html#introduction",
    "href": "302-edav-analysis.html#introduction",
    "title": "8  Exploratory Data Analysis",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nExploratory data analysis is an essential stage in any data science project. It allows you to become familiar with the data you are working with while also to identify potential strategies for progressing the project and flagging any areas of concern.\nIn this chapter we will look at three different perspectives on exploratory data analysis: its purpose for you as a data scientist, its purpose for the broader team working on the project and finally its purpose for the project itself."
  },
  {
    "objectID": "302-edav-analysis.html#get-to-know-your-data",
    "href": "302-edav-analysis.html#get-to-know-your-data",
    "title": "8  Exploratory Data Analysis",
    "section": "8.2 Get to know your data",
    "text": "8.2 Get to know your data\nLet’s first focus on an exploratory data analysis from our own point of view, as data scientists.\nExploratory data analysis (or EDA) is a process of examining a data set to understand its overall structure, contents, and the relationships between the variables it contains. EDA is an iterative process that’s often done before building a model or making other data-driven decisions within a data science project.\n\nExploratory Data Analysis: quick and simple exerpts, summaries and plots to better understand a data set.\n\nOne key aspect of EDA is generating quick and simple summaries and plots of the data. These plots and summary statistics can help to quickly understand the distribution of and relationships between the recorded variables. Additionally, during an exploratory analysis you will familiarise yourself with the structure of the data you’re working with and how that data was collected.\n\n\n\n\n\nInvestigating marginal and pairwise relationships in the Iris dataset.\n\n\n\n\nSince EDA is an initial and iterative process, it’s rare that any component of the analysis will be put into production. Instead, the goal is to get a general understanding of the data that can inform the next steps of the analysis.\nIn terms of workflow, this means that using one or more notebooks is often an effective way of organising your work during an exploratory analysis. This allows for rapid iteration and experimentation, while also providing a level of reproducibility and documentation. Since notebooks allow you to combine code, plots, tables and text in a single document, this makes it easy to share your initial findings with stakeholders and project managers."
  },
  {
    "objectID": "302-edav-analysis.html#start-a-conversation",
    "href": "302-edav-analysis.html#start-a-conversation",
    "title": "8  Exploratory Data Analysis",
    "section": "8.3 Start a conversation",
    "text": "8.3 Start a conversation\n\nAn effective EDA sets a precedent for open communication with the stakeholder and project manager.\n\nWe’ve seen the benefits of an EDA for you as a data scientist, but this isn’t the only perspective.\nOne key benefit of an EDA is that it can kick-start your communication with subject matter experts and project managers. You can build rapport and trust early in a project’s life cycle by sharing your preliminary findings with these stakeholders . This can lead to a deeper understanding of both the available data and the problem being addressed for everyone involved. If done well, it also starts to build trust in your work before you even begin the modelling stage of a project.\n\n8.3.1 Communicating with specialists\nSharing an exploratory analysis will inevitably require a time investment. The graphics, tables, and summaries you produce need to be presented to a higher standard and explained in a way that is clear to a non-specialist. However, this time investment will often pay dividends because of the additional contextual knowledge that the domain-expert can provide. They have a deep understanding of the business or technical domain surrounding the problem. This can provide important insights that aren’t in the data itself, but which are vital to the project’s success.\nAs an example, these stakeholder conversations often reveal important features in the data generating or measurement process that should be accounted for when modelling. These details are usually left out of the data documentation because they would be immediately obvious to any specialist in that field.\n\n\n8.3.2 Communicating with project manager\nAn EDA can sometimes allow us to identify cases where the strength of signal within the available data is clearly insufficient to answer the question of interest. By clearly communicating this to the project manager, the project can be postponed while different, better quality or simply more data are collected. It’s important to note that this data collection is not trivial and can have a high cost in terms of both time and capital. It might be that collecting the data needed to answer a question will cost more than we’re likely to gain from knowing that answer. Whether the project is postponed or cancelled, this constitutes a successful outcome for the project, the aim is to to produce insight or profit - not to fit models for their own sake."
  },
  {
    "objectID": "302-edav-analysis.html#scope-your-project",
    "href": "302-edav-analysis.html#scope-your-project",
    "title": "8  Exploratory Data Analysis",
    "section": "8.4 Scope Your Project",
    "text": "8.4 Scope Your Project\n\nEDA is an initial assessment of whether the available data measure the correct values, in sufficient quality and quantity, to answer a particular question.\n\nA third view on EDA is as an initial assessment of whether the available data measure the correct values, with sufficient quality and quantity, to answer a particular question. In order for EDA to be successful, it’s important to take a few key steps.\nFirst, it’s important to formulate a specific question of interest or line of investigation and agree on it with the stakeholder. By having a clear question in mind, it will be easier to focus the analysis and identify whether the data at hand can answer it.\nNext, it’s important to make a record (if one doesn’t already exist) of how the data were collected, by whom it was collected, what each recorded variable represents and the units in which they are recorded. This meta-data is often known as a data sheet. Having this information in written form is crucial when adding a new collaborator to a project, so that they can understand the data generating and measurement processes, and are aware of the quality and accuracy of the recorded values."
  },
  {
    "objectID": "302-edav-analysis.html#investigate-your-data",
    "href": "302-edav-analysis.html#investigate-your-data",
    "title": "8  Exploratory Data Analysis",
    "section": "8.5 Investigate Your Data",
    "text": "8.5 Investigate Your Data\n\nEDA is an opportunitiy to quantify data completeness and investigate the possibility of informative missingness.\n\nIn addition, it’s essential to investigate and document the structure, precision, completeness and quantity of data available. This includes assessing the degree of measurement noise or misclassification in the data, looking for clear linear or non-linear dependencies between any of the variables, and identifying if any data are missing or if there’s any structure to this missingness. Other data features to be aware of are the presence of any censoring or whether some values tend to be missing together.\nFurthermore, a more advanced EDA might include a simulation study to estimate the amount of data needed to detect the smallest meaningful effect. This is more in-depth than a typical EDA but if you suspect that the signals within your data are weak relative to measurement noise, can help to demonstrate the limitations of the current line of enquiry with the information that is currently available."
  },
  {
    "objectID": "302-edav-analysis.html#what-is-not-eda",
    "href": "302-edav-analysis.html#what-is-not-eda",
    "title": "8  Exploratory Data Analysis",
    "section": "8.6 What is not EDA?",
    "text": "8.6 What is not EDA?\nIt’s important to understand that an exploratory data analysis is not the same thing as modelling. In particular is not the construction of your baseline model, which is sometimes called initial data analysis.\nThough it might inform the choice of baseline model, EDA is usually not model based. Simple plots and summaries are used to identify patterns in the data that inform how you approach the rest of the project.\nSome degree of statistical rigour can be added through the use of non-parametric techniques; methods like rolling averages, smoothing or partitioning can to help identify trends or patterns while making minimal assumptions about the data generating process.\n\n\n\n\n\nDaily change in Dow Jones Index with smoothed estimate of mean and 95% confidence interval.\n\n\n\n\n\n\n\nMean and standard deviation of daily change in Dow Jones Index, before and after 1st of June 1998.\n\n\nafter_june_98\nmean\nsd\n\n\n\n\nFALSE\n5.916798\n65.19093\n\n\nTRUE\n3.972929\n119.56067\n\n\n\n\n\nThough the assumptions in an EDA are often minimal it can help to make them explicit. For example, in this plot a moving averages is shown with a confidence band, but the construction of this band makes the implicit assumption that, at least locally, our observations have the same distribution and so are exchangeable.\nFinally, EDA is not a prescriptive process. While I have given a lot of suggestions on what you might usually want to consider, there is no correct way to go about an EDA because it is so heavily dependent on the particular dataset, its interpretation and the task you want to achieve with it. This is one of the parts of data science that make it a craft that you hone with experience, rather than an algorithmic process. When you work in a particular area for a long time you develop a knowledge for common data quirks in that area, which may or may not translate to other applications.\nNow that we have a better idea of what is and what is not EDA, let’s talk about the issue that an EDA tries to resolve and the other issues that it generates."
  },
  {
    "objectID": "302-edav-analysis.html#issue-forking-paths",
    "href": "302-edav-analysis.html#issue-forking-paths",
    "title": "8  Exploratory Data Analysis",
    "section": "8.7 Issue: Forking Paths",
    "text": "8.7 Issue: Forking Paths\nIn any data science project you have a sequence of very many decisions that you must make, each with many potential options and is difficult to decide upon a priori.\n\n\n\nFocusing in on only one small part of the process, we might consider picking a null or baseline model, which we will then try and improve on. Should that null model make constant predictions, incorporate a simple linear trend or is something more flexible obviously needed? Do you have the option to try all of these or are you working under time constraints? Are there contextual clues that rule some of these null models out on contextual grounds?\nAn EDA lets you narrow down your options by looking at your data and helps you to decide what might be reasonable modelling approaches.\n\n\n\nThe problem that sneaks in here is data leakage. Formally this is where training data is included in test set, but this sort of information leak can happen informally too. Usually this is because you’ve seen the data you’re trying to model or predict and then selected your modelling approach based on that information.\nStandard, frequentist statistical methods for estimation and testing assume no “peeking” of this type has occurred. If we use these methods without acknowledging that we have already observed our data then we will artificially inflate the significance of our findings. For example, we might be comparing two models: the first of which makes constant predictions with regard to a predictor, while the second includes a linear trend. We will of course use a statistical test to confirm that what we are seeing is unlikely by chance. However, we must be aware this test was only performed because we had previously examined at the data and noticed what looked to be a trend.\nSimilar issues arise in Bayesian approaches, particularly when constructing or eliciting prior distributions for our model parameters. One nice thing that we can do in the Bayesian setting is to simulating data from the prior predictive distribution and then get an expert to check that these datasets seem seem reasonable. However, it is often the case this expert is also the person who collected the data we will soon be modelling. It’s very difficult for them to ignore what they have seen, which leads to similar, subtle leakage problems."
  },
  {
    "objectID": "302-edav-analysis.html#correction-methods",
    "href": "302-edav-analysis.html#correction-methods",
    "title": "8  Exploratory Data Analysis",
    "section": "8.8 Correction Methods",
    "text": "8.8 Correction Methods\nThere are various methods or corrections that we can apply during our testing and estimation procedures to ensure that our error rates or confidence intervals account for our previous “peeking” during EDA.\nExamples of these corrections have been developed across many fields of statistics. In medical statistics we have approaches like the Bonferroni correction, to account for carrying out multiple hypothesis tests. In the change-point literature there are techniques for estimating a change location given that a change has been detected somewhere in a time series. While in the extreme value literature there are methods to estimate the required level of protection against rare events, given that the analysis was triggered by the current protections having been compromised.\n\n\n\n\n\nExample sea-height datasets where an analysis has been triggered by an extreme value (above) or a visually identified change in mean (below).\n\n\n\n\n::: ::::\nAll of these corrections require us to make assumptions about the nature of the peeking. They are either very specific about the process that has occurred, or else are very pessimistic about how much information has been leaked. Developing such corrections to account for EDA isn’t really possible, given its adaptive and non-prescriptive nature.\nIn addition to being either highly specific or pessimistic, these corrections can also be hard to derive and complicated to implement. This is why in settings where the power of tests or level of estimation is critically important, the entire analysis is pre-registered. In clinical trials, for example, every step of the analysis is specified before any data are collected. In data science this rigorous approach is rarely taken.\nAs statistically trained data scientists, it is important for us to remain humble about our potential findings and to suggest follow up studies to confirm the presence of any relationships we do find."
  },
  {
    "objectID": "302-edav-analysis.html#learning-more",
    "href": "302-edav-analysis.html#learning-more",
    "title": "8  Exploratory Data Analysis",
    "section": "8.9 Learning More",
    "text": "8.9 Learning More\nIn this chapter we have acknowledged that exploratory analyses are an important part of the data science workflow; this is true not only for us as data scientists, but also for the other people who are involved with our projects.\nWe’ve also seen that an exploratory analysis can help to guide the progression of our projects, but that in doing so we must take care to prevent and acknowledge the risk of data leakage.\nIf you want to explore this topic further, it can be quite challenging: examples of good, exploratory data analyses can be difficult to come by. This is because they are not often made publicly available in the same way that papers and technical reports are. Additionally, they are often kept out of public repositories because they are not as “polished” as the rest of the project. Personally, I think this is a shame and the culture on this is slowly changing.\nFor now, your best approach to learning about what makes a good exploratory analysis is to do lots of your own and to talk to you colleagues about their approaches.\nThere are lots of list-articles out there claiming to give you a comprehensive list of steps for any exploratory analysis. These can be good for inspiration, but I strongly suggest you don’t treat these as gospel.\nDespite the name of the chapter, Roger Peng’s EDA check-list gives an excellent worked example of an exploratory analysis in R. The the discussion article “Exploratory Data Analysis for Complex Models”, Andrew Gelman makes a more abstract discussion of both exploratory analyses (which happen before modelling) and confirmatory analyses (which happen afterwards)."
  },
  {
    "objectID": "310-edav-checklist.html#videos-chapters",
    "href": "310-edav-checklist.html#videos-chapters",
    "title": "Checklist",
    "section": "Videos / Chapters",
    "text": "Videos / Chapters\n\nData Wrangling (20 min) [slides]\nData Exploration (25 min) [slides]\nData Visualisation (27 min) [slides]"
  },
  {
    "objectID": "310-edav-checklist.html#reading",
    "href": "310-edav-checklist.html#reading",
    "title": "Checklist",
    "section": "Reading",
    "text": "Reading\nUse the Data Exploration and Visualisation section of the reading list to support and guide your exploration of this week’s topics. Note that these texts are divided into core reading, reference materials and materials of interest."
  },
  {
    "objectID": "310-edav-checklist.html#activities",
    "href": "310-edav-checklist.html#activities",
    "title": "Checklist",
    "section": "Activities",
    "text": "Activities\nCore:\n\nNormConf is a conference dedicated to the unglamorous but essential aspects of working in the data sciences. The in December 2022 conference talks are available as a Youtube Playlist. Find a talk that interests you and watch it, then post a short summary to EdStem, which includes what you learned from the talk and one thing that you still do not understand.\nWork through this ggplot2 tutorial for beautiful plotting in R by Cédric Scherer, recreating the examples for yourself.\nUsing your rolling_mean() function as inspiration, write a rolling_sd() function that calculates the rolling standard deviation of a numeric vector.\n\nExtend your rolling_sd() function to optionally return approximate point-wise confidence bands for your rolling standard deviations. These should be \\(\\pm2\\) standard errors by default and may be computed using analytical or re-sampling methods.\nCreate a visualisation using your extended rolling_sd() function to assess whether the variability in the daily change in Dow Jones Index is changing over time. [data]\n\n\nBonus:\n\nAdd your rolling_sd() function to your R package, adding documentation and tests.\nDuring an exploratory analysis, we often need to assess the validity of an assumed distribution based on a sample of data. Write your own versions of qqnorm() and qqplot(), which add point-wise tolerance intervals to assess whether deviation from the line \\(y=x\\) are larger than expected.\nAdd your own versions of qqnorm() and qqplot() to your R package, along with documentation and tests."
  },
  {
    "objectID": "310-edav-checklist.html#live-session",
    "href": "310-edav-checklist.html#live-session",
    "title": "Checklist",
    "section": "Live Session",
    "text": "Live Session\nIn the live session we will begin with a discussion of this week’s tasks. We will then break into small groups for two data visualisation exercises.\n(Note: For one of these exercises, it would be helpful to bring a small selection of coloured pens or pencils, of you have access to some. If not, please don’t worry - inventive use of black, blue and shading are perfectly acceptable alternatives!)\nPlease come to the live session prepared to discuss the following points:\n\nWhich NormConf video did you watch and what did you learn from it?\nOther than {ggplot2}, what else have you used to create data visualisations? What are their relative strengths and weaknesses?\nHow did you implement your rolling_sd() function and what conclusions did you draw when applying it to the Dow Jones data?"
  }
]