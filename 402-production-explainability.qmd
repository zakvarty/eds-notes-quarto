# Explainability {#production-explainability}

```{r production-explainability-setup}
#| results: asis
#| echo: false
#source("_common.R")
#status("complete")
```

## What are we explaining and to whom? 

There are many reasons you might need to explain the behaviour of your model before it can be put into production. As an example, we can consider a credit scoring system that determines whether or not customers should be given a line of credit. We might need an explanation that can help us to:

- Meet regulatory or legal requirements to describe how your model works (e.g. ban on "black-box" modelling).
- Understanding how your model works to improve it.
- Explaining individual predictions or outcomes to customers (e.g. loan refusal).

In each of these cases, what exactly do we mean by an explanation? It's likely not the same thing in each example. 

- Data scientists we might be interested to know exactly what types of mapping between covariates and responses can be represented by the neural network architecture underlying the credit scoring system. 

- Stakeholders within the company or regulators are likely indifferent to this and are more concerned about understanding the general behaviour of the model across large numbers of loan applications. 

- Finally, individual customers might have some investment in the overall behaviour of the scoring model but would also like to know what actions they can take to increase their chance of securing a loan. 

Between each of these examples, the level of technical detail differs but more importantly the fundamental nature of the explanations are different too.


## Explaining a Decision Tree

With some models giving an explanation is relatively straightforward. Decision trees are perhaps the easiest model to explain because they mimic human decision making and can be represented like flow-charts that make sequential, linear partitions of the predictor space.

```{r fig-abulance-triage}
#| eval: true
#| echo: false
#| fig-align: center
#| fig-cap: "An example of a decision tree, optimised to correctly identify category 1 ambulance calls in as few questions as possible."
#| fig-alt: "Descision tree to traige ambulance calls into urgency categories."
knitr::include_graphics("images/402-production-explainability/ambulance-triage.png")
```


Decision trees use the same sort of logic that is applied in a medical triage, which determines how urgently a patient needs to be seen in accident and emergency or when calling an abulance. The specific sequence of binary decisions used in this type of triage are optimised to identify critical calls as soon as possible, but this is not the only objective that we could have optimised for. An alternative loss function might pick these partitions to get the most accurate overall classification of calls to urgency categories. If this leads to an algorithm where 30 minutes of detailed questioning is required to assign a category, then it might not be the most appropriate loss function to triage ambulance calls. However, a similar loss function might be well suitable when deciding which loan applicants are granted credit, since this is a less time-sensitive decision.

The core issue with these decision trees is that they are limited in the relationships they can represent (linear relationships approximated by step function) and are sensitive to small changes in the training data. To overcome these deficiencies we can use a bootstrap aggregation or a random forest model to make predictions based on a collection of these trees. This leads to models that are more stable and flexible but also removes any chance of a simple and human-friendly explanation. 

## Explaining Regression Models 

Another model that is relatively straightforward to interpret is a linear regression. We can interpret this model using the estimated regression coefficients, which describe how the predicted outcome changes with a unit change in each covariate (while the values of all other covariates are held constant). 


```{r fig-interpreting-linear-regression}
#| eval: true
#| echo: false
#| fig-align: center
#| fig-cap: "Linear models have global, conditional explanations, provided by the estimated regression coefficients."
#| fig-alt: "Sketch of estimated linear regression model overlaid on data. The gratient of the fitted line is labelled as the estimated regression coefficient."
knitr::include_graphics("images/402-production-explainability/interpreting-linear-regression.png")
```

This is a global and a conditional explanation. 

It is __global__ because the effect of increasing a covariate by one unit is the same no matter what the starting value of that covariate. That is to say the explanation is the same in all parts of the covariate space. 

The explanation is __conditional__ because it assumes that all other values are held constant. This can lead to some odd behaviour in our explanations because they are inherently dependent on what other terms are included (or left out of) the model. 

This can be contrasted against non-linear regression. In this model, the regression coefficients provide local conditional explanations of how altering each covariate changes the prediction made.  These explanations are conditional in the sense that they assume the values of all other covariates are held constant and they are local in that the size and direction of this effect might vary across different values of the covariate we are trying to explain. 

```{r fig-interpreting-nonlinear-models}
#| eval: true
#| echo: false
#| fig-align: center
#| fig-cap: "Non-linear models have local, conditional explanations, provided by the estimated regression coefficients."
#| fig-alt: "Estimated non-linear regression curve overlaid on raw data. At low values of the covariate a unit increase produces a large increase in the estimated response. This gradient is descreasing so that at large covariate values the same change in the covariate produces almost no change in the response."
knitr::include_graphics("images/402-production-explainability/interpreting-nonlinear-regression.png")
```

Here we have an example where a unit increase in the covariate is associated with a large change in the model response at low values of the covariate, but a much smaller change at large values of the covariate.


## Example: Cherrywood regression 

As an example of this we can look at the height, girth and volume of some cherry trees. 

If we are wanting to use a lathe to produce pretty, cherry wood ornaments we might be interested in understanding how the girth of the trees varies with their height and total volume. Using a linear model, we see that both height and volume have a positive linear association with girth. 

```{r fig-cherry-tree-lm}
#| eval: true
#| echo: false
#| warning: false
#| note: false
#| message: false
#| fig-align: center
#| fig-cap: "Cherry tree girth can be well modelled as a linear function of either tree height or harvestable volume."
#| fig-alt: "Linear regression models overlaid on data for predicting tree girth as a linear function of either height or volume. Both have positive coefficients."
library(tidyverse)
library(ggtext)

lm_height <- lm(Girth ~ 1 + Height, data = trees)
lm_volume  <- lm(Girth ~ 1 + Volume, data = trees)

p1 <- ggplot(data = trees, aes(x = Height, y = Girth)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Height", y = "Girth") +
  ylim(c(8,22)) +
  ggtitle("Cherry Tree Dimensions")+
  theme_minimal()

p2 <- ggplot(data = trees, aes(x = Volume, y = Girth)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(x = "Volume", y = "Girth") +
  ylim(c(8,22)) +
  theme_minimal()

cowplot::plot_grid(p1, p2, ncol = 2)
```

```{r cherry-tree-lm-1}
#| eval: true
#| echo: true
lm(Girth ~ 1 + Height, data = trees)
```

```{r cherry-tree-lm-2}
#| eval: true
#| echo: true
lm(Girth ~ 1 + Volume, data = trees)
```

However, when we include both terms in our model, the interpretation changes dramatically.

```{r cherry-tree-lm-combined}
#| eval: true
#| echo: true
lm(Girth ~ 1 + Height + Volume, data = trees)
```


Height is no longer positively associated with girth. This is because the size, direction and significance of our estimated effects is conditional on what other terms are included in the model. For a _fixed volume_ of wood, a taller tree necessarily has to have a smaller girth. 

Techniques such as [SHAP](https://christophm.github.io/interpretable-ml-book/shap.html) try to quantify the importance of a predictor by averaging its effect on the prediction over models with all combinations of predictors included or excluded. You can read more about such techniques in [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar.

## Simpson's Paradox 

This dependence between a model explanation and which predictors it includes is related to __Simpson's Paradox__, where a trend appears in several sub-groups of a data set but disappears or reverses when considering the data as a whole. This regularly arises in fields like epidemiology, where population level trends are assumed to apply at the level of individuals or small groups, where this is known as the ecological fallacy. 


```{r fig-simpsons-paradox}
#| eval: true
#| echo: false
#| message: false
#| fig-align: center
#| fig-cap: "A simulated example in which a covariate has a negative trend at the population level but a positive trend within each sub-group."

set.seed(1234)

data_1 <- mgcv::rmvn(n = 100, mu = c(0,4), V = matrix(c(2,0.6,0.6,2), nrow = 2))
data_2 <- mgcv::rmvn(n = 100, mu = c(2,2), V = matrix(c(2,0.2,0.2,2), nrow = 2))
data_3 <- mgcv::rmvn(n = 100, mu = c(4,0), V = matrix(c(2,0.9,0.9,2), nrow = 2))

simpson <- data.frame(
  x_1 = c(data_1[,1], data_2[,1], data_3[,1]), 
  x_2 = c(data_1[,2], data_2[,2], data_3[,2]),
  group = as.factor(rep(1:3, each = 100))
)

p3 <- simpson %>% 
  ggplot(aes(x = x_1, y = x_2)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  ggtitle("Simpson's Paradox", "Groupings Unknown") +
  theme_minimal()

p4 <- simpson %>% 
  group_by(group) %>% 
  ggplot(aes(x = x_1, y = x_2, col = group)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  ggtitle("", "Groupings known") +
  theme_minimal()

cowplot::plot_grid(p3, p4, ncol = 2)
```

Actually, Simpson's parodox is a terrible name, because it isn't actually a paradox at all. It's not surprising that we have two different answers to two different questions, the supposed contradiction only arises when we fail to distinguish between those questions. 

What I hope to have highlighted here is that even for some of the simplest models we might use as data scientists, while explanations are very much possible they must be made with care and attention to detail. Correctly interpreting models in context can be far from straightforward, regardless of model complexity.  


## What hope do we have?

At this stage, you might be asking yourself what hope we have of explaining more complicated models like random forests or neural networks, given how difficult it is to explain even the simple models we might use as a benchmark. You'd be right to worry about this and it is important to remain humble in what we can and cannot know about these complex systems that we are building. 

All hope isn't lost though - we still have a few tricks up our sleeve! 

### Permutation Testing  

Suppose we're asked by our product manager to determine which predictors or covariates are usually the most import in our model when determining credit scores and loan outcomes.

One way to assess this would be to remove each covariate from the model and investigate how that changes the predictions made by our model. However, we've seen already that removing a predictor can substantitally change the interpretation of some models.  

Instead, we could answer this question by using permutation methods.

If we take the observed values of a covariate, say income, and randomly allocate these among all our training examples then this will destroy any association between income and loan outcome. This allows us to remove the information provided by income but without altering the overall structure of our model (e.g. the joint distribution of the remaining predictors). We can then refit our model to this modified data set and investigate how shuffling that covariate alters our predictions. If there is a large performance drop then the covariate is playing an important role within our model.

There are many variations on this sort of permutation test, which can be simple but powerful tools for understanding the behaviour of all sorts of models.

### Meta-modelling 

As the name suggests, meta-models are models of models. These can be effective tools of providing localised explanations of a model that is too complicated for us to obtain an explanation directly (e.g. a neural network). 

Meta-modelling uses a simple surrogate model to approximate the behaviour of a more complicated, unexplainable model around a single point in covariate space for which we require an explanation. 

To do this, we select several "design points" in a small region of covariate space centred on our explanation point. We then evaluate the more complicated model at each of these design points to obtain a set of predictions. We then fit a simpler, explainable model to this set of predictions and use this fitted model to provide an approximate, local explanation of the more complicated model.

```{r fig-function-approximation}
#| eval: true
#| echo: false
#| fig-align: center
#| fig-cap: "A local linear approximation in two dimensions"
#| fig-alt: "A sketch of a non-linear response surface with two input dimensions. In a small region this is locally approximated by a linear function."
knitr::include_graphics("images/402-production-explainability/local-linear-approximation.png")
```



Linear regression is a common choice for the meta-model. This is partly for convenience and familiarity but also has a theoretical backing: if our complex model is sufficiently smooth then we can appeal to [Taylor's Theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem) to say that in a small enough region it can be well approximated by a linear function. 

This sort of local model explanation is particularly useful where we want to explain individual predictions or decisions made by our model: for example why a single loan applicant was not granted a loan. By inspecting the coefficients of the local surrogate model we can identify which covariates were the most influential in the decision and suggest how the applicant could increase their chance of success by summarising those covariates that were both influential and are within the ability of the applicant to change. This is exactly the approach taken by the [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) methodology developed by Ribeiro et al. 

### Aggregating Meta-models

Using local or conditional explanations of our model's behaviour can be useful in some circumstances but they don't give a broader understanding of what is going on overall. What if we want to know how a covariate influences _all_ outcomes not a particular one? What if we care about the covariate's expected effect over all loan applicants, or the _distribution_ of effects over all applicants? 

This is where local and conditional explanations are particularly nice. By making these explanations at many points we can aggregate these explanations to understand the global and marginal behaviour of our models. 

To aggregate our conditional effects into a marginal effect, or a local effect into a global effect we must integrate these over the joint distribution of all covariates. If this sounds a bit scary to you, then you are right. Integration is hard enough at the best of times without adding in the fact that we don't know the joint distribution of the covariates we are using as predictors. 

Don't worry though, we can take the easy way out and do both of these things approximately. We can approximate the joint distribution of the covariates by the empirical distribution that we observe in our sample, and then our nasty integrals simplify to averages over the measurement units in our data (the loan applicants in our case). 


If we construct a local, conditional model for each loan applicant in our data set, we can approximate the marginal effect of each covariate by averaging the conditional effects we obtain for each loan applicant. 


```{r fig-local-approximation}
#| eval: true
#| echo: false
#| fig-align: center
#| fig-cap: "Local approximations around each observation can be combined to understand global model behaviour."
#| fig-alt: "A sketch of a two dimensional data set where the local region around each observed value is indicated by a blue square."
knitr::include_graphics("images/402-production-explainability/local-neighbourhoods.png")
```


This gives us a global understanding of how each covariate influences the response of our model. It does this over all possible values of the other covariates and appropriately weights these according to their frequency within the sample (and also within the population, if our sample is representative). 

## Wrapping Up  

We've seen that as our models become more flexible they also become more difficult to explain, whether that is to ourselves, and subject expert or a user of our model. 

That's not to say that simple models are always easy to explain or interpret, simple models can be deceptively tricky to communicate accurately. 

Finally, we looked at a couple of techniques for explaining more complex models. 

We can use permutation tests measure feature importance in our models: shuffling the predictor  values breaks any relationship to our response, and we can measure how much this degrades our model performance. A big dip implies that feature was doing a lot of explaining.  

We can also look at the local behaviour of models by making surrogate or meta models, that are interpretable, and aggregate these to understand the model globally. 

Effective explanations are essential if you want your model to be used in production and to feed into real decisions decision making. This requires some level of skill with rhetoric to tailor your explanation so that it is clear to the person who requested it. But this isn't a soft skill, it also requires a surprising amount of computational and mathematical skill to extract such explanations from complex modern models. 
<!-- 

* LIME paper on ArXiV: https://arxiv.org/abs/1602.04938. Ribeiro et al (2016) "Why Should I Trust You?": Explaining the Predictions of Any Classifier.


* LIME pacakge documentation on CRAN https://cran.r-project.org/web/packages/lime/index.html  

* Understanding LIME tutorial -  T Pedersen and M Benesty 

https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html 

* Reference: Interpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar 
-->
